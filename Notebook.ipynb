{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aayushmanda/da6401_assignment1/blob/main/Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yo9bgFzcT9wn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import wandb\n",
        "import argparse\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Fashion-MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.1)\n",
        "\n",
        "\n",
        "\n",
        "# wandb.login(key='e6b43dd118f9a14e83fe12c597ad8d06bdfed432')\n",
        "\n",
        "# # wandb.init(project=\"da6401-asg1\")\n",
        "\n",
        "# # Get the number of classes and their name mappings\n",
        "# num_classes = 10\n",
        "# class_mapping = {0: \"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\", 5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"}\n",
        "# print(\"Done!\")\n",
        "\n",
        "# ##############################################################################\n",
        "# # Plotting a figure from each class\n",
        "# plt.figure(figsize=[12, 5])\n",
        "# img_list = []\n",
        "# class_list = []\n",
        "\n",
        "# for i in range(num_classes):\n",
        "#     position = np.argmax(train_labels==i)\n",
        "#     image = train_images[position,:,:]\n",
        "#     plt.subplot(2, 5, i+1)\n",
        "#     plt.imshow(image)\n",
        "#     plt.title(class_mapping[i])\n",
        "#     img_list.append(image)\n",
        "#     class_list.append(class_mapping[i])\n",
        "\n",
        "# wandb.log({\"Question 1\": [wandb.Image(img, caption=caption) for img, caption in zip(img_list, class_list)]})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgiPWNACcpEJ",
        "outputId": "005656c8-04d3-4bd6-ca5e-102f4e7b2d22"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bAdCIGHhT-IG"
      },
      "outputs": [],
      "source": [
        "# Near copy paste of the layers we have developed in Part 3\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "class Linear:\n",
        "\n",
        "  def __init__(self, fan_in, fan_out, weight_init = \"Xavier\", bias=True):\n",
        "\n",
        "    self.cache = dict(x=None)\n",
        "    self.gradd = dict(weight=None, bias=None)\n",
        "    if weight_init == \"Xavier\":\n",
        "      #XavierInit\n",
        "      self.weight = np.random.randn(fan_in, fan_out) / (fan_in + fan_out)**0.5\n",
        "      self.bias = np.zeros(fan_out, dtype=\"f\") if bias else None\n",
        "    else:\n",
        "      #RandomInit\n",
        "      self.weight = np.random.randn(fan_in, fan_out)\n",
        "      self.bias = np.random.randn(fan_out) if bias else None\n",
        "\n",
        "  def __str__(self):\n",
        "      return \"Linear({:d}, {:d}, bias={})\".format(self.weight.shape[0], self.weight.shape[1], self.bias is not None)\n",
        "\n",
        "\n",
        "  def __call__(self, x):\n",
        "      self.out = x @ self.weight\n",
        "      if self.bias is not None:\n",
        "          self.out += self.bias\n",
        "      # Store input for backward pass\n",
        "      self.cache[\"x\"] = x\n",
        "      return self.out\n",
        "\n",
        "  def grad(self, d_out):\n",
        "    x = self.cache[\"x\"]\n",
        "    # Weight gradient: x^T @ d_out\n",
        "    self.gradd[\"weight\"] = x.T @ d_out\n",
        "\n",
        "    dzdx = d_out@self.weight.T\n",
        "\n",
        "    # Bias gradient: sum over batch\n",
        "    if self.bias is not None:\n",
        "        self.gradd[\"bias\"] = np.sum(d_out, axis=0)\n",
        "\n",
        "    # returning dzdx\n",
        "    return dzdx\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.weight] + ([self.bias] if self.bias is not None else [])\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "\n",
        "class Tanh:\n",
        "  def __call__(self, x):\n",
        "    self.x = x\n",
        "    self.out = np.tanh(self.x)\n",
        "    # self.cache = {\"x\": np.tanh(self.x)}\n",
        "    return self.out\n",
        "  def parameters(self):\n",
        "    # Activation Function\n",
        "    return []\n",
        "\n",
        "  def grad(self, dout):\n",
        "    return dout * (1 - self.out**2)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "\n",
        "class Sigmoid:\n",
        "  def __call__(self, x):\n",
        "      self.out = 1. / (1. + np.exp(-x))\n",
        "      return self.out\n",
        "\n",
        "  def grad(self, d_out):\n",
        "      # sigmoid derivative: σ(x)(1 - σ(x))\n",
        "      return (self.out * (1. - self.out)) * d_out\n",
        "\n",
        "  def parameters(self):\n",
        "      return []\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class Relu:\n",
        "  def __call__(self, x):\n",
        "      self.cache = {\"x\": x}\n",
        "      self.out = np.maximum(0, x)\n",
        "      return self.out\n",
        "\n",
        "  def grad(self, d_out):\n",
        "      x = self.cache[\"x\"]\n",
        "      dx = np.ones_like(x)\n",
        "      dx[x < 0] = 0\n",
        "      return np.array(d_out) * dx\n",
        "\n",
        "  def parameters(self):\n",
        "      return []\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "\n",
        "class CrossEntropyLoss:\n",
        "\n",
        "    def __init__(self, reduction='mean', eps=1e-12):  # More stable epsilon\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def __str__(self):\n",
        "        return f'CrossEntropyLoss(reduction={self.reduction}, eps={self.eps})'\n",
        "\n",
        "    def __call__(self, y, y_true):\n",
        "        return self.forward(y, y_true)\n",
        "\n",
        "    def forward(self, y, y_true):\n",
        "        # Final layer activation is softmax and y here is logits\n",
        "        exp_y = np.exp(y - np.max(y, axis=1, keepdims=True))\n",
        "        probs = exp_y / np.sum(exp_y + 1e-12, axis=1, keepdims=True)\n",
        "\n",
        "        # print(probs)\n",
        "\n",
        "\n",
        "        # Clip probabilities to [eps, 1-eps] to avoid log(0) Done this aftis after many random trials\n",
        "        clipped_probs = np.clip(probs[y_true.astype(bool)], self.eps, 1.0 - self.eps)\n",
        "\n",
        "        per_sample_loss = -np.log(clipped_probs)\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return np.mean(per_sample_loss)\n",
        "        elif self.reduction == 'sum':\n",
        "            return np.sum(per_sample_loss)\n",
        "        else:\n",
        "            return per_sample_loss\n",
        "\n",
        "    def grad(self, y, y_true):\n",
        "        # Simple (1/B)*(One - hot vector - yhat)\n",
        "        return (1.0 / y.shape[0]) * (y - y_true)  # Maintain gradient scaling\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class Sequential:\n",
        "\n",
        "  def __init__(self, layers=None):\n",
        "    self.layers = layers\n",
        "\n",
        "  def __call__(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    self.out = x\n",
        "    return self.out\n",
        "\n",
        "  def append(self, layer):\n",
        "    self.layers.append(layer)\n",
        "\n",
        "  def parameters(self):\n",
        "    # get parameters of all layers and stretch them out into one list\n",
        "    return [p for layer in self.layers for p in layer.parameters()]\n",
        "\n",
        "  def backward(self, d_out):\n",
        "    # Backpropagate through layers in reverse order\n",
        "    d = d_out\n",
        "    gradients = []\n",
        "    #storing layer.weight, layer.bias grad in a list also\n",
        "\n",
        "    for layer in reversed(self.layers):\n",
        "        d = layer.grad(d)\n",
        "        if hasattr(layer, 'weight'):\n",
        "            gradients.append(layer.gradd[\"bias\"] if layer.bias is not None else None)\n",
        "            gradients.append(layer.gradd[\"weight\"])\n",
        "    return d, list(reversed(gradients))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FJeJo2Pr40Q3"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "# Load the Fashion-MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.1)\n",
        "\n",
        "\n",
        "ix = np.random.randint(0, train_images.shape[0], (32,))\n",
        "x = train_images[ix]\n",
        "x = x.reshape(32, -1)\n",
        "y = train_labels[ix]\n",
        "y = np.eye(10)[y]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VIu-PUEiHP8o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc4482ea-9e7f-4c07-e3cc-c348601da405"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 784), (32, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "x.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ocNEDkDDksZk"
      },
      "outputs": [],
      "source": [
        "class MSE:\n",
        "  def __init__(self, eps=1e-8):\n",
        "      self.eps = eps\n",
        "  def __call__(self, y, y_true):\n",
        "      return self.forward(y, y_true)\n",
        "\n",
        "  def forward(self, y, y_true):\n",
        "      exp_y = np.exp(y - np.max(y, axis=1, keepdims=True))\n",
        "      probs = exp_y / np.sum(exp_y + 1e-12, axis=1, keepdims=True)\n",
        "      # Clip probabilities to [eps, 1-eps] to avoid log(0) Done this aftis after many random trials\n",
        "      # clipped_probs = np.clip(probs[y_true.astype(bool)], self.eps, 1.0 - self.eps)\n",
        "      return np.mean((probs - y_true)**2)\n",
        "\n",
        "  def grad(self, y, y_true):\n",
        "      # y: logits with shape (batch_size, num_classes)\n",
        "      # Compute softmax probabilities\n",
        "      exp_y = np.exp(y - np.max(y, axis=1, keepdims=True))\n",
        "      probs = exp_y / (np.sum(exp_y, axis=1, keepdims=True) + 1e-12)\n",
        "      batch_size, num_classes = y.shape\n",
        "      grad_input = np.zeros_like(y)\n",
        "      # For each sample in the batch, compute:\n",
        "      #   dL/dp = 2*(probs - y_true)/batch_size   (MSE derivative w.r.t. softmax outputs)\n",
        "      #   dp/dz = Jacobian of softmax = diag(p) - p pᵀ\n",
        "      # and then dL/dz = (dp/dz) · (dL/dp)\n",
        "      for i in range(batch_size):\n",
        "          p = probs[i].reshape(-1, 1)  # Column vector (num_classes, 1)\n",
        "          # Jacobian for softmax (num_classes x num_classes)\n",
        "          J = np.diagflat(p) - np.dot(p, p.T)\n",
        "          dL_dp = 2 * (probs[i] - y_true[i]) / batch_size\n",
        "          grad_input[i, :] = np.dot(J, dL_dp)\n",
        "      return grad_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "451OcH2QXhPf"
      },
      "outputs": [],
      "source": [
        "# logits[0], y[0], logits[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0tEG2u0nB-ix"
      },
      "outputs": [],
      "source": [
        "# y/logits[range(32),np.argmax(y, axis=1)].reshape(32,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ozZJ43oD0nlC"
      },
      "outputs": [],
      "source": [
        "class Optimizer():\n",
        "    def __init__(self, lr=0.001, optimizer=\"sgd\", momentum=0.9,\n",
        "                 epsilon=1e-8, beta=0.9, beta1=0.9, beta2=0.999, t=0, decay=0, param=None):\n",
        "      self.lr = lr\n",
        "      self.optimizer = optimizer\n",
        "      self.momentum = momentum\n",
        "      self.epsilon = epsilon\n",
        "      self.beta = beta\n",
        "      self.beta1 = beta1\n",
        "      self.beta2 = beta2\n",
        "      self.t = t\n",
        "      self.decay = decay\n",
        "      self.velocity = [np.zeros_like(p) for p in param]\n",
        "      self.moments = [np.zeros_like(p) for p in param]\n",
        "\n",
        "    def __call__(self, param, dparam):\n",
        "      self.run(param, dparam)\n",
        "\n",
        "    def run(self, param, dparam, epoch=None):\n",
        "        if(self.optimizer == \"sgd\"):\n",
        "            self.SGD(param, dparam)\n",
        "        elif(self.optimizer == \"momentum\"):\n",
        "            self.MomentumGD(param, dparam)\n",
        "        elif(self.optimizer == \"nag\"):\n",
        "            self.NAG(param, dparam)\n",
        "        elif(self.optimizer == \"rmsprop\"):\n",
        "            self.RMSProp(param, dparam)\n",
        "        elif(self.optimizer == \"adam\"):\n",
        "            self.Adam(param, dparam)\n",
        "        elif (self.optimizer == \"nadam\"):\n",
        "            self.NAdam(param, dparam)\n",
        "        else:\n",
        "            raise Exception(\"Invalid optimizer\")\n",
        "\n",
        "    def SGD(self, param, dparam):\n",
        "        for p, grad in zip(param, dparam):\n",
        "            p -= self.lr * (grad + self.decay * p) #grad\n",
        "\n",
        "    def MomentumGD(self, param, dparam):\n",
        "        #tried using zeros but got Value error maximum dim support for ndarray is 32.\n",
        "        for i, (u, param, grad) in enumerate(zip(self.velocity, param, dparam)):\n",
        "            u = self.momentum * u + grad\n",
        "            self.velocity[i] = u\n",
        "            param -= self.lr * (u  + self.decay * param)   #clipped_dparam #grad\n",
        "\n",
        "    def NAG(self, param, dparam):\n",
        "        for i, (u, param, grad) in enumerate(zip(self.velocity, param, dparam)):\n",
        "\n",
        "            u = self.momentum * u +  grad\n",
        "            self.velocity[i] = u\n",
        "            m_bar = grad + self.momentum * u\n",
        "            param -= self.lr * (m_bar + self.decay * param)    #clipped_dparam #grad\n",
        "\n",
        "    def RMSProp(self, param, dparam ):\n",
        "        for i, (u, param, grad) in enumerate(zip(self.velocity, param, dparam)):\n",
        "            u = self.beta * u + (1 - self.beta) * (grad**2)\n",
        "            self.velocity[i] = u\n",
        "            param -= self.lr * (grad / (np.sqrt(u + self.epsilon)) + self.decay * param)\n",
        "\n",
        "    def Adam(self, param, dparam):\n",
        "        i = self.t\n",
        "        for i, (m, v, param, grad) in enumerate(zip(self.moments, self.velocity, param, dparam)):\n",
        "            m = self.beta1 * m + (1 - self.beta1) * grad\n",
        "            self.moments[i] = m\n",
        "            m_hat = m/(1-self.beta1**(i+1))\n",
        "\n",
        "            v = self.beta2 * v + (1 - self.beta2) * (grad**2)\n",
        "            self.velocity[i] = v\n",
        "            v_hat = v/(1-self.beta2**(i+1) )\n",
        "\n",
        "            param -= self.lr * ( m_hat / (np.sqrt(v_hat + self.epsilon))+ self.decay * param)\n",
        "\n",
        "\n",
        "    def NAdam(self, param, dparam):\n",
        "        i = self.t\n",
        "        for i, (m, v, param, grad) in enumerate(zip(self.moments, self.velocity, param, dparam)):\n",
        "            g_hat = grad/(1-self.beta1**(i+1))\n",
        "            m = self.beta1 * m + (1 - self.beta1) * grad\n",
        "            self.moments[i] = m\n",
        "            m_hat = m/(1-self.beta1**(i+1))\n",
        "\n",
        "            v = self.beta1 * v + (1 - self.beta2) * (grad**2)\n",
        "            self.velocity[i] = v\n",
        "            v_hat = v/(1-self.beta2**(i+1))\n",
        "\n",
        "            m_bar = (1-self.beta1)*g_hat + self.beta1*m_hat\n",
        "\n",
        "            param -= self.lr * (m_bar / (np.sqrt(v_hat + self.epsilon))+ self.decay * param)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_out = 10\n",
        "n_hidden = 16\n",
        "hid_layers = 5\n",
        "ix = \"relu\"\n",
        "init = \"Xavier\"\n",
        "\n",
        "\n",
        "activation = {\"tanh\": Tanh(), \"relu\": Relu(), \"sigmoid\": Sigmoid()}\n",
        "\n",
        "model = Sequential([Linear(784, n_hidden, weight_init = init)])\n",
        "model.append(activation[ix])\n",
        "# for i in range(hid_layers-1):\n",
        "#   model.append(Linear(n_hidden, n_hidden, weight_init = init))\n",
        "#   model.append(activation[ix])\n",
        "model.append(Linear(n_hidden, 10, weight_init = init))\n",
        "# model.append(Softmax())"
      ],
      "metadata": {
        "id": "DEUQ1rKr2o5W"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[i.shape for i in model.parameters()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Y2PbQVa2roU",
        "outputId": "0062adb1-7de4-43cc-9f70-ac3ead392f83"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(784, 16), (16,), (16, 10), (10,)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ix = np.random.randint(0, train_images.shape[0], (32,))\n",
        "x = train_images[ix]\n",
        "x = x.reshape(32, -1)\n",
        "y = train_labels[ix]\n",
        "y = np.eye(10)[y]\n",
        "opt = Optimizer(lr=1e-3, optimizer=\"rmsprop\", param=model.parameters())\n",
        "\n",
        "#Forward Pass. ## Our cross entropy takes care of softmax activation in final layer\n",
        "\n",
        "logits = model(x)\n",
        "logits = logits/np.sum(logits, axis=-1, keepdims=True) # To stabilize the backward pass\n",
        "loss = CrossEntropyLoss()(logits, y)\n",
        "# print(loss)\n",
        "#Backward Pass\n",
        "\n",
        "dout = MSE().grad(logits, y)\n",
        "dout = model.backward(dout)\n",
        "\n",
        "#parameter update\n",
        "opt(model.parameters(), dout[1])\n"
      ],
      "metadata": {
        "id": "fS2XxLm52NOv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiM5k6jR23kL",
        "outputId": "8c67ff88-8223-4e9d-9332-ff1584bb2c00"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.3341028691864"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = logits\n",
        "exp_y = np.exp(y - np.max(y, axis=1, keepdims=True))\n",
        "probs = exp_y / np.sum(exp_y + 1e-12, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "pwf9lNyz0wR3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ftrDbOy0iwc",
        "outputId": "3d2d5917-c877-4192-96ff-71115288b3d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 10), (32, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ5O51KmtGqL",
        "outputId": "4588f18b-650e-40d0-83f9-48d2c9407fe7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9.56100340e-02, 2.70886914e-01, 3.35212111e-01, 5.57796747e-02,\n",
              "       7.86249261e-02, 3.52629851e-07, 1.59375871e-01, 6.45732291e-02,\n",
              "       1.76690540e-01, 2.71973322e-02])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "np.diag(probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nr01meNf0L3d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fCJLYcOG13Oa"
      },
      "outputs": [],
      "source": [
        "def initialize_model(activation, layer_sizes, init=\"Xavier\"):\n",
        "    if activation == \"tanh\":\n",
        "      print(\"Activation used is Tanh\")\n",
        "\n",
        "      model = Sequential([Linear(784, layer_sizes[1], weight_init=init)])\n",
        "\n",
        "      # Hidden layers\n",
        "      for i in range(1, len(layer_sizes)-1):\n",
        "          model.append(Tanh())\n",
        "          model.append(Linear(layer_sizes[i], layer_sizes[i+1], weight_init=init))\n",
        "\n",
        "      # Final output layer\n",
        "      model.append(Tanh())\n",
        "      model.append(Linear(layer_sizes[-1], 10, weight_init=init))  # 128->10\n",
        "\n",
        "\n",
        "    elif activation == \"relu\":\n",
        "      print(\"Activation used is ReLu\")\n",
        "\n",
        "      model = Sequential([Linear(784, layer_sizes[1], weight_init=init)])\n",
        "\n",
        "      # Hidden layers\n",
        "      for i in range(1, len(layer_sizes)-1):\n",
        "          model.append(Relu())\n",
        "          model.append(Linear(layer_sizes[i], layer_sizes[i+1], weight_init=init))\n",
        "\n",
        "      # Final output layer\n",
        "      model.append(Relu())\n",
        "      model.append(Linear(layer_sizes[-1], 10, weight_init=init))  # 128->10\n",
        "\n",
        "\n",
        "    elif activation == \"sigmoid\":\n",
        "      print(\"Activation used is Sigmoid\")\n",
        "\n",
        "      model = Sequential([Linear(784, layer_sizes[1], weight_init=init)])\n",
        "\n",
        "      # Hidden layers\n",
        "      for i in range(1, len(layer_sizes)-1):\n",
        "          model.append(Sigmoid())\n",
        "          model.append(Linear(layer_sizes[i], layer_sizes[i+1], weight_init=init))\n",
        "\n",
        "      # Final output layer\n",
        "      model.append(Sigmoid())\n",
        "      model.append(Linear(layer_sizes[-1], 10, weight_init=init))  # 128->10\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "u_rcwRQtqxLa"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "def train():\n",
        "    #init run\n",
        "    run = wandb.init()\n",
        "\n",
        "    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "    train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.1)\n",
        "    X = train_images.reshape(train_images.shape[0], -1)/ 255.0\n",
        "    Y = train_labels\n",
        "    Y = np.eye(10)[Y]\n",
        "\n",
        "\n",
        "    config = wandb.config\n",
        "    hid_layers = config.hid_layers\n",
        "    init = config.init\n",
        "    # max_steps = wandb.config.n_steps\n",
        "    batch_size = config.batch_size\n",
        "    activation = config.activation\n",
        "    nepoch = config.nepoch\n",
        "    loss_fn = config.loss\n",
        "    Loss = CrossEntropyLoss() if loss_fn==\"cross_entropy\" else MSE()\n",
        "    #naming the run\n",
        "    run.name = f\"opt_{config.optimizer}|loss_{loss_fn}|lr={config.lr}|batch_{batch_size}|act_{activation}|hid_{hid_layers}|neurons_{config.hid_size}|nrns_{nepoch}|init_{init}\" + str(np.random.randint(1000))\n",
        "\n",
        "    layer_sizes = [config.hid_size] * (hid_layers + 1)\n",
        "\n",
        "\n",
        "\n",
        "    model = initialize_model(activation, layer_sizes, init)\n",
        "\n",
        "    opt = Optimizer(lr=config.lr, optimizer=config.optimizer, decay = config.decay, param=model.parameters())\n",
        "\n",
        "    logits = model(X)\n",
        "    train_loss = Loss(logits, Y)\n",
        "    train_accuracy = np.mean(np.argmax(logits, axis=1) == np.argmax(Y, axis=1))\n",
        "\n",
        "    val_logits = model(val_images.reshape(val_images.shape[0], -1)/ 255.0)\n",
        "    Yv = np.eye(10)[val_labels]\n",
        "    val_loss = Loss(val_logits, Yv)\n",
        "    val_accuracy = np.mean(np.argmax(val_logits, axis=1) == np.argmax(Yv, axis=1))\n",
        "\n",
        "    print(f\"Start of Training: {1} Train Accuracy: {train_accuracy:.4f} Val Loss: {val_loss:.4f} Val Accuracy: {val_accuracy:.4f}\")\n",
        "    wandb.log({\"Epoch\": 0, \"Val Loss\": val_loss, \"Train Accuracy\": train_accuracy, \"Val Accuracy\": val_accuracy})\n",
        "\n",
        "    for epoch in range(nepoch):\n",
        "      print(\"-------x-------\")\n",
        "      #Shuffling\n",
        "      indices = np.random.permutation(X.shape[0])\n",
        "      X = X[indices]\n",
        "      Y = Y[indices]\n",
        "\n",
        "      for i in range(0, train_images.shape[0], batch_size):\n",
        "        Xb = X[i:i + batch_size]\n",
        "        Yb = Y[i:i + batch_size]\n",
        "\n",
        "        logits = model(Xb)\n",
        "\n",
        "        loss = Loss(logits, Yb)\n",
        "\n",
        "        #Backward Pass\n",
        "        dout = Loss.grad(logits, Yb)\n",
        "        dout = model.backward(dout)\n",
        "\n",
        "\n",
        "        batch_num = i//batch_size\n",
        "        total_batch = train_images.shape[0]//batch_size\n",
        "\n",
        "\n",
        "\n",
        "        #Parameter Update\n",
        "        opt(model.parameters(), dout[1])\n",
        "\n",
        "        if batch_num%200 == 0: # print every once in a while uhh to be precise after 200 batch\n",
        "          print(f'Epoch({epoch+1}/{nepoch})\\t Batch({batch_num:2d}/{total_batch:2d}): \\tTrain Loss  {loss:.4f}')\n",
        "\n",
        "      opt.t += 1\n",
        "\n",
        "\n",
        "      #Accuracy Calculation\n",
        "      wandb.log({\"Epoch\": epoch+1, \"Train Loss\": loss})\n",
        "      logits = model(X)\n",
        "      train_loss = Loss(logits, Y)\n",
        "      train_accuracy = np.mean(np.argmax(logits, axis=1) == np.argmax(Y, axis=1))\n",
        "\n",
        "      val_logits = model(val_images.reshape(val_images.shape[0], -1)/ 255.0)\n",
        "      Yv = np.eye(10)[val_labels]\n",
        "      val_loss = Loss(val_logits, Yv)\n",
        "      val_accuracy = np.mean(np.argmax(val_logits, axis=1) == np.argmax(Yv, axis=1))\n",
        "\n",
        "\n",
        "\n",
        "      print(f\"End of Epoch: {epoch+1} Train Accuracy: {train_accuracy:.4f} Val Loss: {val_loss:.4f} Val Accuracy: {val_accuracy:.4f}\")\n",
        "      wandb.log({\"Epoch\": epoch+1, \"Val Loss\": val_loss, \"Train Accuracy\": train_accuracy, \"Val Accuracy\": val_accuracy})\n",
        "\n",
        "    # -----------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key='e6b43dd118f9a14e83fe12c597ad8d06bdfed432')\n",
        "\n",
        "\n",
        "# Sweep configuration\n",
        "sweep_config = {\n",
        "    \"method\": \"random\",\n",
        "    \"metric\": {\"goal\": \"maximize\", \"name\": \"Val Accuracy\"},\n",
        "    \"parameters\": {\n",
        "        \"batch_size\": {\"values\": [16, 32, 64]},\n",
        "        \"decay\": {\"values\": [0, 0.5, 0.0005]},\n",
        "        \"hid_layers\": {\"values\": [3, 4, 5]},\n",
        "        \"hid_size\": {\"values\": [32, 64, 128]},\n",
        "        \"nepoch\": {\"values\": [5, 10]},\n",
        "        \"activation\": {\"values\": [\"relu\", \"tanh\", \"sigmoid\"]},\n",
        "        \"init\": {\"values\": [\"Xavier\", \"Random\"]},\n",
        "        \"optimizer\": {\"values\": [\"sgd\", \"momentum\", \"rmsprop\", \"nag\", \"adam\", \"nadam\"]},\n",
        "        \"loss\": {\"values\": [\"mse\", \"cross_entropy\"]},\n",
        "        \"lr\": {\"values\": [0.0001, 0.001]},\n",
        "    },\n",
        "}\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"da6401-assignment1\")\n",
        "wandb.agent(sweep_id, function=train, count = 100)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3TtR2tag73m5",
        "outputId": "502add9b-746c-44ea-ca2c-e8de3ff7a50c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mda24s016\u001b[0m (\u001b[33mda24s016-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: cd4pcsed\n",
            "Sweep URL: https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7keblrui with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_163151-7keblrui</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/7keblrui' target=\"_blank\">unique-sweep-1</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/7keblrui' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/7keblrui</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0990 Val Loss: 2.3018 Val Accuracy: 0.0985\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/3375): \tTrain Loss  2.3044\n",
            "Epoch(1/5)\t Batch(200/3375): \tTrain Loss  1.8787\n",
            "Epoch(1/5)\t Batch(400/3375): \tTrain Loss  1.7951\n",
            "Epoch(1/5)\t Batch(600/3375): \tTrain Loss  1.7131\n",
            "Epoch(1/5)\t Batch(800/3375): \tTrain Loss  1.7742\n",
            "Epoch(1/5)\t Batch(1000/3375): \tTrain Loss  1.7729\n",
            "Epoch(1/5)\t Batch(1200/3375): \tTrain Loss  1.4992\n",
            "Epoch(1/5)\t Batch(1400/3375): \tTrain Loss  1.6369\n",
            "Epoch(1/5)\t Batch(1600/3375): \tTrain Loss  1.6648\n",
            "Epoch(1/5)\t Batch(1800/3375): \tTrain Loss  1.7440\n",
            "Epoch(1/5)\t Batch(2000/3375): \tTrain Loss  1.7349\n",
            "Epoch(1/5)\t Batch(2200/3375): \tTrain Loss  1.7073\n",
            "Epoch(1/5)\t Batch(2400/3375): \tTrain Loss  1.8350\n",
            "Epoch(1/5)\t Batch(2600/3375): \tTrain Loss  1.6415\n",
            "Epoch(1/5)\t Batch(2800/3375): \tTrain Loss  1.6531\n",
            "Epoch(1/5)\t Batch(3000/3375): \tTrain Loss  1.7091\n",
            "Epoch(1/5)\t Batch(3200/3375): \tTrain Loss  1.7434\n",
            "End of Epoch: 1 Train Accuracy: 0.8428 Val Loss: 1.7203 Val Accuracy: 0.8293\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/3375): \tTrain Loss  1.8199\n",
            "Epoch(2/5)\t Batch(200/3375): \tTrain Loss  1.6776\n",
            "Epoch(2/5)\t Batch(400/3375): \tTrain Loss  1.7617\n",
            "Epoch(2/5)\t Batch(600/3375): \tTrain Loss  1.6546\n",
            "Epoch(2/5)\t Batch(800/3375): \tTrain Loss  1.6354\n",
            "Epoch(2/5)\t Batch(1000/3375): \tTrain Loss  1.8186\n",
            "Epoch(2/5)\t Batch(1200/3375): \tTrain Loss  1.6371\n",
            "Epoch(2/5)\t Batch(1400/3375): \tTrain Loss  1.6070\n",
            "Epoch(2/5)\t Batch(1600/3375): \tTrain Loss  1.6832\n",
            "Epoch(2/5)\t Batch(1800/3375): \tTrain Loss  1.7055\n",
            "Epoch(2/5)\t Batch(2000/3375): \tTrain Loss  1.7748\n",
            "Epoch(2/5)\t Batch(2200/3375): \tTrain Loss  1.7722\n",
            "Epoch(2/5)\t Batch(2400/3375): \tTrain Loss  1.8004\n",
            "Epoch(2/5)\t Batch(2600/3375): \tTrain Loss  1.6891\n",
            "Epoch(2/5)\t Batch(2800/3375): \tTrain Loss  1.6288\n",
            "Epoch(2/5)\t Batch(3000/3375): \tTrain Loss  1.7065\n",
            "Epoch(2/5)\t Batch(3200/3375): \tTrain Loss  1.6510\n",
            "End of Epoch: 2 Train Accuracy: 0.8284 Val Loss: 1.6621 Val Accuracy: 0.8065\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/3375): \tTrain Loss  1.7004\n",
            "Epoch(3/5)\t Batch(200/3375): \tTrain Loss  1.8219\n",
            "Epoch(3/5)\t Batch(400/3375): \tTrain Loss  1.6983\n",
            "Epoch(3/5)\t Batch(600/3375): \tTrain Loss  1.5548\n",
            "Epoch(3/5)\t Batch(800/3375): \tTrain Loss  1.5223\n",
            "Epoch(3/5)\t Batch(1000/3375): \tTrain Loss  1.6566\n",
            "Epoch(3/5)\t Batch(1200/3375): \tTrain Loss  1.6849\n",
            "Epoch(3/5)\t Batch(1400/3375): \tTrain Loss  1.6739\n",
            "Epoch(3/5)\t Batch(1600/3375): \tTrain Loss  1.5534\n",
            "Epoch(3/5)\t Batch(1800/3375): \tTrain Loss  1.5962\n",
            "Epoch(3/5)\t Batch(2000/3375): \tTrain Loss  1.7570\n",
            "Epoch(3/5)\t Batch(2200/3375): \tTrain Loss  1.6947\n",
            "Epoch(3/5)\t Batch(2400/3375): \tTrain Loss  1.7263\n",
            "Epoch(3/5)\t Batch(2600/3375): \tTrain Loss  1.7206\n",
            "Epoch(3/5)\t Batch(2800/3375): \tTrain Loss  1.7691\n",
            "Epoch(3/5)\t Batch(3000/3375): \tTrain Loss  1.6014\n",
            "Epoch(3/5)\t Batch(3200/3375): \tTrain Loss  1.5791\n",
            "End of Epoch: 3 Train Accuracy: 0.8530 Val Loss: 1.7306 Val Accuracy: 0.8343\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/3375): \tTrain Loss  1.7075\n",
            "Epoch(4/5)\t Batch(200/3375): \tTrain Loss  1.6972\n",
            "Epoch(4/5)\t Batch(400/3375): \tTrain Loss  1.7328\n",
            "Epoch(4/5)\t Batch(600/3375): \tTrain Loss  1.7520\n",
            "Epoch(4/5)\t Batch(800/3375): \tTrain Loss  1.6983\n",
            "Epoch(4/5)\t Batch(1000/3375): \tTrain Loss  1.8623\n",
            "Epoch(4/5)\t Batch(1200/3375): \tTrain Loss  1.7492\n",
            "Epoch(4/5)\t Batch(1400/3375): \tTrain Loss  1.7654\n",
            "Epoch(4/5)\t Batch(1600/3375): \tTrain Loss  1.7724\n",
            "Epoch(4/5)\t Batch(1800/3375): \tTrain Loss  1.7083\n",
            "Epoch(4/5)\t Batch(2000/3375): \tTrain Loss  1.6185\n",
            "Epoch(4/5)\t Batch(2200/3375): \tTrain Loss  1.7827\n",
            "Epoch(4/5)\t Batch(2400/3375): \tTrain Loss  1.5806\n",
            "Epoch(4/5)\t Batch(2600/3375): \tTrain Loss  1.6185\n",
            "Epoch(4/5)\t Batch(2800/3375): \tTrain Loss  1.6706\n",
            "Epoch(4/5)\t Batch(3000/3375): \tTrain Loss  1.7139\n",
            "Epoch(4/5)\t Batch(3200/3375): \tTrain Loss  1.5576\n",
            "End of Epoch: 4 Train Accuracy: 0.8281 Val Loss: 1.6923 Val Accuracy: 0.8208\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/3375): \tTrain Loss  1.7054\n",
            "Epoch(5/5)\t Batch(200/3375): \tTrain Loss  1.5935\n",
            "Epoch(5/5)\t Batch(400/3375): \tTrain Loss  1.8386\n",
            "Epoch(5/5)\t Batch(600/3375): \tTrain Loss  1.5278\n",
            "Epoch(5/5)\t Batch(800/3375): \tTrain Loss  1.5529\n",
            "Epoch(5/5)\t Batch(1000/3375): \tTrain Loss  1.6381\n",
            "Epoch(5/5)\t Batch(1200/3375): \tTrain Loss  1.6842\n",
            "Epoch(5/5)\t Batch(1400/3375): \tTrain Loss  1.7634\n",
            "Epoch(5/5)\t Batch(1600/3375): \tTrain Loss  1.7037\n",
            "Epoch(5/5)\t Batch(1800/3375): \tTrain Loss  1.6895\n",
            "Epoch(5/5)\t Batch(2000/3375): \tTrain Loss  1.6894\n",
            "Epoch(5/5)\t Batch(2200/3375): \tTrain Loss  1.7672\n",
            "Epoch(5/5)\t Batch(2400/3375): \tTrain Loss  1.5657\n",
            "Epoch(5/5)\t Batch(2600/3375): \tTrain Loss  1.8994\n",
            "Epoch(5/5)\t Batch(2800/3375): \tTrain Loss  1.6479\n",
            "Epoch(5/5)\t Batch(3000/3375): \tTrain Loss  1.6484\n",
            "Epoch(5/5)\t Batch(3200/3375): \tTrain Loss  1.6767\n",
            "End of Epoch: 5 Train Accuracy: 0.8008 Val Loss: 1.6972 Val Accuracy: 0.7908\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁█████</td></tr><tr><td>Train Loss</td><td>▅▁█▅▃</td></tr><tr><td>Val Accuracy</td><td>▁█████</td></tr><tr><td>Val Loss</td><td>█▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.80081</td></tr><tr><td>Train Loss</td><td>1.57983</td></tr><tr><td>Val Accuracy</td><td>0.79083</td></tr><tr><td>Val Loss</td><td>1.69719</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_cross_entropy|lr=0.001|batch_16|act_relu|hid_3|neurons_128|nrns_5|init_Xavier498</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/7keblrui' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/7keblrui</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_163151-7keblrui/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8rppgz9x with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_163327-8rppgz9x</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8rppgz9x' target=\"_blank\">jolly-sweep-2</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8rppgz9x' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8rppgz9x</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0668 Val Loss: 0.1872 Val Accuracy: 0.0637\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  0.1906\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  0.1844\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  0.1812\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  0.1781\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  0.1693\n",
            "End of Epoch: 1 Train Accuracy: 0.1068 Val Loss: 0.1795 Val Accuracy: 0.1023\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  0.1812\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  0.1906\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  0.1750\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  0.1750\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  0.1781\n",
            "End of Epoch: 2 Train Accuracy: 0.1387 Val Loss: 0.1731 Val Accuracy: 0.1340\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  0.1750\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  0.1656\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  0.1655\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  0.1687\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  0.1562\n",
            "End of Epoch: 3 Train Accuracy: 0.1577 Val Loss: 0.1700 Val Accuracy: 0.1492\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  0.1656\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  0.1719\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  0.1592\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  0.1687\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  0.1562\n",
            "End of Epoch: 4 Train Accuracy: 0.1863 Val Loss: 0.1638 Val Accuracy: 0.1803\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  0.1712\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  0.1625\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  0.1687\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  0.1562\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  0.1812\n",
            "End of Epoch: 5 Train Accuracy: 0.1871 Val Loss: 0.1637 Val Accuracy: 0.1808\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  0.1594\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  0.1656\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  0.1437\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  0.1624\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  0.1531\n",
            "End of Epoch: 6 Train Accuracy: 0.1884 Val Loss: 0.1633 Val Accuracy: 0.1828\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  0.1594\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  0.1625\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  0.1625\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  0.1625\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  0.1590\n",
            "End of Epoch: 7 Train Accuracy: 0.1822 Val Loss: 0.1642 Val Accuracy: 0.1783\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  0.1656\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  0.1623\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  0.1344\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  0.1469\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  0.1531\n",
            "End of Epoch: 8 Train Accuracy: 0.1876 Val Loss: 0.1632 Val Accuracy: 0.1832\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  0.1749\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  0.1594\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  0.1499\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  0.1469\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  0.1839\n",
            "End of Epoch: 9 Train Accuracy: 0.1920 Val Loss: 0.1626 Val Accuracy: 0.1865\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  0.1618\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  0.1500\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  0.1687\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  0.1523\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  0.1680\n",
            "End of Epoch: 10 Train Accuracy: 0.1910 Val Loss: 0.1623 Val Accuracy: 0.1873\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▃▅▆███▇███</td></tr><tr><td>Train Loss</td><td>█▇▄▄▅▅▃▄▁▄</td></tr><tr><td>Val Accuracy</td><td>▁▃▅▆███▇███</td></tr><tr><td>Val Loss</td><td>█▆▄▃▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.191</td></tr><tr><td>Train Loss</td><td>0.157</td></tr><tr><td>Val Accuracy</td><td>0.18733</td></tr><tr><td>Val Loss</td><td>0.16227</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nadam|loss_mse|lr=0.0001|batch_64|act_relu|hid_3|neurons_64|nrns_10|init_Random621</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8rppgz9x' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8rppgz9x</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_163327-8rppgz9x/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5l3w383x with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_163443-5l3w383x</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/5l3w383x' target=\"_blank\">sandy-sweep-3</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/5l3w383x' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/5l3w383x</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.1003 Val Loss: 0.0922 Val Accuracy: 0.0977\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  0.0934\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  0.0908\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  0.0908\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  0.0902\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  0.0910\n",
            "End of Epoch: 1 Train Accuracy: 0.1003 Val Loss: 0.0904 Val Accuracy: 0.0977\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  0.0907\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  0.0902\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  0.0900\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  0.0904\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  0.0905\n",
            "End of Epoch: 2 Train Accuracy: 0.1003 Val Loss: 0.0901 Val Accuracy: 0.0977\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  0.0903\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  0.0904\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  0.0902\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  0.0899\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  0.0897\n",
            "End of Epoch: 3 Train Accuracy: 0.1003 Val Loss: 0.0900 Val Accuracy: 0.0977\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  0.0900\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  0.0902\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  0.0900\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  0.0900\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  0.0901\n",
            "End of Epoch: 4 Train Accuracy: 0.1003 Val Loss: 0.0900 Val Accuracy: 0.0977\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  0.0900\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  0.0899\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  0.0900\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  0.0900\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  0.0900\n",
            "End of Epoch: 5 Train Accuracy: 0.1005 Val Loss: 0.0900 Val Accuracy: 0.0955\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  0.0901\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  0.0900\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  0.0900\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  0.0900\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  0.0900\n",
            "End of Epoch: 6 Train Accuracy: 0.1005 Val Loss: 0.0900 Val Accuracy: 0.0955\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  0.0900\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  0.0900\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  0.0900\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  0.0899\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  0.0900\n",
            "End of Epoch: 7 Train Accuracy: 0.1005 Val Loss: 0.0900 Val Accuracy: 0.0955\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  0.0900\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  0.0900\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  0.0900\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  0.0900\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  0.0900\n",
            "End of Epoch: 8 Train Accuracy: 0.1005 Val Loss: 0.0900 Val Accuracy: 0.0955\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  0.0900\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  0.0900\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  0.0900\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  0.0900\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  0.0900\n",
            "End of Epoch: 9 Train Accuracy: 0.1005 Val Loss: 0.0900 Val Accuracy: 0.0955\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  0.0900\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  0.0900\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  0.0900\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  0.0900\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  0.0900\n",
            "End of Epoch: 10 Train Accuracy: 0.1005 Val Loss: 0.0900 Val Accuracy: 0.0955\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▁▁▁▁██████</td></tr><tr><td>Train Loss</td><td>█▄▁▁▁▁▁▁▁▁</td></tr><tr><td>Val Accuracy</td><td>█████▁▁▁▁▁▁</td></tr><tr><td>Val Loss</td><td>█▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.1005</td></tr><tr><td>Train Loss</td><td>0.09</td></tr><tr><td>Val Accuracy</td><td>0.0955</td></tr><tr><td>Val Loss</td><td>0.09</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_sgd|loss_mse|lr=0.001|batch_64|act_sigmoid|hid_4|neurons_64|nrns_10|init_Xavier50</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/5l3w383x' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/5l3w383x</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_163443-5l3w383x/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ygtsick7 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nag\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_163548-ygtsick7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ygtsick7' target=\"_blank\">happy-sweep-4</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ygtsick7' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ygtsick7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.0988 Val Loss: 0.0925 Val Accuracy: 0.1108\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/1687): \tTrain Loss  0.0935\n",
            "Epoch(1/10)\t Batch(200/1687): \tTrain Loss  0.0911\n",
            "Epoch(1/10)\t Batch(400/1687): \tTrain Loss  0.0906\n",
            "Epoch(1/10)\t Batch(600/1687): \tTrain Loss  0.0904\n",
            "Epoch(1/10)\t Batch(800/1687): \tTrain Loss  0.0898\n",
            "Epoch(1/10)\t Batch(1000/1687): \tTrain Loss  0.0900\n",
            "Epoch(1/10)\t Batch(1200/1687): \tTrain Loss  0.0901\n",
            "Epoch(1/10)\t Batch(1400/1687): \tTrain Loss  0.0901\n",
            "Epoch(1/10)\t Batch(1600/1687): \tTrain Loss  0.0900\n",
            "End of Epoch: 1 Train Accuracy: 0.1006 Val Loss: 0.0900 Val Accuracy: 0.0948\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/1687): \tTrain Loss  0.0900\n",
            "Epoch(2/10)\t Batch(200/1687): \tTrain Loss  0.0901\n",
            "Epoch(2/10)\t Batch(400/1687): \tTrain Loss  0.0900\n",
            "Epoch(2/10)\t Batch(600/1687): \tTrain Loss  0.0900\n",
            "Epoch(2/10)\t Batch(800/1687): \tTrain Loss  0.0901\n",
            "Epoch(2/10)\t Batch(1000/1687): \tTrain Loss  0.0901\n",
            "Epoch(2/10)\t Batch(1200/1687): \tTrain Loss  0.0900\n",
            "Epoch(2/10)\t Batch(1400/1687): \tTrain Loss  0.0899\n",
            "Epoch(2/10)\t Batch(1600/1687): \tTrain Loss  0.0899\n",
            "End of Epoch: 2 Train Accuracy: 0.1006 Val Loss: 0.0900 Val Accuracy: 0.0948\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/1687): \tTrain Loss  0.0900\n",
            "Epoch(3/10)\t Batch(200/1687): \tTrain Loss  0.0900\n",
            "Epoch(3/10)\t Batch(400/1687): \tTrain Loss  0.0900\n",
            "Epoch(3/10)\t Batch(600/1687): \tTrain Loss  0.0900\n",
            "Epoch(3/10)\t Batch(800/1687): \tTrain Loss  0.0901\n",
            "Epoch(3/10)\t Batch(1000/1687): \tTrain Loss  0.0900\n",
            "Epoch(3/10)\t Batch(1200/1687): \tTrain Loss  0.0900\n",
            "Epoch(3/10)\t Batch(1400/1687): \tTrain Loss  0.0900\n",
            "Epoch(3/10)\t Batch(1600/1687): \tTrain Loss  0.0900\n",
            "End of Epoch: 3 Train Accuracy: 0.1001 Val Loss: 0.0900 Val Accuracy: 0.0987\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/1687): \tTrain Loss  0.0901\n",
            "Epoch(4/10)\t Batch(200/1687): \tTrain Loss  0.0900\n",
            "Epoch(4/10)\t Batch(400/1687): \tTrain Loss  0.0900\n",
            "Epoch(4/10)\t Batch(600/1687): \tTrain Loss  0.0900\n",
            "Epoch(4/10)\t Batch(800/1687): \tTrain Loss  0.0900\n",
            "Epoch(4/10)\t Batch(1000/1687): \tTrain Loss  0.0901\n",
            "Epoch(4/10)\t Batch(1200/1687): \tTrain Loss  0.0900\n",
            "Epoch(4/10)\t Batch(1400/1687): \tTrain Loss  0.0900\n",
            "Epoch(4/10)\t Batch(1600/1687): \tTrain Loss  0.0901\n",
            "End of Epoch: 4 Train Accuracy: 0.1006 Val Loss: 0.0900 Val Accuracy: 0.0948\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/1687): \tTrain Loss  0.0899\n",
            "Epoch(5/10)\t Batch(200/1687): \tTrain Loss  0.0900\n",
            "Epoch(5/10)\t Batch(400/1687): \tTrain Loss  0.0900\n",
            "Epoch(5/10)\t Batch(600/1687): \tTrain Loss  0.0902\n",
            "Epoch(5/10)\t Batch(800/1687): \tTrain Loss  0.0900\n",
            "Epoch(5/10)\t Batch(1000/1687): \tTrain Loss  0.0901\n",
            "Epoch(5/10)\t Batch(1200/1687): \tTrain Loss  0.0900\n",
            "Epoch(5/10)\t Batch(1400/1687): \tTrain Loss  0.0900\n",
            "Epoch(5/10)\t Batch(1600/1687): \tTrain Loss  0.0900\n",
            "End of Epoch: 5 Train Accuracy: 0.1002 Val Loss: 0.0900 Val Accuracy: 0.0985\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/1687): \tTrain Loss  0.0900\n",
            "Epoch(6/10)\t Batch(200/1687): \tTrain Loss  0.0900\n",
            "Epoch(6/10)\t Batch(400/1687): \tTrain Loss  0.0900\n",
            "Epoch(6/10)\t Batch(600/1687): \tTrain Loss  0.0900\n",
            "Epoch(6/10)\t Batch(800/1687): \tTrain Loss  0.0900\n",
            "Epoch(6/10)\t Batch(1000/1687): \tTrain Loss  0.0900\n",
            "Epoch(6/10)\t Batch(1200/1687): \tTrain Loss  0.0900\n",
            "Epoch(6/10)\t Batch(1400/1687): \tTrain Loss  0.0900\n",
            "Epoch(6/10)\t Batch(1600/1687): \tTrain Loss  0.0900\n",
            "End of Epoch: 6 Train Accuracy: 0.1002 Val Loss: 0.0900 Val Accuracy: 0.0980\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/1687): \tTrain Loss  0.0900\n",
            "Epoch(7/10)\t Batch(200/1687): \tTrain Loss  0.0900\n",
            "Epoch(7/10)\t Batch(400/1687): \tTrain Loss  0.0899\n",
            "Epoch(7/10)\t Batch(600/1687): \tTrain Loss  0.0900\n",
            "Epoch(7/10)\t Batch(800/1687): \tTrain Loss  0.0900\n",
            "Epoch(7/10)\t Batch(1000/1687): \tTrain Loss  0.0901\n",
            "Epoch(7/10)\t Batch(1200/1687): \tTrain Loss  0.0900\n",
            "Epoch(7/10)\t Batch(1400/1687): \tTrain Loss  0.0899\n",
            "Epoch(7/10)\t Batch(1600/1687): \tTrain Loss  0.0899\n",
            "End of Epoch: 7 Train Accuracy: 0.1006 Val Loss: 0.0900 Val Accuracy: 0.0948\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/1687): \tTrain Loss  0.0900\n",
            "Epoch(8/10)\t Batch(200/1687): \tTrain Loss  0.0899\n",
            "Epoch(8/10)\t Batch(400/1687): \tTrain Loss  0.0900\n",
            "Epoch(8/10)\t Batch(600/1687): \tTrain Loss  0.0900\n",
            "Epoch(8/10)\t Batch(800/1687): \tTrain Loss  0.0901\n",
            "Epoch(8/10)\t Batch(1000/1687): \tTrain Loss  0.0900\n",
            "Epoch(8/10)\t Batch(1200/1687): \tTrain Loss  0.0900\n",
            "Epoch(8/10)\t Batch(1400/1687): \tTrain Loss  0.0900\n",
            "Epoch(8/10)\t Batch(1600/1687): \tTrain Loss  0.0902\n",
            "End of Epoch: 8 Train Accuracy: 0.1005 Val Loss: 0.0900 Val Accuracy: 0.0958\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/1687): \tTrain Loss  0.0901\n",
            "Epoch(9/10)\t Batch(200/1687): \tTrain Loss  0.0900\n",
            "Epoch(9/10)\t Batch(400/1687): \tTrain Loss  0.0900\n",
            "Epoch(9/10)\t Batch(600/1687): \tTrain Loss  0.0900\n",
            "Epoch(9/10)\t Batch(800/1687): \tTrain Loss  0.0901\n",
            "Epoch(9/10)\t Batch(1000/1687): \tTrain Loss  0.0900\n",
            "Epoch(9/10)\t Batch(1200/1687): \tTrain Loss  0.0900\n",
            "Epoch(9/10)\t Batch(1400/1687): \tTrain Loss  0.0899\n",
            "Epoch(9/10)\t Batch(1600/1687): \tTrain Loss  0.0900\n",
            "End of Epoch: 9 Train Accuracy: 0.1002 Val Loss: 0.0900 Val Accuracy: 0.0985\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/1687): \tTrain Loss  0.0899\n",
            "Epoch(10/10)\t Batch(200/1687): \tTrain Loss  0.0900\n",
            "Epoch(10/10)\t Batch(400/1687): \tTrain Loss  0.0900\n",
            "Epoch(10/10)\t Batch(600/1687): \tTrain Loss  0.0901\n",
            "Epoch(10/10)\t Batch(800/1687): \tTrain Loss  0.0901\n",
            "Epoch(10/10)\t Batch(1000/1687): \tTrain Loss  0.0900\n",
            "Epoch(10/10)\t Batch(1200/1687): \tTrain Loss  0.0899\n",
            "Epoch(10/10)\t Batch(1400/1687): \tTrain Loss  0.0901\n",
            "Epoch(10/10)\t Batch(1600/1687): \tTrain Loss  0.0900\n",
            "End of Epoch: 10 Train Accuracy: 0.1006 Val Loss: 0.0900 Val Accuracy: 0.0948\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁██▆█▆▇██▆█</td></tr><tr><td>Train Loss</td><td>▄█▆▁▄▆▅▃▃▆</td></tr><tr><td>Val Accuracy</td><td>█▁▁▃▁▃▂▁▁▃▁</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.10057</td></tr><tr><td>Train Loss</td><td>0.09009</td></tr><tr><td>Val Accuracy</td><td>0.09483</td></tr><tr><td>Val Loss</td><td>0.09001</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nag|loss_mse|lr=0.001|batch_32|act_sigmoid|hid_5|neurons_64|nrns_10|init_Xavier568</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ygtsick7' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ygtsick7</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_163548-ygtsick7/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bdnm6v1p with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: momentum\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_163724-bdnm6v1p</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bdnm6v1p' target=\"_blank\">faithful-sweep-5</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bdnm6v1p' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bdnm6v1p</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.1611 Val Loss: 0.0900 Val Accuracy: 0.1648\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/843): \tTrain Loss  0.0903\n",
            "Epoch(1/5)\t Batch(200/843): \tTrain Loss  0.0894\n",
            "Epoch(1/5)\t Batch(400/843): \tTrain Loss  0.0900\n",
            "Epoch(1/5)\t Batch(600/843): \tTrain Loss  0.0897\n",
            "Epoch(1/5)\t Batch(800/843): \tTrain Loss  0.0886\n",
            "End of Epoch: 1 Train Accuracy: 0.2596 Val Loss: 0.0891 Val Accuracy: 0.2647\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/843): \tTrain Loss  0.0891\n",
            "Epoch(2/5)\t Batch(200/843): \tTrain Loss  0.0885\n",
            "Epoch(2/5)\t Batch(400/843): \tTrain Loss  0.0887\n",
            "Epoch(2/5)\t Batch(600/843): \tTrain Loss  0.0885\n",
            "Epoch(2/5)\t Batch(800/843): \tTrain Loss  0.0882\n",
            "End of Epoch: 2 Train Accuracy: 0.2794 Val Loss: 0.0881 Val Accuracy: 0.2887\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/843): \tTrain Loss  0.0884\n",
            "Epoch(3/5)\t Batch(200/843): \tTrain Loss  0.0880\n",
            "Epoch(3/5)\t Batch(400/843): \tTrain Loss  0.0875\n",
            "Epoch(3/5)\t Batch(600/843): \tTrain Loss  0.0872\n",
            "Epoch(3/5)\t Batch(800/843): \tTrain Loss  0.0867\n",
            "End of Epoch: 3 Train Accuracy: 0.3018 Val Loss: 0.0868 Val Accuracy: 0.3105\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/843): \tTrain Loss  0.0870\n",
            "Epoch(4/5)\t Batch(200/843): \tTrain Loss  0.0861\n",
            "Epoch(4/5)\t Batch(400/843): \tTrain Loss  0.0862\n",
            "Epoch(4/5)\t Batch(600/843): \tTrain Loss  0.0860\n",
            "Epoch(4/5)\t Batch(800/843): \tTrain Loss  0.0861\n",
            "End of Epoch: 4 Train Accuracy: 0.3290 Val Loss: 0.0853 Val Accuracy: 0.3357\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/843): \tTrain Loss  0.0852\n",
            "Epoch(5/5)\t Batch(200/843): \tTrain Loss  0.0861\n",
            "Epoch(5/5)\t Batch(400/843): \tTrain Loss  0.0847\n",
            "Epoch(5/5)\t Batch(600/843): \tTrain Loss  0.0858\n",
            "Epoch(5/5)\t Batch(800/843): \tTrain Loss  0.0836\n",
            "End of Epoch: 5 Train Accuracy: 0.3509 Val Loss: 0.0832 Val Accuracy: 0.3538\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▅▅▆▇█</td></tr><tr><td>Train Loss</td><td>█▆▆▄▁</td></tr><tr><td>Val Accuracy</td><td>▁▅▆▆▇█</td></tr><tr><td>Val Loss</td><td>█▇▆▅▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.35087</td></tr><tr><td>Train Loss</td><td>0.0822</td></tr><tr><td>Val Accuracy</td><td>0.35383</td></tr><tr><td>Val Loss</td><td>0.08323</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_momentum|loss_mse|lr=0.0001|batch_64|act_tanh|hid_4|neurons_64|nrns_5|init_Xavier330</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bdnm6v1p' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bdnm6v1p</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_163724-bdnm6v1p/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zhsf5qh0 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nag\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_163810-zhsf5qh0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/zhsf5qh0' target=\"_blank\">dainty-sweep-6</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/zhsf5qh0' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/zhsf5qh0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.0877 Val Loss: 0.1640 Val Accuracy: 0.0915\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/1687): \tTrain Loss  0.1607\n",
            "Epoch(1/10)\t Batch(200/1687): \tTrain Loss  0.1453\n",
            "Epoch(1/10)\t Batch(400/1687): \tTrain Loss  0.1693\n",
            "Epoch(1/10)\t Batch(600/1687): \tTrain Loss  0.1604\n",
            "Epoch(1/10)\t Batch(800/1687): \tTrain Loss  0.1645\n",
            "Epoch(1/10)\t Batch(1000/1687): \tTrain Loss  0.1590\n",
            "Epoch(1/10)\t Batch(1200/1687): \tTrain Loss  0.1734\n",
            "Epoch(1/10)\t Batch(1400/1687): \tTrain Loss  0.1378\n",
            "Epoch(1/10)\t Batch(1600/1687): \tTrain Loss  0.1687\n",
            "End of Epoch: 1 Train Accuracy: 0.1051 Val Loss: 0.1613 Val Accuracy: 0.1058\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/1687): \tTrain Loss  0.1683\n",
            "Epoch(2/10)\t Batch(200/1687): \tTrain Loss  0.1268\n",
            "Epoch(2/10)\t Batch(400/1687): \tTrain Loss  0.1590\n",
            "Epoch(2/10)\t Batch(600/1687): \tTrain Loss  0.1642\n",
            "Epoch(2/10)\t Batch(800/1687): \tTrain Loss  0.1683\n",
            "Epoch(2/10)\t Batch(1000/1687): \tTrain Loss  0.1631\n",
            "Epoch(2/10)\t Batch(1200/1687): \tTrain Loss  0.1610\n",
            "Epoch(2/10)\t Batch(1400/1687): \tTrain Loss  0.1713\n",
            "Epoch(2/10)\t Batch(1600/1687): \tTrain Loss  0.1648\n",
            "End of Epoch: 2 Train Accuracy: 0.1218 Val Loss: 0.1571 Val Accuracy: 0.1253\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/1687): \tTrain Loss  0.1453\n",
            "Epoch(3/10)\t Batch(200/1687): \tTrain Loss  0.1675\n",
            "Epoch(3/10)\t Batch(400/1687): \tTrain Loss  0.1663\n",
            "Epoch(3/10)\t Batch(600/1687): \tTrain Loss  0.1467\n",
            "Epoch(3/10)\t Batch(800/1687): \tTrain Loss  0.1574\n",
            "Epoch(3/10)\t Batch(1000/1687): \tTrain Loss  0.1398\n",
            "Epoch(3/10)\t Batch(1200/1687): \tTrain Loss  0.1512\n",
            "Epoch(3/10)\t Batch(1400/1687): \tTrain Loss  0.1727\n",
            "Epoch(3/10)\t Batch(1600/1687): \tTrain Loss  0.1556\n",
            "End of Epoch: 3 Train Accuracy: 0.1256 Val Loss: 0.1567 Val Accuracy: 0.1223\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/1687): \tTrain Loss  0.1618\n",
            "Epoch(4/10)\t Batch(200/1687): \tTrain Loss  0.1651\n",
            "Epoch(4/10)\t Batch(400/1687): \tTrain Loss  0.1617\n",
            "Epoch(4/10)\t Batch(600/1687): \tTrain Loss  0.1456\n",
            "Epoch(4/10)\t Batch(800/1687): \tTrain Loss  0.1714\n",
            "Epoch(4/10)\t Batch(1000/1687): \tTrain Loss  0.1535\n",
            "Epoch(4/10)\t Batch(1200/1687): \tTrain Loss  0.1643\n",
            "Epoch(4/10)\t Batch(1400/1687): \tTrain Loss  0.1397\n",
            "Epoch(4/10)\t Batch(1600/1687): \tTrain Loss  0.1578\n",
            "End of Epoch: 4 Train Accuracy: 0.1413 Val Loss: 0.1535 Val Accuracy: 0.1358\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/1687): \tTrain Loss  0.1460\n",
            "Epoch(5/10)\t Batch(200/1687): \tTrain Loss  0.1673\n",
            "Epoch(5/10)\t Batch(400/1687): \tTrain Loss  0.1425\n",
            "Epoch(5/10)\t Batch(600/1687): \tTrain Loss  0.1574\n",
            "Epoch(5/10)\t Batch(800/1687): \tTrain Loss  0.1522\n",
            "Epoch(5/10)\t Batch(1000/1687): \tTrain Loss  0.1532\n",
            "Epoch(5/10)\t Batch(1200/1687): \tTrain Loss  0.1638\n",
            "Epoch(5/10)\t Batch(1400/1687): \tTrain Loss  0.1337\n",
            "Epoch(5/10)\t Batch(1600/1687): \tTrain Loss  0.1635\n",
            "End of Epoch: 5 Train Accuracy: 0.1525 Val Loss: 0.1506 Val Accuracy: 0.1470\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/1687): \tTrain Loss  0.1748\n",
            "Epoch(6/10)\t Batch(200/1687): \tTrain Loss  0.1279\n",
            "Epoch(6/10)\t Batch(400/1687): \tTrain Loss  0.1329\n",
            "Epoch(6/10)\t Batch(600/1687): \tTrain Loss  0.1410\n",
            "Epoch(6/10)\t Batch(800/1687): \tTrain Loss  0.1558\n",
            "Epoch(6/10)\t Batch(1000/1687): \tTrain Loss  0.1312\n",
            "Epoch(6/10)\t Batch(1200/1687): \tTrain Loss  0.1542\n",
            "Epoch(6/10)\t Batch(1400/1687): \tTrain Loss  0.1570\n",
            "Epoch(6/10)\t Batch(1600/1687): \tTrain Loss  0.1468\n",
            "End of Epoch: 6 Train Accuracy: 0.1660 Val Loss: 0.1492 Val Accuracy: 0.1563\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/1687): \tTrain Loss  0.1443\n",
            "Epoch(7/10)\t Batch(200/1687): \tTrain Loss  0.1283\n",
            "Epoch(7/10)\t Batch(400/1687): \tTrain Loss  0.1432\n",
            "Epoch(7/10)\t Batch(600/1687): \tTrain Loss  0.1652\n",
            "Epoch(7/10)\t Batch(800/1687): \tTrain Loss  0.1396\n",
            "Epoch(7/10)\t Batch(1000/1687): \tTrain Loss  0.1536\n",
            "Epoch(7/10)\t Batch(1200/1687): \tTrain Loss  0.1448\n",
            "Epoch(7/10)\t Batch(1400/1687): \tTrain Loss  0.1360\n",
            "Epoch(7/10)\t Batch(1600/1687): \tTrain Loss  0.1512\n",
            "End of Epoch: 7 Train Accuracy: 0.1911 Val Loss: 0.1437 Val Accuracy: 0.1837\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/1687): \tTrain Loss  0.1517\n",
            "Epoch(8/10)\t Batch(200/1687): \tTrain Loss  0.1505\n",
            "Epoch(8/10)\t Batch(400/1687): \tTrain Loss  0.1201\n",
            "Epoch(8/10)\t Batch(600/1687): \tTrain Loss  0.1407\n",
            "Epoch(8/10)\t Batch(800/1687): \tTrain Loss  0.1574\n",
            "Epoch(8/10)\t Batch(1000/1687): \tTrain Loss  0.1647\n",
            "Epoch(8/10)\t Batch(1200/1687): \tTrain Loss  0.1409\n",
            "Epoch(8/10)\t Batch(1400/1687): \tTrain Loss  0.1305\n",
            "Epoch(8/10)\t Batch(1600/1687): \tTrain Loss  0.1519\n",
            "End of Epoch: 8 Train Accuracy: 0.2096 Val Loss: 0.1402 Val Accuracy: 0.2027\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/1687): \tTrain Loss  0.1490\n",
            "Epoch(9/10)\t Batch(200/1687): \tTrain Loss  0.1358\n",
            "Epoch(9/10)\t Batch(400/1687): \tTrain Loss  0.1386\n",
            "Epoch(9/10)\t Batch(600/1687): \tTrain Loss  0.1376\n",
            "Epoch(9/10)\t Batch(800/1687): \tTrain Loss  0.1145\n",
            "Epoch(9/10)\t Batch(1000/1687): \tTrain Loss  0.1480\n",
            "Epoch(9/10)\t Batch(1200/1687): \tTrain Loss  0.1407\n",
            "Epoch(9/10)\t Batch(1400/1687): \tTrain Loss  0.1513\n",
            "Epoch(9/10)\t Batch(1600/1687): \tTrain Loss  0.1295\n",
            "End of Epoch: 9 Train Accuracy: 0.2361 Val Loss: 0.1355 Val Accuracy: 0.2328\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/1687): \tTrain Loss  0.1456\n",
            "Epoch(10/10)\t Batch(200/1687): \tTrain Loss  0.1232\n",
            "Epoch(10/10)\t Batch(400/1687): \tTrain Loss  0.1358\n",
            "Epoch(10/10)\t Batch(600/1687): \tTrain Loss  0.1193\n",
            "Epoch(10/10)\t Batch(800/1687): \tTrain Loss  0.1360\n",
            "Epoch(10/10)\t Batch(1000/1687): \tTrain Loss  0.1590\n",
            "Epoch(10/10)\t Batch(1200/1687): \tTrain Loss  0.1235\n",
            "Epoch(10/10)\t Batch(1400/1687): \tTrain Loss  0.1342\n",
            "Epoch(10/10)\t Batch(1600/1687): \tTrain Loss  0.1395\n",
            "End of Epoch: 10 Train Accuracy: 0.2631 Val Loss: 0.1318 Val Accuracy: 0.2515\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▃▃▄▄▅▆▇█</td></tr><tr><td>Train Loss</td><td>▃▆█▂▅▂▆▁▂▇</td></tr><tr><td>Val Accuracy</td><td>▁▂▂▂▃▃▄▅▆▇█</td></tr><tr><td>Val Loss</td><td>█▇▆▆▆▅▅▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.26309</td></tr><tr><td>Train Loss</td><td>0.15712</td></tr><tr><td>Val Accuracy</td><td>0.2515</td></tr><tr><td>Val Loss</td><td>0.13181</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nag|loss_mse|lr=0.0001|batch_32|act_tanh|hid_5|neurons_64|nrns_10|init_Random445</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/zhsf5qh0' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/zhsf5qh0</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_163810-zhsf5qh0/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: teeddoaz with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: momentum\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_164005-teeddoaz</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/teeddoaz' target=\"_blank\">super-sweep-7</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/teeddoaz' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/teeddoaz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.1067 Val Loss: 0.1781 Val Accuracy: 0.1095\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/843): \tTrain Loss  0.1687\n",
            "Epoch(1/5)\t Batch(200/843): \tTrain Loss  0.1875\n",
            "Epoch(1/5)\t Batch(400/843): \tTrain Loss  0.1531\n",
            "Epoch(1/5)\t Batch(600/843): \tTrain Loss  0.1719\n",
            "Epoch(1/5)\t Batch(800/843): \tTrain Loss  0.1812\n",
            "End of Epoch: 1 Train Accuracy: 0.1066 Val Loss: 0.1781 Val Accuracy: 0.1095\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/843): \tTrain Loss  0.1656\n",
            "Epoch(2/5)\t Batch(200/843): \tTrain Loss  0.1844\n",
            "Epoch(2/5)\t Batch(400/843): \tTrain Loss  0.1812\n",
            "Epoch(2/5)\t Batch(600/843): \tTrain Loss  0.1844\n",
            "Epoch(2/5)\t Batch(800/843): \tTrain Loss  0.1812\n",
            "End of Epoch: 2 Train Accuracy: 0.0998 Val Loss: 0.1796 Val Accuracy: 0.1022\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/843): \tTrain Loss  0.1812\n",
            "Epoch(3/5)\t Batch(200/843): \tTrain Loss  0.1875\n",
            "Epoch(3/5)\t Batch(400/843): \tTrain Loss  0.1844\n",
            "Epoch(3/5)\t Batch(600/843): \tTrain Loss  0.1844\n",
            "Epoch(3/5)\t Batch(800/843): \tTrain Loss  0.1750\n",
            "End of Epoch: 3 Train Accuracy: 0.0998 Val Loss: 0.1796 Val Accuracy: 0.1022\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/843): \tTrain Loss  0.1719\n",
            "Epoch(4/5)\t Batch(200/843): \tTrain Loss  0.1844\n",
            "Epoch(4/5)\t Batch(400/843): \tTrain Loss  0.1781\n",
            "Epoch(4/5)\t Batch(600/843): \tTrain Loss  0.1875\n",
            "Epoch(4/5)\t Batch(800/843): \tTrain Loss  0.1812\n",
            "End of Epoch: 4 Train Accuracy: 0.0998 Val Loss: 0.1796 Val Accuracy: 0.1022\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/843): \tTrain Loss  0.1875\n",
            "Epoch(5/5)\t Batch(200/843): \tTrain Loss  0.1750\n",
            "Epoch(5/5)\t Batch(400/843): \tTrain Loss  0.1781\n",
            "Epoch(5/5)\t Batch(600/843): \tTrain Loss  0.1719\n",
            "Epoch(5/5)\t Batch(800/843): \tTrain Loss  0.1750\n",
            "End of Epoch: 5 Train Accuracy: 0.0998 Val Loss: 0.1796 Val Accuracy: 0.1022\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>██▁▁▁▁</td></tr><tr><td>Train Loss</td><td>▁▇█▄▄</td></tr><tr><td>Val Accuracy</td><td>██▁▁▁▁</td></tr><tr><td>Val Loss</td><td>▁▁████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.09976</td></tr><tr><td>Train Loss</td><td>0.17917</td></tr><tr><td>Val Accuracy</td><td>0.10217</td></tr><tr><td>Val Loss</td><td>0.17957</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_momentum|loss_mse|lr=0.0001|batch_64|act_relu|hid_5|neurons_128|nrns_5|init_Random721</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/teeddoaz' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/teeddoaz</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_164005-teeddoaz/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0hb5lc3z with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: momentum\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_164101-0hb5lc3z</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/0hb5lc3z' target=\"_blank\">sandy-sweep-8</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/0hb5lc3z' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/0hb5lc3z</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.0910 Val Loss: 0.1669 Val Accuracy: 0.0920\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/843): \tTrain Loss  0.1570\n",
            "Epoch(1/5)\t Batch(200/843): \tTrain Loss  0.1551\n",
            "Epoch(1/5)\t Batch(400/843): \tTrain Loss  0.1761\n",
            "Epoch(1/5)\t Batch(600/843): \tTrain Loss  0.1667\n",
            "Epoch(1/5)\t Batch(800/843): \tTrain Loss  0.1701\n",
            "End of Epoch: 1 Train Accuracy: 0.1016 Val Loss: 0.1656 Val Accuracy: 0.0947\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/843): \tTrain Loss  0.1491\n",
            "Epoch(2/5)\t Batch(200/843): \tTrain Loss  0.1568\n",
            "Epoch(2/5)\t Batch(400/843): \tTrain Loss  0.1533\n",
            "Epoch(2/5)\t Batch(600/843): \tTrain Loss  0.1603\n",
            "Epoch(2/5)\t Batch(800/843): \tTrain Loss  0.1681\n",
            "End of Epoch: 2 Train Accuracy: 0.1093 Val Loss: 0.1638 Val Accuracy: 0.1015\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/843): \tTrain Loss  0.1523\n",
            "Epoch(3/5)\t Batch(200/843): \tTrain Loss  0.1679\n",
            "Epoch(3/5)\t Batch(400/843): \tTrain Loss  0.1626\n",
            "Epoch(3/5)\t Batch(600/843): \tTrain Loss  0.1647\n",
            "Epoch(3/5)\t Batch(800/843): \tTrain Loss  0.1721\n",
            "End of Epoch: 3 Train Accuracy: 0.1166 Val Loss: 0.1624 Val Accuracy: 0.1023\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/843): \tTrain Loss  0.1614\n",
            "Epoch(4/5)\t Batch(200/843): \tTrain Loss  0.1606\n",
            "Epoch(4/5)\t Batch(400/843): \tTrain Loss  0.1642\n",
            "Epoch(4/5)\t Batch(600/843): \tTrain Loss  0.1475\n",
            "Epoch(4/5)\t Batch(800/843): \tTrain Loss  0.1528\n",
            "End of Epoch: 4 Train Accuracy: 0.1221 Val Loss: 0.1604 Val Accuracy: 0.1113\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/843): \tTrain Loss  0.1562\n",
            "Epoch(5/5)\t Batch(200/843): \tTrain Loss  0.1616\n",
            "Epoch(5/5)\t Batch(400/843): \tTrain Loss  0.1749\n",
            "Epoch(5/5)\t Batch(600/843): \tTrain Loss  0.1654\n",
            "Epoch(5/5)\t Batch(800/843): \tTrain Loss  0.1737\n",
            "End of Epoch: 5 Train Accuracy: 0.1315 Val Loss: 0.1581 Val Accuracy: 0.1173\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▃▄▅▆█</td></tr><tr><td>Train Loss</td><td>▇▁▂█▂</td></tr><tr><td>Val Accuracy</td><td>▁▂▄▄▆█</td></tr><tr><td>Val Loss</td><td>█▇▆▄▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.13146</td></tr><tr><td>Train Loss</td><td>0.15955</td></tr><tr><td>Val Accuracy</td><td>0.11733</td></tr><tr><td>Val Loss</td><td>0.15809</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_momentum|loss_mse|lr=0.0001|batch_64|act_tanh|hid_4|neurons_128|nrns_5|init_Random140</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/0hb5lc3z' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/0hb5lc3z</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_164101-0hb5lc3z/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0uewsjdp with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_164217-0uewsjdp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/0uewsjdp' target=\"_blank\">dauntless-sweep-9</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/0uewsjdp' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/0uewsjdp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0711 Val Loss: 2.3086 Val Accuracy: 0.0708\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/843): \tTrain Loss  2.3047\n",
            "Epoch(1/5)\t Batch(200/843): \tTrain Loss  1.9427\n",
            "Epoch(1/5)\t Batch(400/843): \tTrain Loss  1.8840\n",
            "Epoch(1/5)\t Batch(600/843): \tTrain Loss  1.8255\n",
            "Epoch(1/5)\t Batch(800/843): \tTrain Loss  1.8269\n",
            "End of Epoch: 1 Train Accuracy: 0.8011 Val Loss: 1.8057 Val Accuracy: 0.8022\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/843): \tTrain Loss  1.8183\n",
            "Epoch(2/5)\t Batch(200/843): \tTrain Loss  1.8538\n",
            "Epoch(2/5)\t Batch(400/843): \tTrain Loss  1.7787\n",
            "Epoch(2/5)\t Batch(600/843): \tTrain Loss  1.7522\n",
            "Epoch(2/5)\t Batch(800/843): \tTrain Loss  1.7255\n",
            "End of Epoch: 2 Train Accuracy: 0.8207 Val Loss: 1.7668 Val Accuracy: 0.8235\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/843): \tTrain Loss  1.7830\n",
            "Epoch(3/5)\t Batch(200/843): \tTrain Loss  1.7876\n",
            "Epoch(3/5)\t Batch(400/843): \tTrain Loss  1.7468\n",
            "Epoch(3/5)\t Batch(600/843): \tTrain Loss  1.7378\n",
            "Epoch(3/5)\t Batch(800/843): \tTrain Loss  1.7906\n",
            "End of Epoch: 3 Train Accuracy: 0.8306 Val Loss: 1.7527 Val Accuracy: 0.8357\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/843): \tTrain Loss  1.7394\n",
            "Epoch(4/5)\t Batch(200/843): \tTrain Loss  1.7372\n",
            "Epoch(4/5)\t Batch(400/843): \tTrain Loss  1.7845\n",
            "Epoch(4/5)\t Batch(600/843): \tTrain Loss  1.7166\n",
            "Epoch(4/5)\t Batch(800/843): \tTrain Loss  1.7467\n",
            "End of Epoch: 4 Train Accuracy: 0.8389 Val Loss: 1.7433 Val Accuracy: 0.8437\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/843): \tTrain Loss  1.7127\n",
            "Epoch(5/5)\t Batch(200/843): \tTrain Loss  1.6971\n",
            "Epoch(5/5)\t Batch(400/843): \tTrain Loss  1.7097\n",
            "Epoch(5/5)\t Batch(600/843): \tTrain Loss  1.6838\n",
            "Epoch(5/5)\t Batch(800/843): \tTrain Loss  1.7613\n",
            "End of Epoch: 5 Train Accuracy: 0.8427 Val Loss: 1.7374 Val Accuracy: 0.8485\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁█████</td></tr><tr><td>Train Loss</td><td>█▆▂▁▃</td></tr><tr><td>Val Accuracy</td><td>▁█████</td></tr><tr><td>Val Loss</td><td>█▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.8427</td></tr><tr><td>Train Loss</td><td>1.71894</td></tr><tr><td>Val Accuracy</td><td>0.8485</td></tr><tr><td>Val Loss</td><td>1.73739</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_adam|loss_cross_entropy|lr=0.0001|batch_64|act_relu|hid_4|neurons_128|nrns_5|init_Xavier403</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/0uewsjdp' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/0uewsjdp</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_164217-0uewsjdp/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: l3zptbt0 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_164312-l3zptbt0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/l3zptbt0' target=\"_blank\">skilled-sweep-10</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/l3zptbt0' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/l3zptbt0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0954 Val Loss: 0.1816 Val Accuracy: 0.0920\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/843): \tTrain Loss  0.1656\n",
            "Epoch(1/5)\t Batch(200/843): \tTrain Loss  0.1844\n",
            "Epoch(1/5)\t Batch(400/843): \tTrain Loss  0.1844\n",
            "Epoch(1/5)\t Batch(600/843): \tTrain Loss  0.1812\n",
            "Epoch(1/5)\t Batch(800/843): \tTrain Loss  0.1844\n",
            "End of Epoch: 1 Train Accuracy: 0.0952 Val Loss: 0.1816 Val Accuracy: 0.0922\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/843): \tTrain Loss  0.1750\n",
            "Epoch(2/5)\t Batch(200/843): \tTrain Loss  0.1844\n",
            "Epoch(2/5)\t Batch(400/843): \tTrain Loss  0.1812\n",
            "Epoch(2/5)\t Batch(600/843): \tTrain Loss  0.1812\n",
            "Epoch(2/5)\t Batch(800/843): \tTrain Loss  0.1812\n",
            "End of Epoch: 2 Train Accuracy: 0.0969 Val Loss: 0.1815 Val Accuracy: 0.0927\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/843): \tTrain Loss  0.1781\n",
            "Epoch(3/5)\t Batch(200/843): \tTrain Loss  0.1656\n",
            "Epoch(3/5)\t Batch(400/843): \tTrain Loss  0.1812\n",
            "Epoch(3/5)\t Batch(600/843): \tTrain Loss  0.1750\n",
            "Epoch(3/5)\t Batch(800/843): \tTrain Loss  0.1812\n",
            "End of Epoch: 3 Train Accuracy: 0.0956 Val Loss: 0.1815 Val Accuracy: 0.0925\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/843): \tTrain Loss  0.1750\n",
            "Epoch(4/5)\t Batch(200/843): \tTrain Loss  0.1781\n",
            "Epoch(4/5)\t Batch(400/843): \tTrain Loss  0.1844\n",
            "Epoch(4/5)\t Batch(600/843): \tTrain Loss  0.1844\n",
            "Epoch(4/5)\t Batch(800/843): \tTrain Loss  0.1781\n",
            "End of Epoch: 4 Train Accuracy: 0.0976 Val Loss: 0.1813 Val Accuracy: 0.0933\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/843): \tTrain Loss  0.1844\n",
            "Epoch(5/5)\t Batch(200/843): \tTrain Loss  0.1750\n",
            "Epoch(5/5)\t Batch(400/843): \tTrain Loss  0.1875\n",
            "Epoch(5/5)\t Batch(600/843): \tTrain Loss  0.1906\n",
            "Epoch(5/5)\t Batch(800/843): \tTrain Loss  0.1719\n",
            "End of Epoch: 5 Train Accuracy: 0.0985 Val Loss: 0.1810 Val Accuracy: 0.0952\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▁▅▂▆█</td></tr><tr><td>Train Loss</td><td>▃▁▃▄█</td></tr><tr><td>Val Accuracy</td><td>▁▁▂▂▄█</td></tr><tr><td>Val Loss</td><td>██▆▇▅▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.0985</td></tr><tr><td>Train Loss</td><td>0.19167</td></tr><tr><td>Val Accuracy</td><td>0.09517</td></tr><tr><td>Val Loss</td><td>0.18097</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_sgd|loss_mse|lr=0.0001|batch_64|act_relu|hid_4|neurons_64|nrns_5|init_Random819</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/l3zptbt0' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/l3zptbt0</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_164312-l3zptbt0/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 34fcolip with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_164348-34fcolip</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/34fcolip' target=\"_blank\">pious-sweep-11</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/34fcolip' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/34fcolip</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.0853 Val Loss: 0.1624 Val Accuracy: 0.0820\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/3375): \tTrain Loss  0.1410\n",
            "Epoch(1/10)\t Batch(200/3375): \tTrain Loss  0.0717\n",
            "Epoch(1/10)\t Batch(400/3375): \tTrain Loss  0.0542\n",
            "Epoch(1/10)\t Batch(600/3375): \tTrain Loss  0.0750\n",
            "Epoch(1/10)\t Batch(800/3375): \tTrain Loss  0.0489\n",
            "Epoch(1/10)\t Batch(1000/3375): \tTrain Loss  0.0545\n",
            "Epoch(1/10)\t Batch(1200/3375): \tTrain Loss  0.0291\n",
            "Epoch(1/10)\t Batch(1400/3375): \tTrain Loss  0.0290\n",
            "Epoch(1/10)\t Batch(1600/3375): \tTrain Loss  0.0237\n",
            "Epoch(1/10)\t Batch(1800/3375): \tTrain Loss  0.0493\n",
            "Epoch(1/10)\t Batch(2000/3375): \tTrain Loss  0.0258\n",
            "Epoch(1/10)\t Batch(2200/3375): \tTrain Loss  0.0603\n",
            "Epoch(1/10)\t Batch(2400/3375): \tTrain Loss  0.0518\n",
            "Epoch(1/10)\t Batch(2600/3375): \tTrain Loss  0.0306\n",
            "Epoch(1/10)\t Batch(2800/3375): \tTrain Loss  0.0396\n",
            "Epoch(1/10)\t Batch(3000/3375): \tTrain Loss  0.0313\n",
            "Epoch(1/10)\t Batch(3200/3375): \tTrain Loss  0.0350\n",
            "End of Epoch: 1 Train Accuracy: 0.7445 Val Loss: 0.0345 Val Accuracy: 0.7495\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/3375): \tTrain Loss  0.0262\n",
            "Epoch(2/10)\t Batch(200/3375): \tTrain Loss  0.0313\n",
            "Epoch(2/10)\t Batch(400/3375): \tTrain Loss  0.0379\n",
            "Epoch(2/10)\t Batch(600/3375): \tTrain Loss  0.0401\n",
            "Epoch(2/10)\t Batch(800/3375): \tTrain Loss  0.0435\n",
            "Epoch(2/10)\t Batch(1000/3375): \tTrain Loss  0.0191\n",
            "Epoch(2/10)\t Batch(1200/3375): \tTrain Loss  0.0229\n",
            "Epoch(2/10)\t Batch(1400/3375): \tTrain Loss  0.0269\n",
            "Epoch(2/10)\t Batch(1600/3375): \tTrain Loss  0.0250\n",
            "Epoch(2/10)\t Batch(1800/3375): \tTrain Loss  0.0170\n",
            "Epoch(2/10)\t Batch(2000/3375): \tTrain Loss  0.0179\n",
            "Epoch(2/10)\t Batch(2200/3375): \tTrain Loss  0.0507\n",
            "Epoch(2/10)\t Batch(2400/3375): \tTrain Loss  0.0414\n",
            "Epoch(2/10)\t Batch(2600/3375): \tTrain Loss  0.0375\n",
            "Epoch(2/10)\t Batch(2800/3375): \tTrain Loss  0.0329\n",
            "Epoch(2/10)\t Batch(3000/3375): \tTrain Loss  0.0253\n",
            "Epoch(2/10)\t Batch(3200/3375): \tTrain Loss  0.0295\n",
            "End of Epoch: 2 Train Accuracy: 0.7662 Val Loss: 0.0322 Val Accuracy: 0.7615\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/3375): \tTrain Loss  0.0264\n",
            "Epoch(3/10)\t Batch(200/3375): \tTrain Loss  0.0146\n",
            "Epoch(3/10)\t Batch(400/3375): \tTrain Loss  0.0208\n",
            "Epoch(3/10)\t Batch(600/3375): \tTrain Loss  0.0336\n",
            "Epoch(3/10)\t Batch(800/3375): \tTrain Loss  0.0502\n",
            "Epoch(3/10)\t Batch(1000/3375): \tTrain Loss  0.0348\n",
            "Epoch(3/10)\t Batch(1200/3375): \tTrain Loss  0.0429\n",
            "Epoch(3/10)\t Batch(1400/3375): \tTrain Loss  0.0560\n",
            "Epoch(3/10)\t Batch(1600/3375): \tTrain Loss  0.0475\n",
            "Epoch(3/10)\t Batch(1800/3375): \tTrain Loss  0.0457\n",
            "Epoch(3/10)\t Batch(2000/3375): \tTrain Loss  0.0199\n",
            "Epoch(3/10)\t Batch(2200/3375): \tTrain Loss  0.0281\n",
            "Epoch(3/10)\t Batch(2400/3375): \tTrain Loss  0.0366\n",
            "Epoch(3/10)\t Batch(2600/3375): \tTrain Loss  0.0046\n",
            "Epoch(3/10)\t Batch(2800/3375): \tTrain Loss  0.0343\n",
            "Epoch(3/10)\t Batch(3000/3375): \tTrain Loss  0.0120\n",
            "Epoch(3/10)\t Batch(3200/3375): \tTrain Loss  0.0289\n",
            "End of Epoch: 3 Train Accuracy: 0.7786 Val Loss: 0.0314 Val Accuracy: 0.7723\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/3375): \tTrain Loss  0.0324\n",
            "Epoch(4/10)\t Batch(200/3375): \tTrain Loss  0.0287\n",
            "Epoch(4/10)\t Batch(400/3375): \tTrain Loss  0.0323\n",
            "Epoch(4/10)\t Batch(600/3375): \tTrain Loss  0.0259\n",
            "Epoch(4/10)\t Batch(800/3375): \tTrain Loss  0.0370\n",
            "Epoch(4/10)\t Batch(1000/3375): \tTrain Loss  0.0076\n",
            "Epoch(4/10)\t Batch(1200/3375): \tTrain Loss  0.0347\n",
            "Epoch(4/10)\t Batch(1400/3375): \tTrain Loss  0.0224\n",
            "Epoch(4/10)\t Batch(1600/3375): \tTrain Loss  0.0284\n",
            "Epoch(4/10)\t Batch(1800/3375): \tTrain Loss  0.0238\n",
            "Epoch(4/10)\t Batch(2000/3375): \tTrain Loss  0.0308\n",
            "Epoch(4/10)\t Batch(2200/3375): \tTrain Loss  0.0452\n",
            "Epoch(4/10)\t Batch(2400/3375): \tTrain Loss  0.0382\n",
            "Epoch(4/10)\t Batch(2600/3375): \tTrain Loss  0.0324\n",
            "Epoch(4/10)\t Batch(2800/3375): \tTrain Loss  0.0481\n",
            "Epoch(4/10)\t Batch(3000/3375): \tTrain Loss  0.0283\n",
            "Epoch(4/10)\t Batch(3200/3375): \tTrain Loss  0.0529\n",
            "End of Epoch: 4 Train Accuracy: 0.7919 Val Loss: 0.0295 Val Accuracy: 0.7852\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/3375): \tTrain Loss  0.0129\n",
            "Epoch(5/10)\t Batch(200/3375): \tTrain Loss  0.0274\n",
            "Epoch(5/10)\t Batch(400/3375): \tTrain Loss  0.0250\n",
            "Epoch(5/10)\t Batch(600/3375): \tTrain Loss  0.0091\n",
            "Epoch(5/10)\t Batch(800/3375): \tTrain Loss  0.0231\n",
            "Epoch(5/10)\t Batch(1000/3375): \tTrain Loss  0.0093\n",
            "Epoch(5/10)\t Batch(1200/3375): \tTrain Loss  0.0323\n",
            "Epoch(5/10)\t Batch(1400/3375): \tTrain Loss  0.0374\n",
            "Epoch(5/10)\t Batch(1600/3375): \tTrain Loss  0.0388\n",
            "Epoch(5/10)\t Batch(1800/3375): \tTrain Loss  0.0332\n",
            "Epoch(5/10)\t Batch(2000/3375): \tTrain Loss  0.0214\n",
            "Epoch(5/10)\t Batch(2200/3375): \tTrain Loss  0.0393\n",
            "Epoch(5/10)\t Batch(2400/3375): \tTrain Loss  0.0333\n",
            "Epoch(5/10)\t Batch(2600/3375): \tTrain Loss  0.0119\n",
            "Epoch(5/10)\t Batch(2800/3375): \tTrain Loss  0.0041\n",
            "Epoch(5/10)\t Batch(3000/3375): \tTrain Loss  0.0214\n",
            "Epoch(5/10)\t Batch(3200/3375): \tTrain Loss  0.0311\n",
            "End of Epoch: 5 Train Accuracy: 0.7971 Val Loss: 0.0292 Val Accuracy: 0.7860\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/3375): \tTrain Loss  0.0250\n",
            "Epoch(6/10)\t Batch(200/3375): \tTrain Loss  0.0080\n",
            "Epoch(6/10)\t Batch(400/3375): \tTrain Loss  0.0641\n",
            "Epoch(6/10)\t Batch(600/3375): \tTrain Loss  0.0588\n",
            "Epoch(6/10)\t Batch(800/3375): \tTrain Loss  0.0132\n",
            "Epoch(6/10)\t Batch(1000/3375): \tTrain Loss  0.0446\n",
            "Epoch(6/10)\t Batch(1200/3375): \tTrain Loss  0.0267\n",
            "Epoch(6/10)\t Batch(1400/3375): \tTrain Loss  0.0149\n",
            "Epoch(6/10)\t Batch(1600/3375): \tTrain Loss  0.0254\n",
            "Epoch(6/10)\t Batch(1800/3375): \tTrain Loss  0.0141\n",
            "Epoch(6/10)\t Batch(2000/3375): \tTrain Loss  0.0125\n",
            "Epoch(6/10)\t Batch(2200/3375): \tTrain Loss  0.0149\n",
            "Epoch(6/10)\t Batch(2400/3375): \tTrain Loss  0.0381\n",
            "Epoch(6/10)\t Batch(2600/3375): \tTrain Loss  0.0028\n",
            "Epoch(6/10)\t Batch(2800/3375): \tTrain Loss  0.0100\n",
            "Epoch(6/10)\t Batch(3000/3375): \tTrain Loss  0.0298\n",
            "Epoch(6/10)\t Batch(3200/3375): \tTrain Loss  0.0080\n",
            "End of Epoch: 6 Train Accuracy: 0.8679 Val Loss: 0.0208 Val Accuracy: 0.8567\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/3375): \tTrain Loss  0.0402\n",
            "Epoch(7/10)\t Batch(200/3375): \tTrain Loss  0.0228\n",
            "Epoch(7/10)\t Batch(400/3375): \tTrain Loss  0.0399\n",
            "Epoch(7/10)\t Batch(600/3375): \tTrain Loss  0.0241\n",
            "Epoch(7/10)\t Batch(800/3375): \tTrain Loss  0.0012\n",
            "Epoch(7/10)\t Batch(1000/3375): \tTrain Loss  0.0439\n",
            "Epoch(7/10)\t Batch(1200/3375): \tTrain Loss  0.0151\n",
            "Epoch(7/10)\t Batch(1400/3375): \tTrain Loss  0.0193\n",
            "Epoch(7/10)\t Batch(1600/3375): \tTrain Loss  0.0276\n",
            "Epoch(7/10)\t Batch(1800/3375): \tTrain Loss  0.0360\n",
            "Epoch(7/10)\t Batch(2000/3375): \tTrain Loss  0.0241\n",
            "Epoch(7/10)\t Batch(2200/3375): \tTrain Loss  0.0089\n",
            "Epoch(7/10)\t Batch(2400/3375): \tTrain Loss  0.0170\n",
            "Epoch(7/10)\t Batch(2600/3375): \tTrain Loss  0.0412\n",
            "Epoch(7/10)\t Batch(2800/3375): \tTrain Loss  0.0240\n",
            "Epoch(7/10)\t Batch(3000/3375): \tTrain Loss  0.0103\n",
            "Epoch(7/10)\t Batch(3200/3375): \tTrain Loss  0.0185\n",
            "End of Epoch: 7 Train Accuracy: 0.8697 Val Loss: 0.0205 Val Accuracy: 0.8602\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/3375): \tTrain Loss  0.0335\n",
            "Epoch(8/10)\t Batch(200/3375): \tTrain Loss  0.0091\n",
            "Epoch(8/10)\t Batch(400/3375): \tTrain Loss  0.0166\n",
            "Epoch(8/10)\t Batch(600/3375): \tTrain Loss  0.0384\n",
            "Epoch(8/10)\t Batch(800/3375): \tTrain Loss  0.0319\n",
            "Epoch(8/10)\t Batch(1000/3375): \tTrain Loss  0.0293\n",
            "Epoch(8/10)\t Batch(1200/3375): \tTrain Loss  0.0102\n",
            "Epoch(8/10)\t Batch(1400/3375): \tTrain Loss  0.0083\n",
            "Epoch(8/10)\t Batch(1600/3375): \tTrain Loss  0.0142\n",
            "Epoch(8/10)\t Batch(1800/3375): \tTrain Loss  0.0081\n",
            "Epoch(8/10)\t Batch(2000/3375): \tTrain Loss  0.0169\n",
            "Epoch(8/10)\t Batch(2200/3375): \tTrain Loss  0.0322\n",
            "Epoch(8/10)\t Batch(2400/3375): \tTrain Loss  0.0144\n",
            "Epoch(8/10)\t Batch(2600/3375): \tTrain Loss  0.0269\n",
            "Epoch(8/10)\t Batch(2800/3375): \tTrain Loss  0.0189\n",
            "Epoch(8/10)\t Batch(3000/3375): \tTrain Loss  0.0267\n",
            "Epoch(8/10)\t Batch(3200/3375): \tTrain Loss  0.0520\n",
            "End of Epoch: 8 Train Accuracy: 0.8771 Val Loss: 0.0200 Val Accuracy: 0.8622\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/3375): \tTrain Loss  0.0210\n",
            "Epoch(9/10)\t Batch(200/3375): \tTrain Loss  0.0047\n",
            "Epoch(9/10)\t Batch(400/3375): \tTrain Loss  0.0126\n",
            "Epoch(9/10)\t Batch(600/3375): \tTrain Loss  0.0311\n",
            "Epoch(9/10)\t Batch(800/3375): \tTrain Loss  0.0323\n",
            "Epoch(9/10)\t Batch(1000/3375): \tTrain Loss  0.0143\n",
            "Epoch(9/10)\t Batch(1200/3375): \tTrain Loss  0.0182\n",
            "Epoch(9/10)\t Batch(1400/3375): \tTrain Loss  0.0179\n",
            "Epoch(9/10)\t Batch(1600/3375): \tTrain Loss  0.0382\n",
            "Epoch(9/10)\t Batch(1800/3375): \tTrain Loss  0.0413\n",
            "Epoch(9/10)\t Batch(2000/3375): \tTrain Loss  0.0221\n",
            "Epoch(9/10)\t Batch(2200/3375): \tTrain Loss  0.0071\n",
            "Epoch(9/10)\t Batch(2400/3375): \tTrain Loss  0.0078\n",
            "Epoch(9/10)\t Batch(2600/3375): \tTrain Loss  0.0176\n",
            "Epoch(9/10)\t Batch(2800/3375): \tTrain Loss  0.0119\n",
            "Epoch(9/10)\t Batch(3000/3375): \tTrain Loss  0.0419\n",
            "Epoch(9/10)\t Batch(3200/3375): \tTrain Loss  0.0267\n",
            "End of Epoch: 9 Train Accuracy: 0.8861 Val Loss: 0.0192 Val Accuracy: 0.8675\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/3375): \tTrain Loss  0.0221\n",
            "Epoch(10/10)\t Batch(200/3375): \tTrain Loss  0.0046\n",
            "Epoch(10/10)\t Batch(400/3375): \tTrain Loss  0.0002\n",
            "Epoch(10/10)\t Batch(600/3375): \tTrain Loss  0.0215\n",
            "Epoch(10/10)\t Batch(800/3375): \tTrain Loss  0.0068\n",
            "Epoch(10/10)\t Batch(1000/3375): \tTrain Loss  0.0125\n",
            "Epoch(10/10)\t Batch(1200/3375): \tTrain Loss  0.0222\n",
            "Epoch(10/10)\t Batch(1400/3375): \tTrain Loss  0.0132\n",
            "Epoch(10/10)\t Batch(1600/3375): \tTrain Loss  0.0096\n",
            "Epoch(10/10)\t Batch(1800/3375): \tTrain Loss  0.0474\n",
            "Epoch(10/10)\t Batch(2000/3375): \tTrain Loss  0.0145\n",
            "Epoch(10/10)\t Batch(2200/3375): \tTrain Loss  0.0247\n",
            "Epoch(10/10)\t Batch(2400/3375): \tTrain Loss  0.0151\n",
            "Epoch(10/10)\t Batch(2600/3375): \tTrain Loss  0.0331\n",
            "Epoch(10/10)\t Batch(2800/3375): \tTrain Loss  0.0214\n",
            "Epoch(10/10)\t Batch(3000/3375): \tTrain Loss  0.0015\n",
            "Epoch(10/10)\t Batch(3200/3375): \tTrain Loss  0.0480\n",
            "End of Epoch: 10 Train Accuracy: 0.8816 Val Loss: 0.0199 Val Accuracy: 0.8657\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▇▇▇▇▇█████</td></tr><tr><td>Train Loss</td><td>█▄▅▄▃▄▄▁▃▅</td></tr><tr><td>Val Accuracy</td><td>▁▇▇▇▇▇█████</td></tr><tr><td>Val Loss</td><td>█▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.88157</td></tr><tr><td>Train Loss</td><td>0.03275</td></tr><tr><td>Val Accuracy</td><td>0.86567</td></tr><tr><td>Val Loss</td><td>0.01992</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_mse|lr=0.001|batch_16|act_sigmoid|hid_3|neurons_128|nrns_10|init_Random613</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/34fcolip' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/34fcolip</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_164348-34fcolip/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9gxbvc2p with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_164659-9gxbvc2p</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/9gxbvc2p' target=\"_blank\">fresh-sweep-12</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/9gxbvc2p' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/9gxbvc2p</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.1001 Val Loss: 6.4415 Val Accuracy: 0.0995\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/3375): \tTrain Loss  5.7622\n",
            "Epoch(1/5)\t Batch(200/3375): \tTrain Loss  2.2247\n",
            "Epoch(1/5)\t Batch(400/3375): \tTrain Loss  2.1177\n",
            "Epoch(1/5)\t Batch(600/3375): \tTrain Loss  2.1235\n",
            "Epoch(1/5)\t Batch(800/3375): \tTrain Loss  2.0871\n",
            "Epoch(1/5)\t Batch(1000/3375): \tTrain Loss  1.9489\n",
            "Epoch(1/5)\t Batch(1200/3375): \tTrain Loss  2.0762\n",
            "Epoch(1/5)\t Batch(1400/3375): \tTrain Loss  1.9594\n",
            "Epoch(1/5)\t Batch(1600/3375): \tTrain Loss  2.0568\n",
            "Epoch(1/5)\t Batch(1800/3375): \tTrain Loss  1.9368\n",
            "Epoch(1/5)\t Batch(2000/3375): \tTrain Loss  1.9977\n",
            "Epoch(1/5)\t Batch(2200/3375): \tTrain Loss  1.8686\n",
            "Epoch(1/5)\t Batch(2400/3375): \tTrain Loss  1.8904\n",
            "Epoch(1/5)\t Batch(2600/3375): \tTrain Loss  1.9298\n",
            "Epoch(1/5)\t Batch(2800/3375): \tTrain Loss  2.0118\n",
            "Epoch(1/5)\t Batch(3000/3375): \tTrain Loss  1.9099\n",
            "Epoch(1/5)\t Batch(3200/3375): \tTrain Loss  1.9344\n",
            "End of Epoch: 1 Train Accuracy: 0.7172 Val Loss: 1.9411 Val Accuracy: 0.7118\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/3375): \tTrain Loss  1.8874\n",
            "Epoch(2/5)\t Batch(200/3375): \tTrain Loss  1.9677\n",
            "Epoch(2/5)\t Batch(400/3375): \tTrain Loss  1.9484\n",
            "Epoch(2/5)\t Batch(600/3375): \tTrain Loss  1.8894\n",
            "Epoch(2/5)\t Batch(800/3375): \tTrain Loss  2.0149\n",
            "Epoch(2/5)\t Batch(1000/3375): \tTrain Loss  1.8342\n",
            "Epoch(2/5)\t Batch(1200/3375): \tTrain Loss  1.9212\n",
            "Epoch(2/5)\t Batch(1400/3375): \tTrain Loss  2.0454\n",
            "Epoch(2/5)\t Batch(1600/3375): \tTrain Loss  1.8967\n",
            "Epoch(2/5)\t Batch(1800/3375): \tTrain Loss  1.9093\n",
            "Epoch(2/5)\t Batch(2000/3375): \tTrain Loss  1.9187\n",
            "Epoch(2/5)\t Batch(2200/3375): \tTrain Loss  1.9038\n",
            "Epoch(2/5)\t Batch(2400/3375): \tTrain Loss  1.8773\n",
            "Epoch(2/5)\t Batch(2600/3375): \tTrain Loss  1.9050\n",
            "Epoch(2/5)\t Batch(2800/3375): \tTrain Loss  1.9084\n",
            "Epoch(2/5)\t Batch(3000/3375): \tTrain Loss  1.9359\n",
            "Epoch(2/5)\t Batch(3200/3375): \tTrain Loss  1.8180\n",
            "End of Epoch: 2 Train Accuracy: 0.7193 Val Loss: 1.9122 Val Accuracy: 0.7155\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/3375): \tTrain Loss  1.9857\n",
            "Epoch(3/5)\t Batch(200/3375): \tTrain Loss  1.8201\n",
            "Epoch(3/5)\t Batch(400/3375): \tTrain Loss  1.8503\n",
            "Epoch(3/5)\t Batch(600/3375): \tTrain Loss  1.8906\n",
            "Epoch(3/5)\t Batch(800/3375): \tTrain Loss  1.9906\n",
            "Epoch(3/5)\t Batch(1000/3375): \tTrain Loss  1.9434\n",
            "Epoch(3/5)\t Batch(1200/3375): \tTrain Loss  1.8825\n",
            "Epoch(3/5)\t Batch(1400/3375): \tTrain Loss  1.9787\n",
            "Epoch(3/5)\t Batch(1600/3375): \tTrain Loss  1.8780\n",
            "Epoch(3/5)\t Batch(1800/3375): \tTrain Loss  1.9232\n",
            "Epoch(3/5)\t Batch(2000/3375): \tTrain Loss  1.9541\n",
            "Epoch(3/5)\t Batch(2200/3375): \tTrain Loss  1.9418\n",
            "Epoch(3/5)\t Batch(2400/3375): \tTrain Loss  1.9220\n",
            "Epoch(3/5)\t Batch(2600/3375): \tTrain Loss  2.0543\n",
            "Epoch(3/5)\t Batch(2800/3375): \tTrain Loss  1.9602\n",
            "Epoch(3/5)\t Batch(3000/3375): \tTrain Loss  2.0266\n",
            "Epoch(3/5)\t Batch(3200/3375): \tTrain Loss  1.8039\n",
            "End of Epoch: 3 Train Accuracy: 0.6489 Val Loss: 1.9126 Val Accuracy: 0.6460\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/3375): \tTrain Loss  1.8224\n",
            "Epoch(4/5)\t Batch(200/3375): \tTrain Loss  2.0054\n",
            "Epoch(4/5)\t Batch(400/3375): \tTrain Loss  1.8624\n",
            "Epoch(4/5)\t Batch(600/3375): \tTrain Loss  1.9000\n",
            "Epoch(4/5)\t Batch(800/3375): \tTrain Loss  1.8488\n",
            "Epoch(4/5)\t Batch(1000/3375): \tTrain Loss  1.9314\n",
            "Epoch(4/5)\t Batch(1200/3375): \tTrain Loss  1.9352\n",
            "Epoch(4/5)\t Batch(1400/3375): \tTrain Loss  1.8466\n",
            "Epoch(4/5)\t Batch(1600/3375): \tTrain Loss  1.7543\n",
            "Epoch(4/5)\t Batch(1800/3375): \tTrain Loss  1.8827\n",
            "Epoch(4/5)\t Batch(2000/3375): \tTrain Loss  1.8793\n",
            "Epoch(4/5)\t Batch(2200/3375): \tTrain Loss  1.9018\n",
            "Epoch(4/5)\t Batch(2400/3375): \tTrain Loss  1.8390\n",
            "Epoch(4/5)\t Batch(2600/3375): \tTrain Loss  1.9021\n",
            "Epoch(4/5)\t Batch(2800/3375): \tTrain Loss  1.9233\n",
            "Epoch(4/5)\t Batch(3000/3375): \tTrain Loss  1.9598\n",
            "Epoch(4/5)\t Batch(3200/3375): \tTrain Loss  1.9735\n",
            "End of Epoch: 4 Train Accuracy: 0.6492 Val Loss: 1.8958 Val Accuracy: 0.6547\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/3375): \tTrain Loss  2.0230\n",
            "Epoch(5/5)\t Batch(200/3375): \tTrain Loss  1.7565\n",
            "Epoch(5/5)\t Batch(400/3375): \tTrain Loss  1.9874\n",
            "Epoch(5/5)\t Batch(600/3375): \tTrain Loss  1.8904\n",
            "Epoch(5/5)\t Batch(800/3375): \tTrain Loss  1.9036\n",
            "Epoch(5/5)\t Batch(1000/3375): \tTrain Loss  1.9653\n",
            "Epoch(5/5)\t Batch(1200/3375): \tTrain Loss  1.8952\n",
            "Epoch(5/5)\t Batch(1400/3375): \tTrain Loss  1.8963\n",
            "Epoch(5/5)\t Batch(1600/3375): \tTrain Loss  1.8969\n",
            "Epoch(5/5)\t Batch(1800/3375): \tTrain Loss  1.9111\n",
            "Epoch(5/5)\t Batch(2000/3375): \tTrain Loss  1.8780\n",
            "Epoch(5/5)\t Batch(2200/3375): \tTrain Loss  1.8097\n",
            "Epoch(5/5)\t Batch(2400/3375): \tTrain Loss  1.9666\n",
            "Epoch(5/5)\t Batch(2600/3375): \tTrain Loss  1.8023\n",
            "Epoch(5/5)\t Batch(2800/3375): \tTrain Loss  1.8276\n",
            "Epoch(5/5)\t Batch(3000/3375): \tTrain Loss  1.9817\n",
            "Epoch(5/5)\t Batch(3200/3375): \tTrain Loss  1.8770\n",
            "End of Epoch: 5 Train Accuracy: 0.6483 Val Loss: 1.9064 Val Accuracy: 0.6513\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁██▇▇▇</td></tr><tr><td>Train Loss</td><td>▆▄▇▁█</td></tr><tr><td>Val Accuracy</td><td>▁██▇▇▇</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.64826</td></tr><tr><td>Train Loss</td><td>1.98621</td></tr><tr><td>Val Accuracy</td><td>0.65133</td></tr><tr><td>Val Loss</td><td>1.90639</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_cross_entropy|lr=0.001|batch_16|act_sigmoid|hid_5|neurons_32|nrns_5|init_Random10</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/9gxbvc2p' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/9gxbvc2p</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_164659-9gxbvc2p/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: h3g2gt37 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nag\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_164739-h3g2gt37</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/h3g2gt37' target=\"_blank\">fallen-sweep-13</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/h3g2gt37' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/h3g2gt37</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0999 Val Loss: 25.0613 Val Accuracy: 0.0930\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/1687): \tTrain Loss  25.0406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-4f78d507ffd0>:34: RuntimeWarning: overflow encountered in matmul\n",
            "  self.gradd[\"weight\"] = x.T @ d_out\n",
            "<ipython-input-2-4f78d507ffd0>:36: RuntimeWarning: overflow encountered in matmul\n",
            "  dzdx = d_out@self.weight.T\n",
            "<ipython-input-2-4f78d507ffd0>:95: RuntimeWarning: invalid value encountered in multiply\n",
            "  return np.array(d_out) * dx\n",
            "<ipython-input-2-4f78d507ffd0>:34: RuntimeWarning: invalid value encountered in matmul\n",
            "  self.gradd[\"weight\"] = x.T @ d_out\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch(1/5)\t Batch(200/1687): \tTrain Loss  nan\n",
            "Epoch(1/5)\t Batch(400/1687): \tTrain Loss  nan\n",
            "Epoch(1/5)\t Batch(600/1687): \tTrain Loss  nan\n",
            "Epoch(1/5)\t Batch(800/1687): \tTrain Loss  nan\n",
            "Epoch(1/5)\t Batch(1000/1687): \tTrain Loss  nan\n",
            "Epoch(1/5)\t Batch(1200/1687): \tTrain Loss  nan\n",
            "Epoch(1/5)\t Batch(1400/1687): \tTrain Loss  nan\n",
            "Epoch(1/5)\t Batch(1600/1687): \tTrain Loss  nan\n",
            "End of Epoch: 1 Train Accuracy: 0.0996 Val Loss: nan Val Accuracy: 0.1033\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/1687): \tTrain Loss  nan\n",
            "Epoch(2/5)\t Batch(200/1687): \tTrain Loss  nan\n",
            "Epoch(2/5)\t Batch(400/1687): \tTrain Loss  nan\n",
            "Epoch(2/5)\t Batch(600/1687): \tTrain Loss  nan\n",
            "Epoch(2/5)\t Batch(800/1687): \tTrain Loss  nan\n",
            "Epoch(2/5)\t Batch(1000/1687): \tTrain Loss  nan\n",
            "Epoch(2/5)\t Batch(1200/1687): \tTrain Loss  nan\n",
            "Epoch(2/5)\t Batch(1400/1687): \tTrain Loss  nan\n",
            "Epoch(2/5)\t Batch(1600/1687): \tTrain Loss  nan\n",
            "End of Epoch: 2 Train Accuracy: 0.0996 Val Loss: nan Val Accuracy: 0.1033\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/1687): \tTrain Loss  nan\n",
            "Epoch(3/5)\t Batch(200/1687): \tTrain Loss  nan\n",
            "Epoch(3/5)\t Batch(400/1687): \tTrain Loss  nan\n",
            "Epoch(3/5)\t Batch(600/1687): \tTrain Loss  nan\n",
            "Epoch(3/5)\t Batch(800/1687): \tTrain Loss  nan\n",
            "Epoch(3/5)\t Batch(1000/1687): \tTrain Loss  nan\n",
            "Epoch(3/5)\t Batch(1200/1687): \tTrain Loss  nan\n",
            "Epoch(3/5)\t Batch(1400/1687): \tTrain Loss  nan\n",
            "Epoch(3/5)\t Batch(1600/1687): \tTrain Loss  nan\n",
            "End of Epoch: 3 Train Accuracy: 0.0996 Val Loss: nan Val Accuracy: 0.1033\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/1687): \tTrain Loss  nan\n",
            "Epoch(4/5)\t Batch(200/1687): \tTrain Loss  nan\n",
            "Epoch(4/5)\t Batch(400/1687): \tTrain Loss  nan\n",
            "Epoch(4/5)\t Batch(600/1687): \tTrain Loss  nan\n",
            "Epoch(4/5)\t Batch(800/1687): \tTrain Loss  nan\n",
            "Epoch(4/5)\t Batch(1000/1687): \tTrain Loss  nan\n",
            "Epoch(4/5)\t Batch(1200/1687): \tTrain Loss  nan\n",
            "Epoch(4/5)\t Batch(1400/1687): \tTrain Loss  nan\n",
            "Epoch(4/5)\t Batch(1600/1687): \tTrain Loss  nan\n",
            "End of Epoch: 4 Train Accuracy: 0.0996 Val Loss: nan Val Accuracy: 0.1033\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/1687): \tTrain Loss  nan\n",
            "Epoch(5/5)\t Batch(200/1687): \tTrain Loss  nan\n",
            "Epoch(5/5)\t Batch(400/1687): \tTrain Loss  nan\n",
            "Epoch(5/5)\t Batch(600/1687): \tTrain Loss  nan\n",
            "Epoch(5/5)\t Batch(800/1687): \tTrain Loss  nan\n",
            "Epoch(5/5)\t Batch(1000/1687): \tTrain Loss  nan\n",
            "Epoch(5/5)\t Batch(1200/1687): \tTrain Loss  nan\n",
            "Epoch(5/5)\t Batch(1400/1687): \tTrain Loss  nan\n",
            "Epoch(5/5)\t Batch(1600/1687): \tTrain Loss  nan\n",
            "End of Epoch: 5 Train Accuracy: 0.0996 Val Loss: nan Val Accuracy: 0.1033\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>█▁▁▁▁▁</td></tr><tr><td>Val Accuracy</td><td>▁█████</td></tr><tr><td>Val Loss</td><td>▁     </td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.09963</td></tr><tr><td>Train Loss</td><td>nan</td></tr><tr><td>Val Accuracy</td><td>0.10333</td></tr><tr><td>Val Loss</td><td>nan</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nag|loss_cross_entropy|lr=0.001|batch_32|act_relu|hid_5|neurons_64|nrns_5|init_Random881</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/h3g2gt37' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/h3g2gt37</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_164739-h3g2gt37/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9ev9vzhp with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_164815-9ev9vzhp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/9ev9vzhp' target=\"_blank\">divine-sweep-14</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/9ev9vzhp' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/9ev9vzhp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.0925 Val Loss: 9.8265 Val Accuracy: 0.0965\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/843): \tTrain Loss  9.8054\n",
            "Epoch(1/5)\t Batch(200/843): \tTrain Loss  3.5961\n",
            "Epoch(1/5)\t Batch(400/843): \tTrain Loss  2.5795\n",
            "Epoch(1/5)\t Batch(600/843): \tTrain Loss  2.2981\n",
            "Epoch(1/5)\t Batch(800/843): \tTrain Loss  2.3194\n",
            "End of Epoch: 1 Train Accuracy: 0.1628 Val Loss: 2.2708 Val Accuracy: 0.1575\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/843): \tTrain Loss  2.2738\n",
            "Epoch(2/5)\t Batch(200/843): \tTrain Loss  2.2495\n",
            "Epoch(2/5)\t Batch(400/843): \tTrain Loss  2.1885\n",
            "Epoch(2/5)\t Batch(600/843): \tTrain Loss  2.2143\n",
            "Epoch(2/5)\t Batch(800/843): \tTrain Loss  2.1346\n",
            "End of Epoch: 2 Train Accuracy: 0.4771 Val Loss: 2.1512 Val Accuracy: 0.4698\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/843): \tTrain Loss  2.1156\n",
            "Epoch(3/5)\t Batch(200/843): \tTrain Loss  2.1086\n",
            "Epoch(3/5)\t Batch(400/843): \tTrain Loss  2.0861\n",
            "Epoch(3/5)\t Batch(600/843): \tTrain Loss  2.0437\n",
            "Epoch(3/5)\t Batch(800/843): \tTrain Loss  2.0363\n",
            "End of Epoch: 3 Train Accuracy: 0.5956 Val Loss: 2.0534 Val Accuracy: 0.5902\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/843): \tTrain Loss  2.0572\n",
            "Epoch(4/5)\t Batch(200/843): \tTrain Loss  2.0119\n",
            "Epoch(4/5)\t Batch(400/843): \tTrain Loss  2.0086\n",
            "Epoch(4/5)\t Batch(600/843): \tTrain Loss  2.0105\n",
            "Epoch(4/5)\t Batch(800/843): \tTrain Loss  1.9811\n",
            "End of Epoch: 4 Train Accuracy: 0.6697 Val Loss: 1.9749 Val Accuracy: 0.6628\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/843): \tTrain Loss  1.9771\n",
            "Epoch(5/5)\t Batch(200/843): \tTrain Loss  1.9666\n",
            "Epoch(5/5)\t Batch(400/843): \tTrain Loss  1.9318\n",
            "Epoch(5/5)\t Batch(600/843): \tTrain Loss  1.9097\n",
            "Epoch(5/5)\t Batch(800/843): \tTrain Loss  1.8601\n",
            "End of Epoch: 5 Train Accuracy: 0.6992 Val Loss: 1.9118 Val Accuracy: 0.6907\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▂▅▇██</td></tr><tr><td>Train Loss</td><td>█▅▄▃▁</td></tr><tr><td>Val Accuracy</td><td>▁▂▅▇██</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.69924</td></tr><tr><td>Train Loss</td><td>1.84991</td></tr><tr><td>Val Accuracy</td><td>0.69067</td></tr><tr><td>Val Loss</td><td>1.91183</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_cross_entropy|lr=0.0001|batch_64|act_sigmoid|hid_4|neurons_128|nrns_5|init_Random47</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/9ev9vzhp' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/9ev9vzhp</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_164815-9ev9vzhp/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5xang9rc with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: momentum\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_164905-5xang9rc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/5xang9rc' target=\"_blank\">fresh-sweep-15</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/5xang9rc' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/5xang9rc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.1070 Val Loss: 6.3098 Val Accuracy: 0.1090\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  6.1330\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  2.2546\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  2.2932\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  2.2473\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  2.2488\n",
            "End of Epoch: 1 Train Accuracy: 0.2449 Val Loss: 2.2423 Val Accuracy: 0.2440\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  2.2228\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  2.2380\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  2.2460\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  2.2211\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  2.2245\n",
            "End of Epoch: 2 Train Accuracy: 0.3064 Val Loss: 2.2423 Val Accuracy: 0.3013\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  2.2415\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  2.2174\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  2.2334\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  2.2241\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  2.2470\n",
            "End of Epoch: 3 Train Accuracy: 0.3218 Val Loss: 2.2304 Val Accuracy: 0.3180\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  2.2528\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  2.2409\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  2.2296\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  2.2416\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  2.2373\n",
            "End of Epoch: 4 Train Accuracy: 0.3486 Val Loss: 2.2157 Val Accuracy: 0.3405\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  2.2063\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  2.1971\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  2.2092\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  2.2053\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  2.1946\n",
            "End of Epoch: 5 Train Accuracy: 0.3664 Val Loss: 2.1995 Val Accuracy: 0.3692\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  2.2237\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  2.1904\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  2.1992\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  2.1744\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  2.1691\n",
            "End of Epoch: 6 Train Accuracy: 0.3868 Val Loss: 2.1841 Val Accuracy: 0.3808\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  2.1984\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  2.1798\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  2.1587\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  2.1408\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  2.1768\n",
            "End of Epoch: 7 Train Accuracy: 0.3978 Val Loss: 2.1691 Val Accuracy: 0.3933\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  2.1955\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  2.1665\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  2.1453\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  2.1499\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  2.1790\n",
            "End of Epoch: 8 Train Accuracy: 0.4009 Val Loss: 2.1563 Val Accuracy: 0.3933\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  2.1477\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  2.1162\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  2.1245\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  2.1686\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  2.1254\n",
            "End of Epoch: 9 Train Accuracy: 0.4177 Val Loss: 2.1457 Val Accuracy: 0.4133\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  2.1581\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  2.1740\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  2.1063\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  2.1335\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  2.1290\n",
            "End of Epoch: 10 Train Accuracy: 0.4314 Val Loss: 2.1354 Val Accuracy: 0.4217\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▄▅▆▆▇▇▇▇██</td></tr><tr><td>Train Loss</td><td>█▆█▅▄▂▁▄▅▁</td></tr><tr><td>Val Accuracy</td><td>▁▄▅▆▆▇▇▇▇██</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.43144</td></tr><tr><td>Train Loss</td><td>2.13398</td></tr><tr><td>Val Accuracy</td><td>0.42167</td></tr><tr><td>Val Loss</td><td>2.13542</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_momentum|loss_cross_entropy|lr=0.001|batch_64|act_sigmoid|hid_3|neurons_64|nrns_10|init_Random251</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/5xang9rc' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/5xang9rc</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_164905-5xang9rc/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2qrdt6oj with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_164945-2qrdt6oj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/2qrdt6oj' target=\"_blank\">rich-sweep-16</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/2qrdt6oj' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/2qrdt6oj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0829 Val Loss: 0.1828 Val Accuracy: 0.0862\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  0.1844\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  0.1781\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  0.1781\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  0.1781\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  0.1656\n",
            "End of Epoch: 1 Train Accuracy: 0.1002 Val Loss: 0.1802 Val Accuracy: 0.0990\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  0.1750\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  0.1844\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  0.1812\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  0.1781\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  0.1687\n",
            "End of Epoch: 2 Train Accuracy: 0.1001 Val Loss: 0.1802 Val Accuracy: 0.0988\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  0.1875\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  0.1812\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  0.1719\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  0.1844\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  0.1625\n",
            "End of Epoch: 3 Train Accuracy: 0.1001 Val Loss: 0.1802 Val Accuracy: 0.0988\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  0.1812\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  0.1552\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  0.0277\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  0.0245\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  0.0339\n",
            "End of Epoch: 4 Train Accuracy: 0.8341 Val Loss: 0.0245 Val Accuracy: 0.8348\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  0.0334\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  0.0238\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  0.0286\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  0.0234\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  0.0274\n",
            "End of Epoch: 5 Train Accuracy: 0.8601 Val Loss: 0.0215 Val Accuracy: 0.8498\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  0.0148\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  0.0188\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  0.0174\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  0.0141\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  0.0204\n",
            "End of Epoch: 6 Train Accuracy: 0.8564 Val Loss: 0.0221 Val Accuracy: 0.8510\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  0.0215\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  0.0155\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  0.0143\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  0.0119\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  0.0226\n",
            "End of Epoch: 7 Train Accuracy: 0.8663 Val Loss: 0.0208 Val Accuracy: 0.8548\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  0.0169\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  0.0141\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  0.0170\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  0.0301\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  0.0193\n",
            "End of Epoch: 8 Train Accuracy: 0.8723 Val Loss: 0.0196 Val Accuracy: 0.8663\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  0.0283\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  0.0222\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  0.0123\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  0.0253\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  0.0279\n",
            "End of Epoch: 9 Train Accuracy: 0.8773 Val Loss: 0.0195 Val Accuracy: 0.8628\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  0.0135\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  0.0254\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  0.0182\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  0.0201\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  0.0254\n",
            "End of Epoch: 10 Train Accuracy: 0.8611 Val Loss: 0.0213 Val Accuracy: 0.8550\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▁▁▁███████</td></tr><tr><td>Train Loss</td><td>███▁▁▁▁▁▁▁</td></tr><tr><td>Val Accuracy</td><td>▁▁▁▁███████</td></tr><tr><td>Val Loss</td><td>████▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.86113</td></tr><tr><td>Train Loss</td><td>0.01375</td></tr><tr><td>Val Accuracy</td><td>0.855</td></tr><tr><td>Val Loss</td><td>0.02131</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nadam|loss_mse|lr=0.001|batch_64|act_relu|hid_5|neurons_64|nrns_10|init_Random907</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/2qrdt6oj' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/2qrdt6oj</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_164945-2qrdt6oj/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1pwvzszp with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_165111-1pwvzszp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/1pwvzszp' target=\"_blank\">fanciful-sweep-17</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/1pwvzszp' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/1pwvzszp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0246 Val Loss: 0.0900 Val Accuracy: 0.0273\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/3375): \tTrain Loss  0.0902\n",
            "Epoch(1/10)\t Batch(200/3375): \tTrain Loss  0.0889\n",
            "Epoch(1/10)\t Batch(400/3375): \tTrain Loss  0.0860\n",
            "Epoch(1/10)\t Batch(600/3375): \tTrain Loss  0.0816\n",
            "Epoch(1/10)\t Batch(800/3375): \tTrain Loss  0.0795\n",
            "Epoch(1/10)\t Batch(1000/3375): \tTrain Loss  0.0821\n",
            "Epoch(1/10)\t Batch(1200/3375): \tTrain Loss  0.0649\n",
            "Epoch(1/10)\t Batch(1400/3375): \tTrain Loss  0.0750\n",
            "Epoch(1/10)\t Batch(1600/3375): \tTrain Loss  0.0620\n",
            "Epoch(1/10)\t Batch(1800/3375): \tTrain Loss  0.0590\n",
            "Epoch(1/10)\t Batch(2000/3375): \tTrain Loss  0.0758\n",
            "Epoch(1/10)\t Batch(2200/3375): \tTrain Loss  0.0506\n",
            "Epoch(1/10)\t Batch(2400/3375): \tTrain Loss  0.0643\n",
            "Epoch(1/10)\t Batch(2600/3375): \tTrain Loss  0.0467\n",
            "Epoch(1/10)\t Batch(2800/3375): \tTrain Loss  0.0526\n",
            "Epoch(1/10)\t Batch(3000/3375): \tTrain Loss  0.0676\n",
            "Epoch(1/10)\t Batch(3200/3375): \tTrain Loss  0.0429\n",
            "End of Epoch: 1 Train Accuracy: 0.6011 Val Loss: 0.0524 Val Accuracy: 0.6073\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/3375): \tTrain Loss  0.0285\n",
            "Epoch(2/10)\t Batch(200/3375): \tTrain Loss  0.0627\n",
            "Epoch(2/10)\t Batch(400/3375): \tTrain Loss  0.0635\n",
            "Epoch(2/10)\t Batch(600/3375): \tTrain Loss  0.0525\n",
            "Epoch(2/10)\t Batch(800/3375): \tTrain Loss  0.0503\n",
            "Epoch(2/10)\t Batch(1000/3375): \tTrain Loss  0.0424\n",
            "Epoch(2/10)\t Batch(1200/3375): \tTrain Loss  0.0279\n",
            "Epoch(2/10)\t Batch(1400/3375): \tTrain Loss  0.0426\n",
            "Epoch(2/10)\t Batch(1600/3375): \tTrain Loss  0.0523\n",
            "Epoch(2/10)\t Batch(1800/3375): \tTrain Loss  0.0458\n",
            "Epoch(2/10)\t Batch(2000/3375): \tTrain Loss  0.0368\n",
            "Epoch(2/10)\t Batch(2200/3375): \tTrain Loss  0.0545\n",
            "Epoch(2/10)\t Batch(2400/3375): \tTrain Loss  0.0441\n",
            "Epoch(2/10)\t Batch(2600/3375): \tTrain Loss  0.0447\n",
            "Epoch(2/10)\t Batch(2800/3375): \tTrain Loss  0.0363\n",
            "Epoch(2/10)\t Batch(3000/3375): \tTrain Loss  0.0292\n",
            "Epoch(2/10)\t Batch(3200/3375): \tTrain Loss  0.0462\n",
            "End of Epoch: 2 Train Accuracy: 0.7066 Val Loss: 0.0409 Val Accuracy: 0.7063\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/3375): \tTrain Loss  0.0525\n",
            "Epoch(3/10)\t Batch(200/3375): \tTrain Loss  0.0526\n",
            "Epoch(3/10)\t Batch(400/3375): \tTrain Loss  0.0518\n",
            "Epoch(3/10)\t Batch(600/3375): \tTrain Loss  0.0291\n",
            "Epoch(3/10)\t Batch(800/3375): \tTrain Loss  0.0352\n",
            "Epoch(3/10)\t Batch(1000/3375): \tTrain Loss  0.0510\n",
            "Epoch(3/10)\t Batch(1200/3375): \tTrain Loss  0.0392\n",
            "Epoch(3/10)\t Batch(1400/3375): \tTrain Loss  0.0431\n",
            "Epoch(3/10)\t Batch(1600/3375): \tTrain Loss  0.0315\n",
            "Epoch(3/10)\t Batch(1800/3375): \tTrain Loss  0.0415\n",
            "Epoch(3/10)\t Batch(2000/3375): \tTrain Loss  0.0523\n",
            "Epoch(3/10)\t Batch(2200/3375): \tTrain Loss  0.0610\n",
            "Epoch(3/10)\t Batch(2400/3375): \tTrain Loss  0.0293\n",
            "Epoch(3/10)\t Batch(2600/3375): \tTrain Loss  0.0513\n",
            "Epoch(3/10)\t Batch(2800/3375): \tTrain Loss  0.0407\n",
            "Epoch(3/10)\t Batch(3000/3375): \tTrain Loss  0.0315\n",
            "Epoch(3/10)\t Batch(3200/3375): \tTrain Loss  0.0380\n",
            "End of Epoch: 3 Train Accuracy: 0.7434 Val Loss: 0.0364 Val Accuracy: 0.7482\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/3375): \tTrain Loss  0.0208\n",
            "Epoch(4/10)\t Batch(200/3375): \tTrain Loss  0.0316\n",
            "Epoch(4/10)\t Batch(400/3375): \tTrain Loss  0.0413\n",
            "Epoch(4/10)\t Batch(600/3375): \tTrain Loss  0.0391\n",
            "Epoch(4/10)\t Batch(800/3375): \tTrain Loss  0.0321\n",
            "Epoch(4/10)\t Batch(1000/3375): \tTrain Loss  0.0377\n",
            "Epoch(4/10)\t Batch(1200/3375): \tTrain Loss  0.0419\n",
            "Epoch(4/10)\t Batch(1400/3375): \tTrain Loss  0.0285\n",
            "Epoch(4/10)\t Batch(1600/3375): \tTrain Loss  0.0343\n",
            "Epoch(4/10)\t Batch(1800/3375): \tTrain Loss  0.0274\n",
            "Epoch(4/10)\t Batch(2000/3375): \tTrain Loss  0.0412\n",
            "Epoch(4/10)\t Batch(2200/3375): \tTrain Loss  0.0340\n",
            "Epoch(4/10)\t Batch(2400/3375): \tTrain Loss  0.0194\n",
            "Epoch(4/10)\t Batch(2600/3375): \tTrain Loss  0.0388\n",
            "Epoch(4/10)\t Batch(2800/3375): \tTrain Loss  0.0293\n",
            "Epoch(4/10)\t Batch(3000/3375): \tTrain Loss  0.0474\n",
            "Epoch(4/10)\t Batch(3200/3375): \tTrain Loss  0.0377\n",
            "End of Epoch: 4 Train Accuracy: 0.7673 Val Loss: 0.0338 Val Accuracy: 0.7758\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/3375): \tTrain Loss  0.0290\n",
            "Epoch(5/10)\t Batch(200/3375): \tTrain Loss  0.0285\n",
            "Epoch(5/10)\t Batch(400/3375): \tTrain Loss  0.0248\n",
            "Epoch(5/10)\t Batch(600/3375): \tTrain Loss  0.0425\n",
            "Epoch(5/10)\t Batch(800/3375): \tTrain Loss  0.0307\n",
            "Epoch(5/10)\t Batch(1000/3375): \tTrain Loss  0.0432\n",
            "Epoch(5/10)\t Batch(1200/3375): \tTrain Loss  0.0329\n",
            "Epoch(5/10)\t Batch(1400/3375): \tTrain Loss  0.0332\n",
            "Epoch(5/10)\t Batch(1600/3375): \tTrain Loss  0.0129\n",
            "Epoch(5/10)\t Batch(1800/3375): \tTrain Loss  0.0258\n",
            "Epoch(5/10)\t Batch(2000/3375): \tTrain Loss  0.0288\n",
            "Epoch(5/10)\t Batch(2200/3375): \tTrain Loss  0.0392\n",
            "Epoch(5/10)\t Batch(2400/3375): \tTrain Loss  0.0338\n",
            "Epoch(5/10)\t Batch(2600/3375): \tTrain Loss  0.0354\n",
            "Epoch(5/10)\t Batch(2800/3375): \tTrain Loss  0.0267\n",
            "Epoch(5/10)\t Batch(3000/3375): \tTrain Loss  0.0289\n",
            "Epoch(5/10)\t Batch(3200/3375): \tTrain Loss  0.0378\n",
            "End of Epoch: 5 Train Accuracy: 0.7800 Val Loss: 0.0319 Val Accuracy: 0.7845\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/3375): \tTrain Loss  0.0254\n",
            "Epoch(6/10)\t Batch(200/3375): \tTrain Loss  0.0452\n",
            "Epoch(6/10)\t Batch(400/3375): \tTrain Loss  0.0330\n",
            "Epoch(6/10)\t Batch(600/3375): \tTrain Loss  0.0381\n",
            "Epoch(6/10)\t Batch(800/3375): \tTrain Loss  0.0385\n",
            "Epoch(6/10)\t Batch(1000/3375): \tTrain Loss  0.0391\n",
            "Epoch(6/10)\t Batch(1200/3375): \tTrain Loss  0.0258\n",
            "Epoch(6/10)\t Batch(1400/3375): \tTrain Loss  0.0374\n",
            "Epoch(6/10)\t Batch(1600/3375): \tTrain Loss  0.0261\n",
            "Epoch(6/10)\t Batch(1800/3375): \tTrain Loss  0.0444\n",
            "Epoch(6/10)\t Batch(2000/3375): \tTrain Loss  0.0229\n",
            "Epoch(6/10)\t Batch(2200/3375): \tTrain Loss  0.0285\n",
            "Epoch(6/10)\t Batch(2400/3375): \tTrain Loss  0.0135\n",
            "Epoch(6/10)\t Batch(2600/3375): \tTrain Loss  0.0176\n",
            "Epoch(6/10)\t Batch(2800/3375): \tTrain Loss  0.0223\n",
            "Epoch(6/10)\t Batch(3000/3375): \tTrain Loss  0.0425\n",
            "Epoch(6/10)\t Batch(3200/3375): \tTrain Loss  0.0461\n",
            "End of Epoch: 6 Train Accuracy: 0.7915 Val Loss: 0.0303 Val Accuracy: 0.7998\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/3375): \tTrain Loss  0.0159\n",
            "Epoch(7/10)\t Batch(200/3375): \tTrain Loss  0.0227\n",
            "Epoch(7/10)\t Batch(400/3375): \tTrain Loss  0.0256\n",
            "Epoch(7/10)\t Batch(600/3375): \tTrain Loss  0.0385\n",
            "Epoch(7/10)\t Batch(800/3375): \tTrain Loss  0.0225\n",
            "Epoch(7/10)\t Batch(1000/3375): \tTrain Loss  0.0412\n",
            "Epoch(7/10)\t Batch(1200/3375): \tTrain Loss  0.0188\n",
            "Epoch(7/10)\t Batch(1400/3375): \tTrain Loss  0.0224\n",
            "Epoch(7/10)\t Batch(1600/3375): \tTrain Loss  0.0363\n",
            "Epoch(7/10)\t Batch(1800/3375): \tTrain Loss  0.0385\n",
            "Epoch(7/10)\t Batch(2000/3375): \tTrain Loss  0.0281\n",
            "Epoch(7/10)\t Batch(2200/3375): \tTrain Loss  0.0304\n",
            "Epoch(7/10)\t Batch(2400/3375): \tTrain Loss  0.0235\n",
            "Epoch(7/10)\t Batch(2600/3375): \tTrain Loss  0.0135\n",
            "Epoch(7/10)\t Batch(2800/3375): \tTrain Loss  0.0467\n",
            "Epoch(7/10)\t Batch(3000/3375): \tTrain Loss  0.0451\n",
            "Epoch(7/10)\t Batch(3200/3375): \tTrain Loss  0.0340\n",
            "End of Epoch: 7 Train Accuracy: 0.8047 Val Loss: 0.0292 Val Accuracy: 0.8105\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/3375): \tTrain Loss  0.0175\n",
            "Epoch(8/10)\t Batch(200/3375): \tTrain Loss  0.0277\n",
            "Epoch(8/10)\t Batch(400/3375): \tTrain Loss  0.0237\n",
            "Epoch(8/10)\t Batch(600/3375): \tTrain Loss  0.0270\n",
            "Epoch(8/10)\t Batch(800/3375): \tTrain Loss  0.0519\n",
            "Epoch(8/10)\t Batch(1000/3375): \tTrain Loss  0.0318\n",
            "Epoch(8/10)\t Batch(1200/3375): \tTrain Loss  0.0223\n",
            "Epoch(8/10)\t Batch(1400/3375): \tTrain Loss  0.0157\n",
            "Epoch(8/10)\t Batch(1600/3375): \tTrain Loss  0.0211\n",
            "Epoch(8/10)\t Batch(1800/3375): \tTrain Loss  0.0234\n",
            "Epoch(8/10)\t Batch(2000/3375): \tTrain Loss  0.0215\n",
            "Epoch(8/10)\t Batch(2200/3375): \tTrain Loss  0.0276\n",
            "Epoch(8/10)\t Batch(2400/3375): \tTrain Loss  0.0310\n",
            "Epoch(8/10)\t Batch(2600/3375): \tTrain Loss  0.0561\n",
            "Epoch(8/10)\t Batch(2800/3375): \tTrain Loss  0.0263\n",
            "Epoch(8/10)\t Batch(3000/3375): \tTrain Loss  0.0297\n",
            "Epoch(8/10)\t Batch(3200/3375): \tTrain Loss  0.0064\n",
            "End of Epoch: 8 Train Accuracy: 0.8111 Val Loss: 0.0282 Val Accuracy: 0.8138\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/3375): \tTrain Loss  0.0292\n",
            "Epoch(9/10)\t Batch(200/3375): \tTrain Loss  0.0226\n",
            "Epoch(9/10)\t Batch(400/3375): \tTrain Loss  0.0192\n",
            "Epoch(9/10)\t Batch(600/3375): \tTrain Loss  0.0300\n",
            "Epoch(9/10)\t Batch(800/3375): \tTrain Loss  0.0249\n",
            "Epoch(9/10)\t Batch(1000/3375): \tTrain Loss  0.0470\n",
            "Epoch(9/10)\t Batch(1200/3375): \tTrain Loss  0.0277\n",
            "Epoch(9/10)\t Batch(1400/3375): \tTrain Loss  0.0184\n",
            "Epoch(9/10)\t Batch(1600/3375): \tTrain Loss  0.0234\n",
            "Epoch(9/10)\t Batch(1800/3375): \tTrain Loss  0.0200\n",
            "Epoch(9/10)\t Batch(2000/3375): \tTrain Loss  0.0400\n",
            "Epoch(9/10)\t Batch(2200/3375): \tTrain Loss  0.0257\n",
            "Epoch(9/10)\t Batch(2400/3375): \tTrain Loss  0.0195\n",
            "Epoch(9/10)\t Batch(2600/3375): \tTrain Loss  0.0126\n",
            "Epoch(9/10)\t Batch(2800/3375): \tTrain Loss  0.0080\n",
            "Epoch(9/10)\t Batch(3000/3375): \tTrain Loss  0.0314\n",
            "Epoch(9/10)\t Batch(3200/3375): \tTrain Loss  0.0165\n",
            "End of Epoch: 9 Train Accuracy: 0.8166 Val Loss: 0.0274 Val Accuracy: 0.8195\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/3375): \tTrain Loss  0.0195\n",
            "Epoch(10/10)\t Batch(200/3375): \tTrain Loss  0.0312\n",
            "Epoch(10/10)\t Batch(400/3375): \tTrain Loss  0.0385\n",
            "Epoch(10/10)\t Batch(600/3375): \tTrain Loss  0.0292\n",
            "Epoch(10/10)\t Batch(800/3375): \tTrain Loss  0.0355\n",
            "Epoch(10/10)\t Batch(1000/3375): \tTrain Loss  0.0342\n",
            "Epoch(10/10)\t Batch(1200/3375): \tTrain Loss  0.0465\n",
            "Epoch(10/10)\t Batch(1400/3375): \tTrain Loss  0.0304\n",
            "Epoch(10/10)\t Batch(1600/3375): \tTrain Loss  0.0283\n",
            "Epoch(10/10)\t Batch(1800/3375): \tTrain Loss  0.0232\n",
            "Epoch(10/10)\t Batch(2000/3375): \tTrain Loss  0.0251\n",
            "Epoch(10/10)\t Batch(2200/3375): \tTrain Loss  0.0113\n",
            "Epoch(10/10)\t Batch(2400/3375): \tTrain Loss  0.0179\n",
            "Epoch(10/10)\t Batch(2600/3375): \tTrain Loss  0.0361\n",
            "Epoch(10/10)\t Batch(2800/3375): \tTrain Loss  0.0236\n",
            "Epoch(10/10)\t Batch(3000/3375): \tTrain Loss  0.0356\n",
            "Epoch(10/10)\t Batch(3200/3375): \tTrain Loss  0.0402\n",
            "End of Epoch: 10 Train Accuracy: 0.8210 Val Loss: 0.0267 Val Accuracy: 0.8233\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▆▇▇███████</td></tr><tr><td>Train Loss</td><td>█▂▁▅▄▄▅▁▃▁</td></tr><tr><td>Val Accuracy</td><td>▁▆▇▇███████</td></tr><tr><td>Val Loss</td><td>█▄▃▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.82098</td></tr><tr><td>Train Loss</td><td>0.02027</td></tr><tr><td>Val Accuracy</td><td>0.82333</td></tr><tr><td>Val Loss</td><td>0.02671</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_adam|loss_mse|lr=0.0001|batch_16|act_relu|hid_5|neurons_32|nrns_10|init_Xavier114</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/1pwvzszp' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/1pwvzszp</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_165111-1pwvzszp/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: euzeijic with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_165312-euzeijic</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/euzeijic' target=\"_blank\">lucky-sweep-18</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/euzeijic' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/euzeijic</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.1217 Val Loss: 0.1769 Val Accuracy: 0.1148\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/1687): \tTrain Loss  0.1750\n",
            "Epoch(1/10)\t Batch(200/1687): \tTrain Loss  0.1937\n",
            "Epoch(1/10)\t Batch(400/1687): \tTrain Loss  0.1812\n",
            "Epoch(1/10)\t Batch(600/1687): \tTrain Loss  0.1500\n",
            "Epoch(1/10)\t Batch(800/1687): \tTrain Loss  0.1500\n",
            "Epoch(1/10)\t Batch(1000/1687): \tTrain Loss  0.1679\n",
            "Epoch(1/10)\t Batch(1200/1687): \tTrain Loss  0.1687\n",
            "Epoch(1/10)\t Batch(1400/1687): \tTrain Loss  0.1316\n",
            "Epoch(1/10)\t Batch(1600/1687): \tTrain Loss  0.1625\n",
            "End of Epoch: 1 Train Accuracy: 0.1558 Val Loss: 0.1698 Val Accuracy: 0.1508\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/1687): \tTrain Loss  0.1687\n",
            "Epoch(2/10)\t Batch(200/1687): \tTrain Loss  0.1562\n",
            "Epoch(2/10)\t Batch(400/1687): \tTrain Loss  0.1812\n",
            "Epoch(2/10)\t Batch(600/1687): \tTrain Loss  0.1500\n",
            "Epoch(2/10)\t Batch(800/1687): \tTrain Loss  0.1687\n",
            "Epoch(2/10)\t Batch(1000/1687): \tTrain Loss  0.1562\n",
            "Epoch(2/10)\t Batch(1200/1687): \tTrain Loss  0.1625\n",
            "Epoch(2/10)\t Batch(1400/1687): \tTrain Loss  0.1500\n",
            "Epoch(2/10)\t Batch(1600/1687): \tTrain Loss  0.1937\n",
            "End of Epoch: 2 Train Accuracy: 0.1875 Val Loss: 0.1639 Val Accuracy: 0.1800\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/1687): \tTrain Loss  0.1375\n",
            "Epoch(3/10)\t Batch(200/1687): \tTrain Loss  0.1562\n",
            "Epoch(3/10)\t Batch(400/1687): \tTrain Loss  0.1562\n",
            "Epoch(3/10)\t Batch(600/1687): \tTrain Loss  0.1312\n",
            "Epoch(3/10)\t Batch(800/1687): \tTrain Loss  0.1623\n",
            "Epoch(3/10)\t Batch(1000/1687): \tTrain Loss  0.1625\n",
            "Epoch(3/10)\t Batch(1200/1687): \tTrain Loss  0.1611\n",
            "Epoch(3/10)\t Batch(1400/1687): \tTrain Loss  0.1562\n",
            "Epoch(3/10)\t Batch(1600/1687): \tTrain Loss  0.1312\n",
            "End of Epoch: 3 Train Accuracy: 0.2762 Val Loss: 0.1458 Val Accuracy: 0.2702\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/1687): \tTrain Loss  0.1375\n",
            "Epoch(4/10)\t Batch(200/1687): \tTrain Loss  0.1741\n",
            "Epoch(4/10)\t Batch(400/1687): \tTrain Loss  0.1500\n",
            "Epoch(4/10)\t Batch(600/1687): \tTrain Loss  0.1312\n",
            "Epoch(4/10)\t Batch(800/1687): \tTrain Loss  0.1312\n",
            "Epoch(4/10)\t Batch(1000/1687): \tTrain Loss  0.1562\n",
            "Epoch(4/10)\t Batch(1200/1687): \tTrain Loss  0.1375\n",
            "Epoch(4/10)\t Batch(1400/1687): \tTrain Loss  0.1437\n",
            "Epoch(4/10)\t Batch(1600/1687): \tTrain Loss  0.1437\n",
            "End of Epoch: 4 Train Accuracy: 0.3062 Val Loss: 0.1400 Val Accuracy: 0.2988\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/1687): \tTrain Loss  0.1500\n",
            "Epoch(5/10)\t Batch(200/1687): \tTrain Loss  0.1312\n",
            "Epoch(5/10)\t Batch(400/1687): \tTrain Loss  0.1062\n",
            "Epoch(5/10)\t Batch(600/1687): \tTrain Loss  0.1625\n",
            "Epoch(5/10)\t Batch(800/1687): \tTrain Loss  0.1187\n",
            "Epoch(5/10)\t Batch(1000/1687): \tTrain Loss  0.1187\n",
            "Epoch(5/10)\t Batch(1200/1687): \tTrain Loss  0.1750\n",
            "Epoch(5/10)\t Batch(1400/1687): \tTrain Loss  0.1500\n",
            "Epoch(5/10)\t Batch(1600/1687): \tTrain Loss  0.1250\n",
            "End of Epoch: 5 Train Accuracy: 0.3176 Val Loss: 0.1385 Val Accuracy: 0.3065\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/1687): \tTrain Loss  0.1245\n",
            "Epoch(6/10)\t Batch(200/1687): \tTrain Loss  0.1375\n",
            "Epoch(6/10)\t Batch(400/1687): \tTrain Loss  0.1625\n",
            "Epoch(6/10)\t Batch(600/1687): \tTrain Loss  0.1500\n",
            "Epoch(6/10)\t Batch(800/1687): \tTrain Loss  0.1499\n",
            "Epoch(6/10)\t Batch(1000/1687): \tTrain Loss  0.1250\n",
            "Epoch(6/10)\t Batch(1200/1687): \tTrain Loss  0.1438\n",
            "Epoch(6/10)\t Batch(1400/1687): \tTrain Loss  0.1374\n",
            "Epoch(6/10)\t Batch(1600/1687): \tTrain Loss  0.1375\n",
            "End of Epoch: 6 Train Accuracy: 0.3194 Val Loss: 0.1378 Val Accuracy: 0.3100\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/1687): \tTrain Loss  0.1248\n",
            "Epoch(7/10)\t Batch(200/1687): \tTrain Loss  0.1250\n",
            "Epoch(7/10)\t Batch(400/1687): \tTrain Loss  0.1125\n",
            "Epoch(7/10)\t Batch(600/1687): \tTrain Loss  0.1375\n",
            "Epoch(7/10)\t Batch(800/1687): \tTrain Loss  0.1625\n",
            "Epoch(7/10)\t Batch(1000/1687): \tTrain Loss  0.1437\n",
            "Epoch(7/10)\t Batch(1200/1687): \tTrain Loss  0.1375\n",
            "Epoch(7/10)\t Batch(1400/1687): \tTrain Loss  0.1187\n",
            "Epoch(7/10)\t Batch(1600/1687): \tTrain Loss  0.1312\n",
            "End of Epoch: 7 Train Accuracy: 0.3564 Val Loss: 0.1303 Val Accuracy: 0.3472\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/1687): \tTrain Loss  0.1312\n",
            "Epoch(8/10)\t Batch(200/1687): \tTrain Loss  0.1500\n",
            "Epoch(8/10)\t Batch(400/1687): \tTrain Loss  0.1062\n",
            "Epoch(8/10)\t Batch(600/1687): \tTrain Loss  0.1187\n",
            "Epoch(8/10)\t Batch(800/1687): \tTrain Loss  0.1250\n",
            "Epoch(8/10)\t Batch(1000/1687): \tTrain Loss  0.1250\n",
            "Epoch(8/10)\t Batch(1200/1687): \tTrain Loss  0.0942\n",
            "Epoch(8/10)\t Batch(1400/1687): \tTrain Loss  0.1265\n",
            "Epoch(8/10)\t Batch(1600/1687): \tTrain Loss  0.1187\n",
            "End of Epoch: 8 Train Accuracy: 0.4149 Val Loss: 0.1167 Val Accuracy: 0.4145\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/1687): \tTrain Loss  0.1416\n",
            "Epoch(9/10)\t Batch(200/1687): \tTrain Loss  0.1359\n",
            "Epoch(9/10)\t Batch(400/1687): \tTrain Loss  0.1520\n",
            "Epoch(9/10)\t Batch(600/1687): \tTrain Loss  0.1187\n",
            "Epoch(9/10)\t Batch(800/1687): \tTrain Loss  0.1187\n",
            "Epoch(9/10)\t Batch(1000/1687): \tTrain Loss  0.1000\n",
            "Epoch(9/10)\t Batch(1200/1687): \tTrain Loss  0.1235\n",
            "Epoch(9/10)\t Batch(1400/1687): \tTrain Loss  0.1000\n",
            "Epoch(9/10)\t Batch(1600/1687): \tTrain Loss  0.0750\n",
            "End of Epoch: 9 Train Accuracy: 0.4391 Val Loss: 0.1130 Val Accuracy: 0.4323\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/1687): \tTrain Loss  0.1312\n",
            "Epoch(10/10)\t Batch(200/1687): \tTrain Loss  0.0996\n",
            "Epoch(10/10)\t Batch(400/1687): \tTrain Loss  0.1124\n",
            "Epoch(10/10)\t Batch(600/1687): \tTrain Loss  0.0735\n",
            "Epoch(10/10)\t Batch(800/1687): \tTrain Loss  0.1062\n",
            "Epoch(10/10)\t Batch(1000/1687): \tTrain Loss  0.1187\n",
            "Epoch(10/10)\t Batch(1200/1687): \tTrain Loss  0.1062\n",
            "Epoch(10/10)\t Batch(1400/1687): \tTrain Loss  0.0937\n",
            "Epoch(10/10)\t Batch(1600/1687): \tTrain Loss  0.0937\n",
            "End of Epoch: 10 Train Accuracy: 0.4434 Val Loss: 0.1118 Val Accuracy: 0.4373\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▄▅▅▅▆▇██</td></tr><tr><td>Train Loss</td><td>█▆▇▆▄▅▄▃▆▁</td></tr><tr><td>Val Accuracy</td><td>▁▂▂▄▅▅▅▆███</td></tr><tr><td>Val Loss</td><td>█▇▇▅▄▄▄▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.44343</td></tr><tr><td>Train Loss</td><td>0.0875</td></tr><tr><td>Val Accuracy</td><td>0.43733</td></tr><tr><td>Val Loss</td><td>0.11176</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nadam|loss_mse|lr=0.0001|batch_32|act_relu|hid_3|neurons_64|nrns_10|init_Random952</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/euzeijic' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/euzeijic</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_165312-euzeijic/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: iicblsax with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_165457-iicblsax</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/iicblsax' target=\"_blank\">fanciful-sweep-19</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/iicblsax' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/iicblsax</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.1004 Val Loss: 0.1625 Val Accuracy: 0.0985\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/3375): \tTrain Loss  0.1652\n",
            "Epoch(1/10)\t Batch(200/3375): \tTrain Loss  0.1377\n",
            "Epoch(1/10)\t Batch(400/3375): \tTrain Loss  0.1409\n",
            "Epoch(1/10)\t Batch(600/3375): \tTrain Loss  0.1477\n",
            "Epoch(1/10)\t Batch(800/3375): \tTrain Loss  0.1595\n",
            "Epoch(1/10)\t Batch(1000/3375): \tTrain Loss  0.1572\n",
            "Epoch(1/10)\t Batch(1200/3375): \tTrain Loss  0.1480\n",
            "Epoch(1/10)\t Batch(1400/3375): \tTrain Loss  0.1795\n",
            "Epoch(1/10)\t Batch(1600/3375): \tTrain Loss  0.1843\n",
            "Epoch(1/10)\t Batch(1800/3375): \tTrain Loss  0.1689\n",
            "Epoch(1/10)\t Batch(2000/3375): \tTrain Loss  0.1451\n",
            "Epoch(1/10)\t Batch(2200/3375): \tTrain Loss  0.1556\n",
            "Epoch(1/10)\t Batch(2400/3375): \tTrain Loss  0.1328\n",
            "Epoch(1/10)\t Batch(2600/3375): \tTrain Loss  0.1639\n",
            "Epoch(1/10)\t Batch(2800/3375): \tTrain Loss  0.1673\n",
            "Epoch(1/10)\t Batch(3000/3375): \tTrain Loss  0.1735\n",
            "Epoch(1/10)\t Batch(3200/3375): \tTrain Loss  0.1460\n",
            "End of Epoch: 1 Train Accuracy: 0.1549 Val Loss: 0.1519 Val Accuracy: 0.1457\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/3375): \tTrain Loss  0.1375\n",
            "Epoch(2/10)\t Batch(200/3375): \tTrain Loss  0.1232\n",
            "Epoch(2/10)\t Batch(400/3375): \tTrain Loss  0.1615\n",
            "Epoch(2/10)\t Batch(600/3375): \tTrain Loss  0.1475\n",
            "Epoch(2/10)\t Batch(800/3375): \tTrain Loss  0.1589\n",
            "Epoch(2/10)\t Batch(1000/3375): \tTrain Loss  0.1621\n",
            "Epoch(2/10)\t Batch(1200/3375): \tTrain Loss  0.1311\n",
            "Epoch(2/10)\t Batch(1400/3375): \tTrain Loss  0.1438\n",
            "Epoch(2/10)\t Batch(1600/3375): \tTrain Loss  0.1665\n",
            "Epoch(2/10)\t Batch(1800/3375): \tTrain Loss  0.1820\n",
            "Epoch(2/10)\t Batch(2000/3375): \tTrain Loss  0.1399\n",
            "Epoch(2/10)\t Batch(2200/3375): \tTrain Loss  0.1602\n",
            "Epoch(2/10)\t Batch(2400/3375): \tTrain Loss  0.1648\n",
            "Epoch(2/10)\t Batch(2600/3375): \tTrain Loss  0.1326\n",
            "Epoch(2/10)\t Batch(2800/3375): \tTrain Loss  0.1393\n",
            "Epoch(2/10)\t Batch(3000/3375): \tTrain Loss  0.1273\n",
            "Epoch(2/10)\t Batch(3200/3375): \tTrain Loss  0.1435\n",
            "End of Epoch: 2 Train Accuracy: 0.1954 Val Loss: 0.1445 Val Accuracy: 0.1857\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/3375): \tTrain Loss  0.1203\n",
            "Epoch(3/10)\t Batch(200/3375): \tTrain Loss  0.1379\n",
            "Epoch(3/10)\t Batch(400/3375): \tTrain Loss  0.1447\n",
            "Epoch(3/10)\t Batch(600/3375): \tTrain Loss  0.1399\n",
            "Epoch(3/10)\t Batch(800/3375): \tTrain Loss  0.1621\n",
            "Epoch(3/10)\t Batch(1000/3375): \tTrain Loss  0.1312\n",
            "Epoch(3/10)\t Batch(1200/3375): \tTrain Loss  0.1429\n",
            "Epoch(3/10)\t Batch(1400/3375): \tTrain Loss  0.1339\n",
            "Epoch(3/10)\t Batch(1600/3375): \tTrain Loss  0.1614\n",
            "Epoch(3/10)\t Batch(1800/3375): \tTrain Loss  0.1267\n",
            "Epoch(3/10)\t Batch(2000/3375): \tTrain Loss  0.1320\n",
            "Epoch(3/10)\t Batch(2200/3375): \tTrain Loss  0.1501\n",
            "Epoch(3/10)\t Batch(2400/3375): \tTrain Loss  0.1487\n",
            "Epoch(3/10)\t Batch(2600/3375): \tTrain Loss  0.1399\n",
            "Epoch(3/10)\t Batch(2800/3375): \tTrain Loss  0.1422\n",
            "Epoch(3/10)\t Batch(3000/3375): \tTrain Loss  0.1368\n",
            "Epoch(3/10)\t Batch(3200/3375): \tTrain Loss  0.1568\n",
            "End of Epoch: 3 Train Accuracy: 0.2293 Val Loss: 0.1399 Val Accuracy: 0.2117\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/3375): \tTrain Loss  0.1502\n",
            "Epoch(4/10)\t Batch(200/3375): \tTrain Loss  0.1132\n",
            "Epoch(4/10)\t Batch(400/3375): \tTrain Loss  0.1408\n",
            "Epoch(4/10)\t Batch(600/3375): \tTrain Loss  0.1287\n",
            "Epoch(4/10)\t Batch(800/3375): \tTrain Loss  0.1461\n",
            "Epoch(4/10)\t Batch(1000/3375): \tTrain Loss  0.1201\n",
            "Epoch(4/10)\t Batch(1200/3375): \tTrain Loss  0.1196\n",
            "Epoch(4/10)\t Batch(1400/3375): \tTrain Loss  0.1145\n",
            "Epoch(4/10)\t Batch(1600/3375): \tTrain Loss  0.1241\n",
            "Epoch(4/10)\t Batch(1800/3375): \tTrain Loss  0.1389\n",
            "Epoch(4/10)\t Batch(2000/3375): \tTrain Loss  0.1376\n",
            "Epoch(4/10)\t Batch(2200/3375): \tTrain Loss  0.1545\n",
            "Epoch(4/10)\t Batch(2400/3375): \tTrain Loss  0.1358\n",
            "Epoch(4/10)\t Batch(2600/3375): \tTrain Loss  0.1406\n",
            "Epoch(4/10)\t Batch(2800/3375): \tTrain Loss  0.1157\n",
            "Epoch(4/10)\t Batch(3000/3375): \tTrain Loss  0.1666\n",
            "Epoch(4/10)\t Batch(3200/3375): \tTrain Loss  0.1225\n",
            "End of Epoch: 4 Train Accuracy: 0.2655 Val Loss: 0.1333 Val Accuracy: 0.2500\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/3375): \tTrain Loss  0.1457\n",
            "Epoch(5/10)\t Batch(200/3375): \tTrain Loss  0.1314\n",
            "Epoch(5/10)\t Batch(400/3375): \tTrain Loss  0.1180\n",
            "Epoch(5/10)\t Batch(600/3375): \tTrain Loss  0.0997\n",
            "Epoch(5/10)\t Batch(800/3375): \tTrain Loss  0.1323\n",
            "Epoch(5/10)\t Batch(1000/3375): \tTrain Loss  0.1126\n",
            "Epoch(5/10)\t Batch(1200/3375): \tTrain Loss  0.1091\n",
            "Epoch(5/10)\t Batch(1400/3375): \tTrain Loss  0.1382\n",
            "Epoch(5/10)\t Batch(1600/3375): \tTrain Loss  0.1458\n",
            "Epoch(5/10)\t Batch(1800/3375): \tTrain Loss  0.1182\n",
            "Epoch(5/10)\t Batch(2000/3375): \tTrain Loss  0.1730\n",
            "Epoch(5/10)\t Batch(2200/3375): \tTrain Loss  0.1575\n",
            "Epoch(5/10)\t Batch(2400/3375): \tTrain Loss  0.1327\n",
            "Epoch(5/10)\t Batch(2600/3375): \tTrain Loss  0.1254\n",
            "Epoch(5/10)\t Batch(2800/3375): \tTrain Loss  0.1320\n",
            "Epoch(5/10)\t Batch(3000/3375): \tTrain Loss  0.1115\n",
            "Epoch(5/10)\t Batch(3200/3375): \tTrain Loss  0.1158\n",
            "End of Epoch: 5 Train Accuracy: 0.2899 Val Loss: 0.1294 Val Accuracy: 0.2697\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/3375): \tTrain Loss  0.1476\n",
            "Epoch(6/10)\t Batch(200/3375): \tTrain Loss  0.0910\n",
            "Epoch(6/10)\t Batch(400/3375): \tTrain Loss  0.1139\n",
            "Epoch(6/10)\t Batch(600/3375): \tTrain Loss  0.1044\n",
            "Epoch(6/10)\t Batch(800/3375): \tTrain Loss  0.1566\n",
            "Epoch(6/10)\t Batch(1000/3375): \tTrain Loss  0.1049\n",
            "Epoch(6/10)\t Batch(1200/3375): \tTrain Loss  0.1324\n",
            "Epoch(6/10)\t Batch(1400/3375): \tTrain Loss  0.1284\n",
            "Epoch(6/10)\t Batch(1600/3375): \tTrain Loss  0.1394\n",
            "Epoch(6/10)\t Batch(1800/3375): \tTrain Loss  0.1171\n",
            "Epoch(6/10)\t Batch(2000/3375): \tTrain Loss  0.1029\n",
            "Epoch(6/10)\t Batch(2200/3375): \tTrain Loss  0.1233\n",
            "Epoch(6/10)\t Batch(2400/3375): \tTrain Loss  0.1053\n",
            "Epoch(6/10)\t Batch(2600/3375): \tTrain Loss  0.0764\n",
            "Epoch(6/10)\t Batch(2800/3375): \tTrain Loss  0.1144\n",
            "Epoch(6/10)\t Batch(3000/3375): \tTrain Loss  0.1199\n",
            "Epoch(6/10)\t Batch(3200/3375): \tTrain Loss  0.1148\n",
            "End of Epoch: 6 Train Accuracy: 0.3226 Val Loss: 0.1239 Val Accuracy: 0.3000\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/3375): \tTrain Loss  0.1115\n",
            "Epoch(7/10)\t Batch(200/3375): \tTrain Loss  0.1270\n",
            "Epoch(7/10)\t Batch(400/3375): \tTrain Loss  0.1347\n",
            "Epoch(7/10)\t Batch(600/3375): \tTrain Loss  0.1520\n",
            "Epoch(7/10)\t Batch(800/3375): \tTrain Loss  0.1405\n",
            "Epoch(7/10)\t Batch(1000/3375): \tTrain Loss  0.1362\n",
            "Epoch(7/10)\t Batch(1200/3375): \tTrain Loss  0.1293\n",
            "Epoch(7/10)\t Batch(1400/3375): \tTrain Loss  0.0819\n",
            "Epoch(7/10)\t Batch(1600/3375): \tTrain Loss  0.1017\n",
            "Epoch(7/10)\t Batch(1800/3375): \tTrain Loss  0.1407\n",
            "Epoch(7/10)\t Batch(2000/3375): \tTrain Loss  0.1155\n",
            "Epoch(7/10)\t Batch(2200/3375): \tTrain Loss  0.1056\n",
            "Epoch(7/10)\t Batch(2400/3375): \tTrain Loss  0.1488\n",
            "Epoch(7/10)\t Batch(2600/3375): \tTrain Loss  0.0942\n",
            "Epoch(7/10)\t Batch(2800/3375): \tTrain Loss  0.1336\n",
            "Epoch(7/10)\t Batch(3000/3375): \tTrain Loss  0.1292\n",
            "Epoch(7/10)\t Batch(3200/3375): \tTrain Loss  0.0691\n",
            "End of Epoch: 7 Train Accuracy: 0.3515 Val Loss: 0.1179 Val Accuracy: 0.3328\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/3375): \tTrain Loss  0.1401\n",
            "Epoch(8/10)\t Batch(200/3375): \tTrain Loss  0.1330\n",
            "Epoch(8/10)\t Batch(400/3375): \tTrain Loss  0.0957\n",
            "Epoch(8/10)\t Batch(600/3375): \tTrain Loss  0.1137\n",
            "Epoch(8/10)\t Batch(800/3375): \tTrain Loss  0.1116\n",
            "Epoch(8/10)\t Batch(1000/3375): \tTrain Loss  0.1200\n",
            "Epoch(8/10)\t Batch(1200/3375): \tTrain Loss  0.0879\n",
            "Epoch(8/10)\t Batch(1400/3375): \tTrain Loss  0.0520\n",
            "Epoch(8/10)\t Batch(1600/3375): \tTrain Loss  0.1125\n",
            "Epoch(8/10)\t Batch(1800/3375): \tTrain Loss  0.0854\n",
            "Epoch(8/10)\t Batch(2000/3375): \tTrain Loss  0.0961\n",
            "Epoch(8/10)\t Batch(2200/3375): \tTrain Loss  0.1302\n",
            "Epoch(8/10)\t Batch(2400/3375): \tTrain Loss  0.0824\n",
            "Epoch(8/10)\t Batch(2600/3375): \tTrain Loss  0.0917\n",
            "Epoch(8/10)\t Batch(2800/3375): \tTrain Loss  0.0681\n",
            "Epoch(8/10)\t Batch(3000/3375): \tTrain Loss  0.0865\n",
            "Epoch(8/10)\t Batch(3200/3375): \tTrain Loss  0.1242\n",
            "End of Epoch: 8 Train Accuracy: 0.3757 Val Loss: 0.1141 Val Accuracy: 0.3533\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/3375): \tTrain Loss  0.1106\n",
            "Epoch(9/10)\t Batch(200/3375): \tTrain Loss  0.0811\n",
            "Epoch(9/10)\t Batch(400/3375): \tTrain Loss  0.1238\n",
            "Epoch(9/10)\t Batch(600/3375): \tTrain Loss  0.1169\n",
            "Epoch(9/10)\t Batch(800/3375): \tTrain Loss  0.1631\n",
            "Epoch(9/10)\t Batch(1000/3375): \tTrain Loss  0.1149\n",
            "Epoch(9/10)\t Batch(1200/3375): \tTrain Loss  0.1321\n",
            "Epoch(9/10)\t Batch(1400/3375): \tTrain Loss  0.1333\n",
            "Epoch(9/10)\t Batch(1600/3375): \tTrain Loss  0.1191\n",
            "Epoch(9/10)\t Batch(1800/3375): \tTrain Loss  0.0960\n",
            "Epoch(9/10)\t Batch(2000/3375): \tTrain Loss  0.1316\n",
            "Epoch(9/10)\t Batch(2200/3375): \tTrain Loss  0.1666\n",
            "Epoch(9/10)\t Batch(2400/3375): \tTrain Loss  0.1450\n",
            "Epoch(9/10)\t Batch(2600/3375): \tTrain Loss  0.1155\n",
            "Epoch(9/10)\t Batch(2800/3375): \tTrain Loss  0.1112\n",
            "Epoch(9/10)\t Batch(3000/3375): \tTrain Loss  0.1374\n",
            "Epoch(9/10)\t Batch(3200/3375): \tTrain Loss  0.1315\n",
            "End of Epoch: 9 Train Accuracy: 0.3946 Val Loss: 0.1115 Val Accuracy: 0.3707\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/3375): \tTrain Loss  0.1139\n",
            "Epoch(10/10)\t Batch(200/3375): \tTrain Loss  0.1306\n",
            "Epoch(10/10)\t Batch(400/3375): \tTrain Loss  0.1457\n",
            "Epoch(10/10)\t Batch(600/3375): \tTrain Loss  0.1217\n",
            "Epoch(10/10)\t Batch(800/3375): \tTrain Loss  0.1525\n",
            "Epoch(10/10)\t Batch(1000/3375): \tTrain Loss  0.0942\n",
            "Epoch(10/10)\t Batch(1200/3375): \tTrain Loss  0.1460\n",
            "Epoch(10/10)\t Batch(1400/3375): \tTrain Loss  0.1025\n",
            "Epoch(10/10)\t Batch(1600/3375): \tTrain Loss  0.1086\n",
            "Epoch(10/10)\t Batch(1800/3375): \tTrain Loss  0.1023\n",
            "Epoch(10/10)\t Batch(2000/3375): \tTrain Loss  0.0853\n",
            "Epoch(10/10)\t Batch(2200/3375): \tTrain Loss  0.0922\n",
            "Epoch(10/10)\t Batch(2400/3375): \tTrain Loss  0.0575\n",
            "Epoch(10/10)\t Batch(2600/3375): \tTrain Loss  0.0907\n",
            "Epoch(10/10)\t Batch(2800/3375): \tTrain Loss  0.1282\n",
            "Epoch(10/10)\t Batch(3000/3375): \tTrain Loss  0.1102\n",
            "Epoch(10/10)\t Batch(3200/3375): \tTrain Loss  0.1039\n",
            "End of Epoch: 10 Train Accuracy: 0.4064 Val Loss: 0.1077 Val Accuracy: 0.3907\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▄▅▅▆▇▇██</td></tr><tr><td>Train Loss</td><td>█▅▅▇▅▄▄▁▄▅</td></tr><tr><td>Val Accuracy</td><td>▁▂▃▄▅▅▆▇▇██</td></tr><tr><td>Val Loss</td><td>█▇▆▅▄▄▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.40641</td></tr><tr><td>Train Loss</td><td>0.12577</td></tr><tr><td>Val Accuracy</td><td>0.39067</td></tr><tr><td>Val Loss</td><td>0.10772</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_sgd|loss_mse|lr=0.001|batch_16|act_tanh|hid_4|neurons_64|nrns_10|init_Random935</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/iicblsax' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/iicblsax</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_165457-iicblsax/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vho53ggs with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_165653-vho53ggs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/vho53ggs' target=\"_blank\">breezy-sweep-20</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/vho53ggs' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/vho53ggs</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0734 Val Loss: 25.6376 Val Accuracy: 0.0718\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  27.1993\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  25.9041\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  24.6089\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  25.0406\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  26.6738\n",
            "End of Epoch: 1 Train Accuracy: 0.0807 Val Loss: 25.2093 Val Accuracy: 0.0832\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  25.4723\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  27.1647\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  26.7676\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  25.6952\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  26.7676\n",
            "End of Epoch: 2 Train Accuracy: 0.0795 Val Loss: 25.1586 Val Accuracy: 0.0838\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  26.3358\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  23.3430\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  24.8887\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  25.4727\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  25.1572\n",
            "End of Epoch: 3 Train Accuracy: 0.0782 Val Loss: 25.1106 Val Accuracy: 0.0802\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  25.3081\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  25.4626\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  24.8784\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  23.8436\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  26.1774\n",
            "End of Epoch: 4 Train Accuracy: 0.0784 Val Loss: 24.9982 Val Accuracy: 0.0795\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  23.6614\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  25.2059\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  26.7522\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  25.6475\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  25.6439\n",
            "End of Epoch: 5 Train Accuracy: 0.0781 Val Loss: 24.8209 Val Accuracy: 0.0812\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  24.5524\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  24.2082\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  24.9015\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  26.7723\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  25.8629\n",
            "End of Epoch: 6 Train Accuracy: 0.0783 Val Loss: 24.5954 Val Accuracy: 0.0777\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  23.5651\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  24.5577\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  25.3796\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  25.2625\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  25.6074\n",
            "End of Epoch: 7 Train Accuracy: 0.0791 Val Loss: 24.2200 Val Accuracy: 0.0805\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  24.1785\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  23.2483\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  22.7056\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  24.3266\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  23.4201\n",
            "End of Epoch: 8 Train Accuracy: 0.0803 Val Loss: 23.7012 Val Accuracy: 0.0855\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  23.8237\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  22.5207\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  22.7525\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  22.4988\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  25.3914\n",
            "End of Epoch: 9 Train Accuracy: 0.0824 Val Loss: 22.9501 Val Accuracy: 0.0840\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  23.4752\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  23.4540\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  23.6256\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  23.3399\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  21.9009\n",
            "End of Epoch: 10 Train Accuracy: 0.0840 Val Loss: 21.8995 Val Accuracy: 0.0833\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▆▅▄▄▄▄▅▆▇█</td></tr><tr><td>Train Loss</td><td>▃▆▅▇█▅▃▆▂▁</td></tr><tr><td>Val Accuracy</td><td>▁▇▇▅▅▆▄▅█▇▇</td></tr><tr><td>Val Loss</td><td>█▇▇▇▇▆▆▅▄▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.08398</td></tr><tr><td>Train Loss</td><td>22.76525</td></tr><tr><td>Val Accuracy</td><td>0.08333</td></tr><tr><td>Val Loss</td><td>21.8995</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_adam|loss_cross_entropy|lr=0.0001|batch_64|act_relu|hid_4|neurons_32|nrns_10|init_Random654</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/vho53ggs' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/vho53ggs</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_165653-vho53ggs/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ied4vgvr with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_165729-ied4vgvr</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ied4vgvr' target=\"_blank\">lively-sweep-21</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ied4vgvr' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ied4vgvr</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.1426 Val Loss: 14.3422 Val Accuracy: 0.1425\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/1687): \tTrain Loss  16.2000\n",
            "Epoch(1/10)\t Batch(200/1687): \tTrain Loss  2.4484\n",
            "Epoch(1/10)\t Batch(400/1687): \tTrain Loss  2.3024\n",
            "Epoch(1/10)\t Batch(600/1687): \tTrain Loss  2.3003\n",
            "Epoch(1/10)\t Batch(800/1687): \tTrain Loss  2.3074\n",
            "Epoch(1/10)\t Batch(1000/1687): \tTrain Loss  2.2881\n",
            "Epoch(1/10)\t Batch(1200/1687): \tTrain Loss  2.2282\n",
            "Epoch(1/10)\t Batch(1400/1687): \tTrain Loss  2.0105\n",
            "Epoch(1/10)\t Batch(1600/1687): \tTrain Loss  1.9562\n",
            "End of Epoch: 1 Train Accuracy: 0.7134 Val Loss: 1.9373 Val Accuracy: 0.7148\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/1687): \tTrain Loss  1.8953\n",
            "Epoch(2/10)\t Batch(200/1687): \tTrain Loss  1.8183\n",
            "Epoch(2/10)\t Batch(400/1687): \tTrain Loss  1.8745\n",
            "Epoch(2/10)\t Batch(600/1687): \tTrain Loss  1.7238\n",
            "Epoch(2/10)\t Batch(800/1687): \tTrain Loss  1.6778\n",
            "Epoch(2/10)\t Batch(1000/1687): \tTrain Loss  1.7509\n",
            "Epoch(2/10)\t Batch(1200/1687): \tTrain Loss  1.7594\n",
            "Epoch(2/10)\t Batch(1400/1687): \tTrain Loss  1.7637\n",
            "Epoch(2/10)\t Batch(1600/1687): \tTrain Loss  1.6917\n",
            "End of Epoch: 2 Train Accuracy: 0.8393 Val Loss: 1.7013 Val Accuracy: 0.8305\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/1687): \tTrain Loss  1.6925\n",
            "Epoch(3/10)\t Batch(200/1687): \tTrain Loss  1.7168\n",
            "Epoch(3/10)\t Batch(400/1687): \tTrain Loss  1.7906\n",
            "Epoch(3/10)\t Batch(600/1687): \tTrain Loss  1.6201\n",
            "Epoch(3/10)\t Batch(800/1687): \tTrain Loss  1.6424\n",
            "Epoch(3/10)\t Batch(1000/1687): \tTrain Loss  1.6926\n",
            "Epoch(3/10)\t Batch(1200/1687): \tTrain Loss  1.6373\n",
            "Epoch(3/10)\t Batch(1400/1687): \tTrain Loss  1.6304\n",
            "Epoch(3/10)\t Batch(1600/1687): \tTrain Loss  1.7071\n",
            "End of Epoch: 3 Train Accuracy: 0.8653 Val Loss: 1.6798 Val Accuracy: 0.8595\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/1687): \tTrain Loss  1.6278\n",
            "Epoch(4/10)\t Batch(200/1687): \tTrain Loss  1.6912\n",
            "Epoch(4/10)\t Batch(400/1687): \tTrain Loss  1.6716\n",
            "Epoch(4/10)\t Batch(600/1687): \tTrain Loss  1.6417\n",
            "Epoch(4/10)\t Batch(800/1687): \tTrain Loss  1.5906\n",
            "Epoch(4/10)\t Batch(1000/1687): \tTrain Loss  1.7598\n",
            "Epoch(4/10)\t Batch(1200/1687): \tTrain Loss  1.6817\n",
            "Epoch(4/10)\t Batch(1400/1687): \tTrain Loss  1.6295\n",
            "Epoch(4/10)\t Batch(1600/1687): \tTrain Loss  1.6425\n",
            "End of Epoch: 4 Train Accuracy: 0.8573 Val Loss: 1.6614 Val Accuracy: 0.8513\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/1687): \tTrain Loss  1.6675\n",
            "Epoch(5/10)\t Batch(200/1687): \tTrain Loss  1.6918\n",
            "Epoch(5/10)\t Batch(400/1687): \tTrain Loss  1.6545\n",
            "Epoch(5/10)\t Batch(600/1687): \tTrain Loss  1.6197\n",
            "Epoch(5/10)\t Batch(800/1687): \tTrain Loss  1.6472\n",
            "Epoch(5/10)\t Batch(1000/1687): \tTrain Loss  1.7075\n",
            "Epoch(5/10)\t Batch(1200/1687): \tTrain Loss  1.6893\n",
            "Epoch(5/10)\t Batch(1400/1687): \tTrain Loss  1.6490\n",
            "Epoch(5/10)\t Batch(1600/1687): \tTrain Loss  1.6964\n",
            "End of Epoch: 5 Train Accuracy: 0.8744 Val Loss: 1.6504 Val Accuracy: 0.8658\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/1687): \tTrain Loss  1.6244\n",
            "Epoch(6/10)\t Batch(200/1687): \tTrain Loss  1.7252\n",
            "Epoch(6/10)\t Batch(400/1687): \tTrain Loss  1.6735\n",
            "Epoch(6/10)\t Batch(600/1687): \tTrain Loss  1.6184\n",
            "Epoch(6/10)\t Batch(800/1687): \tTrain Loss  1.6665\n",
            "Epoch(6/10)\t Batch(1000/1687): \tTrain Loss  1.6931\n",
            "Epoch(6/10)\t Batch(1200/1687): \tTrain Loss  1.6070\n",
            "Epoch(6/10)\t Batch(1400/1687): \tTrain Loss  1.6475\n",
            "Epoch(6/10)\t Batch(1600/1687): \tTrain Loss  1.7342\n",
            "End of Epoch: 6 Train Accuracy: 0.8410 Val Loss: 1.6642 Val Accuracy: 0.8307\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/1687): \tTrain Loss  1.6082\n",
            "Epoch(7/10)\t Batch(200/1687): \tTrain Loss  1.6482\n",
            "Epoch(7/10)\t Batch(400/1687): \tTrain Loss  1.6510\n",
            "Epoch(7/10)\t Batch(600/1687): \tTrain Loss  1.6412\n",
            "Epoch(7/10)\t Batch(800/1687): \tTrain Loss  1.6392\n",
            "Epoch(7/10)\t Batch(1000/1687): \tTrain Loss  1.6873\n",
            "Epoch(7/10)\t Batch(1200/1687): \tTrain Loss  1.6421\n",
            "Epoch(7/10)\t Batch(1400/1687): \tTrain Loss  1.6187\n",
            "Epoch(7/10)\t Batch(1600/1687): \tTrain Loss  1.6091\n",
            "End of Epoch: 7 Train Accuracy: 0.8677 Val Loss: 1.6624 Val Accuracy: 0.8578\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/1687): \tTrain Loss  1.6698\n",
            "Epoch(8/10)\t Batch(200/1687): \tTrain Loss  1.6759\n",
            "Epoch(8/10)\t Batch(400/1687): \tTrain Loss  1.6971\n",
            "Epoch(8/10)\t Batch(600/1687): \tTrain Loss  1.7072\n",
            "Epoch(8/10)\t Batch(800/1687): \tTrain Loss  1.7711\n",
            "Epoch(8/10)\t Batch(1000/1687): \tTrain Loss  1.5967\n",
            "Epoch(8/10)\t Batch(1200/1687): \tTrain Loss  1.6664\n",
            "Epoch(8/10)\t Batch(1400/1687): \tTrain Loss  1.6221\n",
            "Epoch(8/10)\t Batch(1600/1687): \tTrain Loss  1.6385\n",
            "End of Epoch: 8 Train Accuracy: 0.8633 Val Loss: 1.6440 Val Accuracy: 0.8627\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/1687): \tTrain Loss  1.6856\n",
            "Epoch(9/10)\t Batch(200/1687): \tTrain Loss  1.6573\n",
            "Epoch(9/10)\t Batch(400/1687): \tTrain Loss  1.6170\n",
            "Epoch(9/10)\t Batch(600/1687): \tTrain Loss  1.7340\n",
            "Epoch(9/10)\t Batch(800/1687): \tTrain Loss  1.6267\n",
            "Epoch(9/10)\t Batch(1000/1687): \tTrain Loss  1.5895\n",
            "Epoch(9/10)\t Batch(1200/1687): \tTrain Loss  1.7090\n",
            "Epoch(9/10)\t Batch(1400/1687): \tTrain Loss  1.6927\n",
            "Epoch(9/10)\t Batch(1600/1687): \tTrain Loss  1.6481\n",
            "End of Epoch: 9 Train Accuracy: 0.8679 Val Loss: 1.6506 Val Accuracy: 0.8618\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/1687): \tTrain Loss  1.5848\n",
            "Epoch(10/10)\t Batch(200/1687): \tTrain Loss  1.6147\n",
            "Epoch(10/10)\t Batch(400/1687): \tTrain Loss  1.5999\n",
            "Epoch(10/10)\t Batch(600/1687): \tTrain Loss  1.7196\n",
            "Epoch(10/10)\t Batch(800/1687): \tTrain Loss  1.6575\n",
            "Epoch(10/10)\t Batch(1000/1687): \tTrain Loss  1.7287\n",
            "Epoch(10/10)\t Batch(1200/1687): \tTrain Loss  1.6147\n",
            "Epoch(10/10)\t Batch(1400/1687): \tTrain Loss  1.6660\n",
            "Epoch(10/10)\t Batch(1600/1687): \tTrain Loss  1.7066\n",
            "End of Epoch: 10 Train Accuracy: 0.8497 Val Loss: 1.6635 Val Accuracy: 0.8422\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▆█████████</td></tr><tr><td>Train Loss</td><td>█▄▂▁▆▁▂▃▁▇</td></tr><tr><td>Val Accuracy</td><td>▁▇█████████</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.84969</td></tr><tr><td>Train Loss</td><td>1.80401</td></tr><tr><td>Val Accuracy</td><td>0.84217</td></tr><tr><td>Val Loss</td><td>1.6635</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nadam|loss_cross_entropy|lr=0.001|batch_32|act_tanh|hid_3|neurons_128|nrns_10|init_Random505</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ied4vgvr' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ied4vgvr</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_165729-ied4vgvr/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c9u2s3pu with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_170014-c9u2s3pu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/c9u2s3pu' target=\"_blank\">balmy-sweep-22</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/c9u2s3pu' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/c9u2s3pu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0809 Val Loss: 0.0902 Val Accuracy: 0.0830\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/1687): \tTrain Loss  0.0901\n",
            "Epoch(1/5)\t Batch(200/1687): \tTrain Loss  0.0417\n",
            "Epoch(1/5)\t Batch(400/1687): \tTrain Loss  0.0430\n",
            "Epoch(1/5)\t Batch(600/1687): \tTrain Loss  0.0380\n",
            "Epoch(1/5)\t Batch(800/1687): \tTrain Loss  0.0318\n",
            "Epoch(1/5)\t Batch(1000/1687): \tTrain Loss  0.0283\n",
            "Epoch(1/5)\t Batch(1200/1687): \tTrain Loss  0.0316\n",
            "Epoch(1/5)\t Batch(1400/1687): \tTrain Loss  0.0313\n",
            "Epoch(1/5)\t Batch(1600/1687): \tTrain Loss  0.0436\n",
            "End of Epoch: 1 Train Accuracy: 0.8121 Val Loss: 0.0279 Val Accuracy: 0.8103\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/1687): \tTrain Loss  0.0279\n",
            "Epoch(2/5)\t Batch(200/1687): \tTrain Loss  0.0307\n",
            "Epoch(2/5)\t Batch(400/1687): \tTrain Loss  0.0277\n",
            "Epoch(2/5)\t Batch(600/1687): \tTrain Loss  0.0228\n",
            "Epoch(2/5)\t Batch(800/1687): \tTrain Loss  0.0316\n",
            "Epoch(2/5)\t Batch(1000/1687): \tTrain Loss  0.0225\n",
            "Epoch(2/5)\t Batch(1200/1687): \tTrain Loss  0.0135\n",
            "Epoch(2/5)\t Batch(1400/1687): \tTrain Loss  0.0295\n",
            "Epoch(2/5)\t Batch(1600/1687): \tTrain Loss  0.0210\n",
            "End of Epoch: 2 Train Accuracy: 0.8290 Val Loss: 0.0253 Val Accuracy: 0.8195\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/1687): \tTrain Loss  0.0347\n",
            "Epoch(3/5)\t Batch(200/1687): \tTrain Loss  0.0207\n",
            "Epoch(3/5)\t Batch(400/1687): \tTrain Loss  0.0271\n",
            "Epoch(3/5)\t Batch(600/1687): \tTrain Loss  0.0130\n",
            "Epoch(3/5)\t Batch(800/1687): \tTrain Loss  0.0298\n",
            "Epoch(3/5)\t Batch(1000/1687): \tTrain Loss  0.0148\n",
            "Epoch(3/5)\t Batch(1200/1687): \tTrain Loss  0.0207\n",
            "Epoch(3/5)\t Batch(1400/1687): \tTrain Loss  0.0136\n",
            "Epoch(3/5)\t Batch(1600/1687): \tTrain Loss  0.0276\n",
            "End of Epoch: 3 Train Accuracy: 0.8574 Val Loss: 0.0221 Val Accuracy: 0.8457\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/1687): \tTrain Loss  0.0309\n",
            "Epoch(4/5)\t Batch(200/1687): \tTrain Loss  0.0196\n",
            "Epoch(4/5)\t Batch(400/1687): \tTrain Loss  0.0218\n",
            "Epoch(4/5)\t Batch(600/1687): \tTrain Loss  0.0115\n",
            "Epoch(4/5)\t Batch(800/1687): \tTrain Loss  0.0282\n",
            "Epoch(4/5)\t Batch(1000/1687): \tTrain Loss  0.0181\n",
            "Epoch(4/5)\t Batch(1200/1687): \tTrain Loss  0.0278\n",
            "Epoch(4/5)\t Batch(1400/1687): \tTrain Loss  0.0230\n",
            "Epoch(4/5)\t Batch(1600/1687): \tTrain Loss  0.0160\n",
            "End of Epoch: 4 Train Accuracy: 0.8700 Val Loss: 0.0206 Val Accuracy: 0.8563\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/1687): \tTrain Loss  0.0121\n",
            "Epoch(5/5)\t Batch(200/1687): \tTrain Loss  0.0238\n",
            "Epoch(5/5)\t Batch(400/1687): \tTrain Loss  0.0286\n",
            "Epoch(5/5)\t Batch(600/1687): \tTrain Loss  0.0343\n",
            "Epoch(5/5)\t Batch(800/1687): \tTrain Loss  0.0166\n",
            "Epoch(5/5)\t Batch(1000/1687): \tTrain Loss  0.0165\n",
            "Epoch(5/5)\t Batch(1200/1687): \tTrain Loss  0.0151\n",
            "Epoch(5/5)\t Batch(1400/1687): \tTrain Loss  0.0275\n",
            "Epoch(5/5)\t Batch(1600/1687): \tTrain Loss  0.0082\n",
            "End of Epoch: 5 Train Accuracy: 0.8669 Val Loss: 0.0211 Val Accuracy: 0.8508\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▇████</td></tr><tr><td>Train Loss</td><td>▄█▇▁▅</td></tr><tr><td>Val Accuracy</td><td>▁█████</td></tr><tr><td>Val Loss</td><td>█▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.86693</td></tr><tr><td>Train Loss</td><td>0.02251</td></tr><tr><td>Val Accuracy</td><td>0.85083</td></tr><tr><td>Val Loss</td><td>0.02114</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_mse|lr=0.0001|batch_32|act_relu|hid_4|neurons_128|nrns_5|init_Xavier565</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/c9u2s3pu' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/c9u2s3pu</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_170014-c9u2s3pu/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: khcwg627 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nag\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_170131-khcwg627</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/khcwg627' target=\"_blank\">vital-sweep-23</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/khcwg627' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/khcwg627</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.1200 Val Loss: 12.1281 Val Accuracy: 0.1220\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/1687): \tTrain Loss  13.8456\n",
            "Epoch(1/10)\t Batch(200/1687): \tTrain Loss  2.3003\n",
            "Epoch(1/10)\t Batch(400/1687): \tTrain Loss  2.2938\n",
            "Epoch(1/10)\t Batch(600/1687): \tTrain Loss  2.2955\n",
            "Epoch(1/10)\t Batch(800/1687): \tTrain Loss  2.3045\n",
            "Epoch(1/10)\t Batch(1000/1687): \tTrain Loss  2.2908\n",
            "Epoch(1/10)\t Batch(1200/1687): \tTrain Loss  2.2900\n",
            "Epoch(1/10)\t Batch(1400/1687): \tTrain Loss  2.2924\n",
            "Epoch(1/10)\t Batch(1600/1687): \tTrain Loss  2.2947\n",
            "End of Epoch: 1 Train Accuracy: 0.1432 Val Loss: 2.2888 Val Accuracy: 0.1552\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/1687): \tTrain Loss  2.2960\n",
            "Epoch(2/10)\t Batch(200/1687): \tTrain Loss  2.2877\n",
            "Epoch(2/10)\t Batch(400/1687): \tTrain Loss  2.2878\n",
            "Epoch(2/10)\t Batch(600/1687): \tTrain Loss  2.2905\n",
            "Epoch(2/10)\t Batch(800/1687): \tTrain Loss  2.2918\n",
            "Epoch(2/10)\t Batch(1000/1687): \tTrain Loss  2.2843\n",
            "Epoch(2/10)\t Batch(1200/1687): \tTrain Loss  2.2982\n",
            "Epoch(2/10)\t Batch(1400/1687): \tTrain Loss  2.2871\n",
            "Epoch(2/10)\t Batch(1600/1687): \tTrain Loss  2.2816\n",
            "End of Epoch: 2 Train Accuracy: 0.1814 Val Loss: 2.2831 Val Accuracy: 0.1920\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/1687): \tTrain Loss  2.2930\n",
            "Epoch(3/10)\t Batch(200/1687): \tTrain Loss  2.2821\n",
            "Epoch(3/10)\t Batch(400/1687): \tTrain Loss  2.2817\n",
            "Epoch(3/10)\t Batch(600/1687): \tTrain Loss  2.2715\n",
            "Epoch(3/10)\t Batch(800/1687): \tTrain Loss  2.2757\n",
            "Epoch(3/10)\t Batch(1000/1687): \tTrain Loss  2.2818\n",
            "Epoch(3/10)\t Batch(1200/1687): \tTrain Loss  2.2758\n",
            "Epoch(3/10)\t Batch(1400/1687): \tTrain Loss  2.2702\n",
            "Epoch(3/10)\t Batch(1600/1687): \tTrain Loss  2.2515\n",
            "End of Epoch: 3 Train Accuracy: 0.1756 Val Loss: 2.2679 Val Accuracy: 0.1813\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/1687): \tTrain Loss  2.2787\n",
            "Epoch(4/10)\t Batch(200/1687): \tTrain Loss  2.2736\n",
            "Epoch(4/10)\t Batch(400/1687): \tTrain Loss  2.2734\n",
            "Epoch(4/10)\t Batch(600/1687): \tTrain Loss  2.2704\n",
            "Epoch(4/10)\t Batch(800/1687): \tTrain Loss  2.2566\n",
            "Epoch(4/10)\t Batch(1000/1687): \tTrain Loss  2.2788\n",
            "Epoch(4/10)\t Batch(1200/1687): \tTrain Loss  2.2592\n",
            "Epoch(4/10)\t Batch(1400/1687): \tTrain Loss  2.2567\n",
            "Epoch(4/10)\t Batch(1600/1687): \tTrain Loss  2.2745\n",
            "End of Epoch: 4 Train Accuracy: 0.1877 Val Loss: 2.2624 Val Accuracy: 0.1797\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/1687): \tTrain Loss  2.2654\n",
            "Epoch(5/10)\t Batch(200/1687): \tTrain Loss  2.2480\n",
            "Epoch(5/10)\t Batch(400/1687): \tTrain Loss  2.2623\n",
            "Epoch(5/10)\t Batch(600/1687): \tTrain Loss  2.2605\n",
            "Epoch(5/10)\t Batch(800/1687): \tTrain Loss  2.2583\n",
            "Epoch(5/10)\t Batch(1000/1687): \tTrain Loss  2.2748\n",
            "Epoch(5/10)\t Batch(1200/1687): \tTrain Loss  2.2618\n",
            "Epoch(5/10)\t Batch(1400/1687): \tTrain Loss  2.2474\n",
            "Epoch(5/10)\t Batch(1600/1687): \tTrain Loss  2.2462\n",
            "End of Epoch: 5 Train Accuracy: 0.1925 Val Loss: 2.2572 Val Accuracy: 0.2020\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/1687): \tTrain Loss  2.2591\n",
            "Epoch(6/10)\t Batch(200/1687): \tTrain Loss  2.2470\n",
            "Epoch(6/10)\t Batch(400/1687): \tTrain Loss  2.2405\n",
            "Epoch(6/10)\t Batch(600/1687): \tTrain Loss  2.2708\n",
            "Epoch(6/10)\t Batch(800/1687): \tTrain Loss  2.2380\n",
            "Epoch(6/10)\t Batch(1000/1687): \tTrain Loss  2.2685\n",
            "Epoch(6/10)\t Batch(1200/1687): \tTrain Loss  2.2623\n",
            "Epoch(6/10)\t Batch(1400/1687): \tTrain Loss  2.2558\n",
            "Epoch(6/10)\t Batch(1600/1687): \tTrain Loss  2.2404\n",
            "End of Epoch: 6 Train Accuracy: 0.2630 Val Loss: 2.2502 Val Accuracy: 0.2563\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/1687): \tTrain Loss  2.2359\n",
            "Epoch(7/10)\t Batch(200/1687): \tTrain Loss  2.2318\n",
            "Epoch(7/10)\t Batch(400/1687): \tTrain Loss  2.2231\n",
            "Epoch(7/10)\t Batch(600/1687): \tTrain Loss  2.2665\n",
            "Epoch(7/10)\t Batch(800/1687): \tTrain Loss  2.2602\n",
            "Epoch(7/10)\t Batch(1000/1687): \tTrain Loss  2.2493\n",
            "Epoch(7/10)\t Batch(1200/1687): \tTrain Loss  2.2414\n",
            "Epoch(7/10)\t Batch(1400/1687): \tTrain Loss  2.2195\n",
            "Epoch(7/10)\t Batch(1600/1687): \tTrain Loss  2.2461\n",
            "End of Epoch: 7 Train Accuracy: 0.2180 Val Loss: 2.2443 Val Accuracy: 0.2117\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/1687): \tTrain Loss  2.2471\n",
            "Epoch(8/10)\t Batch(200/1687): \tTrain Loss  2.2513\n",
            "Epoch(8/10)\t Batch(400/1687): \tTrain Loss  2.2447\n",
            "Epoch(8/10)\t Batch(600/1687): \tTrain Loss  2.2383\n",
            "Epoch(8/10)\t Batch(800/1687): \tTrain Loss  2.2562\n",
            "Epoch(8/10)\t Batch(1000/1687): \tTrain Loss  2.2267\n",
            "Epoch(8/10)\t Batch(1200/1687): \tTrain Loss  2.2556\n",
            "Epoch(8/10)\t Batch(1400/1687): \tTrain Loss  2.2260\n",
            "Epoch(8/10)\t Batch(1600/1687): \tTrain Loss  2.2504\n",
            "End of Epoch: 8 Train Accuracy: 0.2477 Val Loss: 2.2387 Val Accuracy: 0.2458\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/1687): \tTrain Loss  2.2367\n",
            "Epoch(9/10)\t Batch(200/1687): \tTrain Loss  2.2392\n",
            "Epoch(9/10)\t Batch(400/1687): \tTrain Loss  2.2326\n",
            "Epoch(9/10)\t Batch(600/1687): \tTrain Loss  2.2425\n",
            "Epoch(9/10)\t Batch(800/1687): \tTrain Loss  2.2502\n",
            "Epoch(9/10)\t Batch(1000/1687): \tTrain Loss  2.2254\n",
            "Epoch(9/10)\t Batch(1200/1687): \tTrain Loss  2.2087\n",
            "Epoch(9/10)\t Batch(1400/1687): \tTrain Loss  2.2177\n",
            "Epoch(9/10)\t Batch(1600/1687): \tTrain Loss  2.2387\n",
            "End of Epoch: 9 Train Accuracy: 0.3005 Val Loss: 2.2217 Val Accuracy: 0.3003\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/1687): \tTrain Loss  2.2004\n",
            "Epoch(10/10)\t Batch(200/1687): \tTrain Loss  2.2159\n",
            "Epoch(10/10)\t Batch(400/1687): \tTrain Loss  2.2039\n",
            "Epoch(10/10)\t Batch(600/1687): \tTrain Loss  2.2177\n",
            "Epoch(10/10)\t Batch(800/1687): \tTrain Loss  2.2133\n",
            "Epoch(10/10)\t Batch(1000/1687): \tTrain Loss  2.2250\n",
            "Epoch(10/10)\t Batch(1200/1687): \tTrain Loss  2.2239\n",
            "Epoch(10/10)\t Batch(1400/1687): \tTrain Loss  2.1926\n",
            "Epoch(10/10)\t Batch(1600/1687): \tTrain Loss  2.2088\n",
            "End of Epoch: 10 Train Accuracy: 0.2484 Val Loss: 2.2107 Val Accuracy: 0.2455\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▃▄▄▇▅▆█▆</td></tr><tr><td>Train Loss</td><td>█▆▆▇▅▄▄▄▁▂</td></tr><tr><td>Val Accuracy</td><td>▁▂▄▃▃▄▆▅▆█▆</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.24839</td></tr><tr><td>Train Loss</td><td>2.20896</td></tr><tr><td>Val Accuracy</td><td>0.2455</td></tr><tr><td>Val Loss</td><td>2.21067</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nag|loss_cross_entropy|lr=0.001|batch_32|act_tanh|hid_5|neurons_64|nrns_10|init_Random182</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/khcwg627' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/khcwg627</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_170131-khcwg627/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bqb1qk21 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: momentum\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_170301-bqb1qk21</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bqb1qk21' target=\"_blank\">toasty-sweep-24</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bqb1qk21' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bqb1qk21</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.0997 Val Loss: 0.1774 Val Accuracy: 0.1017\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/3375): \tTrain Loss  0.1990\n",
            "Epoch(1/10)\t Batch(200/3375): \tTrain Loss  0.1739\n",
            "Epoch(1/10)\t Batch(400/3375): \tTrain Loss  0.1709\n",
            "Epoch(1/10)\t Batch(600/3375): \tTrain Loss  0.1709\n",
            "Epoch(1/10)\t Batch(800/3375): \tTrain Loss  0.1832\n",
            "Epoch(1/10)\t Batch(1000/3375): \tTrain Loss  0.1606\n",
            "Epoch(1/10)\t Batch(1200/3375): \tTrain Loss  0.1706\n",
            "Epoch(1/10)\t Batch(1400/3375): \tTrain Loss  0.1725\n",
            "Epoch(1/10)\t Batch(1600/3375): \tTrain Loss  0.1474\n",
            "Epoch(1/10)\t Batch(1800/3375): \tTrain Loss  0.1622\n",
            "Epoch(1/10)\t Batch(2000/3375): \tTrain Loss  0.1519\n",
            "Epoch(1/10)\t Batch(2200/3375): \tTrain Loss  0.1454\n",
            "Epoch(1/10)\t Batch(2400/3375): \tTrain Loss  0.1487\n",
            "Epoch(1/10)\t Batch(2600/3375): \tTrain Loss  0.1307\n",
            "Epoch(1/10)\t Batch(2800/3375): \tTrain Loss  0.1556\n",
            "Epoch(1/10)\t Batch(3000/3375): \tTrain Loss  0.1440\n",
            "Epoch(1/10)\t Batch(3200/3375): \tTrain Loss  0.1390\n",
            "End of Epoch: 1 Train Accuracy: 0.1675 Val Loss: 0.1375 Val Accuracy: 0.1638\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/3375): \tTrain Loss  0.1392\n",
            "Epoch(2/10)\t Batch(200/3375): \tTrain Loss  0.1238\n",
            "Epoch(2/10)\t Batch(400/3375): \tTrain Loss  0.1117\n",
            "Epoch(2/10)\t Batch(600/3375): \tTrain Loss  0.1021\n",
            "Epoch(2/10)\t Batch(800/3375): \tTrain Loss  0.1352\n",
            "Epoch(2/10)\t Batch(1000/3375): \tTrain Loss  0.1473\n",
            "Epoch(2/10)\t Batch(1200/3375): \tTrain Loss  0.1217\n",
            "Epoch(2/10)\t Batch(1400/3375): \tTrain Loss  0.1491\n",
            "Epoch(2/10)\t Batch(1600/3375): \tTrain Loss  0.1295\n",
            "Epoch(2/10)\t Batch(1800/3375): \tTrain Loss  0.1221\n",
            "Epoch(2/10)\t Batch(2000/3375): \tTrain Loss  0.1493\n",
            "Epoch(2/10)\t Batch(2200/3375): \tTrain Loss  0.1132\n",
            "Epoch(2/10)\t Batch(2400/3375): \tTrain Loss  0.1138\n",
            "Epoch(2/10)\t Batch(2600/3375): \tTrain Loss  0.1255\n",
            "Epoch(2/10)\t Batch(2800/3375): \tTrain Loss  0.1156\n",
            "Epoch(2/10)\t Batch(3000/3375): \tTrain Loss  0.1199\n",
            "Epoch(2/10)\t Batch(3200/3375): \tTrain Loss  0.1111\n",
            "End of Epoch: 2 Train Accuracy: 0.2250 Val Loss: 0.1254 Val Accuracy: 0.2245\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/3375): \tTrain Loss  0.1493\n",
            "Epoch(3/10)\t Batch(200/3375): \tTrain Loss  0.1122\n",
            "Epoch(3/10)\t Batch(400/3375): \tTrain Loss  0.1544\n",
            "Epoch(3/10)\t Batch(600/3375): \tTrain Loss  0.1212\n",
            "Epoch(3/10)\t Batch(800/3375): \tTrain Loss  0.1180\n",
            "Epoch(3/10)\t Batch(1000/3375): \tTrain Loss  0.0795\n",
            "Epoch(3/10)\t Batch(1200/3375): \tTrain Loss  0.1051\n",
            "Epoch(3/10)\t Batch(1400/3375): \tTrain Loss  0.1364\n",
            "Epoch(3/10)\t Batch(1600/3375): \tTrain Loss  0.0976\n",
            "Epoch(3/10)\t Batch(1800/3375): \tTrain Loss  0.1213\n",
            "Epoch(3/10)\t Batch(2000/3375): \tTrain Loss  0.1049\n",
            "Epoch(3/10)\t Batch(2200/3375): \tTrain Loss  0.1000\n",
            "Epoch(3/10)\t Batch(2400/3375): \tTrain Loss  0.1411\n",
            "Epoch(3/10)\t Batch(2600/3375): \tTrain Loss  0.1011\n",
            "Epoch(3/10)\t Batch(2800/3375): \tTrain Loss  0.1412\n",
            "Epoch(3/10)\t Batch(3000/3375): \tTrain Loss  0.1144\n",
            "Epoch(3/10)\t Batch(3200/3375): \tTrain Loss  0.0846\n",
            "End of Epoch: 3 Train Accuracy: 0.2530 Val Loss: 0.1199 Val Accuracy: 0.2465\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/3375): \tTrain Loss  0.0990\n",
            "Epoch(4/10)\t Batch(200/3375): \tTrain Loss  0.1333\n",
            "Epoch(4/10)\t Batch(400/3375): \tTrain Loss  0.1254\n",
            "Epoch(4/10)\t Batch(600/3375): \tTrain Loss  0.1164\n",
            "Epoch(4/10)\t Batch(800/3375): \tTrain Loss  0.1127\n",
            "Epoch(4/10)\t Batch(1000/3375): \tTrain Loss  0.1162\n",
            "Epoch(4/10)\t Batch(1200/3375): \tTrain Loss  0.1174\n",
            "Epoch(4/10)\t Batch(1400/3375): \tTrain Loss  0.1009\n",
            "Epoch(4/10)\t Batch(1600/3375): \tTrain Loss  0.1341\n",
            "Epoch(4/10)\t Batch(1800/3375): \tTrain Loss  0.1298\n",
            "Epoch(4/10)\t Batch(2000/3375): \tTrain Loss  0.1023\n",
            "Epoch(4/10)\t Batch(2200/3375): \tTrain Loss  0.0849\n",
            "Epoch(4/10)\t Batch(2400/3375): \tTrain Loss  0.1399\n",
            "Epoch(4/10)\t Batch(2600/3375): \tTrain Loss  0.1439\n",
            "Epoch(4/10)\t Batch(2800/3375): \tTrain Loss  0.1003\n",
            "Epoch(4/10)\t Batch(3000/3375): \tTrain Loss  0.1320\n",
            "Epoch(4/10)\t Batch(3200/3375): \tTrain Loss  0.0808\n",
            "End of Epoch: 4 Train Accuracy: 0.2704 Val Loss: 0.1162 Val Accuracy: 0.2662\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/3375): \tTrain Loss  0.1155\n",
            "Epoch(5/10)\t Batch(200/3375): \tTrain Loss  0.1071\n",
            "Epoch(5/10)\t Batch(400/3375): \tTrain Loss  0.1206\n",
            "Epoch(5/10)\t Batch(600/3375): \tTrain Loss  0.1210\n",
            "Epoch(5/10)\t Batch(800/3375): \tTrain Loss  0.1321\n",
            "Epoch(5/10)\t Batch(1000/3375): \tTrain Loss  0.1360\n",
            "Epoch(5/10)\t Batch(1200/3375): \tTrain Loss  0.1158\n",
            "Epoch(5/10)\t Batch(1400/3375): \tTrain Loss  0.0927\n",
            "Epoch(5/10)\t Batch(1600/3375): \tTrain Loss  0.0740\n",
            "Epoch(5/10)\t Batch(1800/3375): \tTrain Loss  0.0857\n",
            "Epoch(5/10)\t Batch(2000/3375): \tTrain Loss  0.0920\n",
            "Epoch(5/10)\t Batch(2200/3375): \tTrain Loss  0.0995\n",
            "Epoch(5/10)\t Batch(2400/3375): \tTrain Loss  0.1246\n",
            "Epoch(5/10)\t Batch(2600/3375): \tTrain Loss  0.0802\n",
            "Epoch(5/10)\t Batch(2800/3375): \tTrain Loss  0.1193\n",
            "Epoch(5/10)\t Batch(3000/3375): \tTrain Loss  0.0938\n",
            "Epoch(5/10)\t Batch(3200/3375): \tTrain Loss  0.1454\n",
            "End of Epoch: 5 Train Accuracy: 0.2823 Val Loss: 0.1134 Val Accuracy: 0.2787\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/3375): \tTrain Loss  0.1377\n",
            "Epoch(6/10)\t Batch(200/3375): \tTrain Loss  0.1339\n",
            "Epoch(6/10)\t Batch(400/3375): \tTrain Loss  0.0899\n",
            "Epoch(6/10)\t Batch(600/3375): \tTrain Loss  0.1355\n",
            "Epoch(6/10)\t Batch(800/3375): \tTrain Loss  0.0745\n",
            "Epoch(6/10)\t Batch(1000/3375): \tTrain Loss  0.1147\n",
            "Epoch(6/10)\t Batch(1200/3375): \tTrain Loss  0.1016\n",
            "Epoch(6/10)\t Batch(1400/3375): \tTrain Loss  0.1176\n",
            "Epoch(6/10)\t Batch(1600/3375): \tTrain Loss  0.1098\n",
            "Epoch(6/10)\t Batch(1800/3375): \tTrain Loss  0.1315\n",
            "Epoch(6/10)\t Batch(2000/3375): \tTrain Loss  0.1099\n",
            "Epoch(6/10)\t Batch(2200/3375): \tTrain Loss  0.1234\n",
            "Epoch(6/10)\t Batch(2400/3375): \tTrain Loss  0.0623\n",
            "Epoch(6/10)\t Batch(2600/3375): \tTrain Loss  0.1455\n",
            "Epoch(6/10)\t Batch(2800/3375): \tTrain Loss  0.1142\n",
            "Epoch(6/10)\t Batch(3000/3375): \tTrain Loss  0.1102\n",
            "Epoch(6/10)\t Batch(3200/3375): \tTrain Loss  0.1010\n",
            "End of Epoch: 6 Train Accuracy: 0.2898 Val Loss: 0.1113 Val Accuracy: 0.2868\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/3375): \tTrain Loss  0.1100\n",
            "Epoch(7/10)\t Batch(200/3375): \tTrain Loss  0.1333\n",
            "Epoch(7/10)\t Batch(400/3375): \tTrain Loss  0.1093\n",
            "Epoch(7/10)\t Batch(600/3375): \tTrain Loss  0.1253\n",
            "Epoch(7/10)\t Batch(800/3375): \tTrain Loss  0.0840\n",
            "Epoch(7/10)\t Batch(1000/3375): \tTrain Loss  0.0795\n",
            "Epoch(7/10)\t Batch(1200/3375): \tTrain Loss  0.1242\n",
            "Epoch(7/10)\t Batch(1400/3375): \tTrain Loss  0.0872\n",
            "Epoch(7/10)\t Batch(1600/3375): \tTrain Loss  0.0852\n",
            "Epoch(7/10)\t Batch(1800/3375): \tTrain Loss  0.1116\n",
            "Epoch(7/10)\t Batch(2000/3375): \tTrain Loss  0.0972\n",
            "Epoch(7/10)\t Batch(2200/3375): \tTrain Loss  0.0863\n",
            "Epoch(7/10)\t Batch(2400/3375): \tTrain Loss  0.1128\n",
            "Epoch(7/10)\t Batch(2600/3375): \tTrain Loss  0.0945\n",
            "Epoch(7/10)\t Batch(2800/3375): \tTrain Loss  0.0805\n",
            "Epoch(7/10)\t Batch(3000/3375): \tTrain Loss  0.0886\n",
            "Epoch(7/10)\t Batch(3200/3375): \tTrain Loss  0.0959\n",
            "End of Epoch: 7 Train Accuracy: 0.2962 Val Loss: 0.1095 Val Accuracy: 0.2920\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/3375): \tTrain Loss  0.1228\n",
            "Epoch(8/10)\t Batch(200/3375): \tTrain Loss  0.0959\n",
            "Epoch(8/10)\t Batch(400/3375): \tTrain Loss  0.0686\n",
            "Epoch(8/10)\t Batch(600/3375): \tTrain Loss  0.1087\n",
            "Epoch(8/10)\t Batch(800/3375): \tTrain Loss  0.1310\n",
            "Epoch(8/10)\t Batch(1000/3375): \tTrain Loss  0.1591\n",
            "Epoch(8/10)\t Batch(1200/3375): \tTrain Loss  0.0992\n",
            "Epoch(8/10)\t Batch(1400/3375): \tTrain Loss  0.0821\n",
            "Epoch(8/10)\t Batch(1600/3375): \tTrain Loss  0.1031\n",
            "Epoch(8/10)\t Batch(1800/3375): \tTrain Loss  0.1291\n",
            "Epoch(8/10)\t Batch(2000/3375): \tTrain Loss  0.0914\n",
            "Epoch(8/10)\t Batch(2200/3375): \tTrain Loss  0.1001\n",
            "Epoch(8/10)\t Batch(2400/3375): \tTrain Loss  0.1390\n",
            "Epoch(8/10)\t Batch(2600/3375): \tTrain Loss  0.0934\n",
            "Epoch(8/10)\t Batch(2800/3375): \tTrain Loss  0.1207\n",
            "Epoch(8/10)\t Batch(3000/3375): \tTrain Loss  0.1131\n",
            "Epoch(8/10)\t Batch(3200/3375): \tTrain Loss  0.1059\n",
            "End of Epoch: 8 Train Accuracy: 0.2997 Val Loss: 0.1079 Val Accuracy: 0.2938\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/3375): \tTrain Loss  0.1189\n",
            "Epoch(9/10)\t Batch(200/3375): \tTrain Loss  0.1093\n",
            "Epoch(9/10)\t Batch(400/3375): \tTrain Loss  0.1069\n",
            "Epoch(9/10)\t Batch(600/3375): \tTrain Loss  0.1395\n",
            "Epoch(9/10)\t Batch(800/3375): \tTrain Loss  0.0856\n",
            "Epoch(9/10)\t Batch(1000/3375): \tTrain Loss  0.0915\n",
            "Epoch(9/10)\t Batch(1200/3375): \tTrain Loss  0.1020\n",
            "Epoch(9/10)\t Batch(1400/3375): \tTrain Loss  0.1250\n",
            "Epoch(9/10)\t Batch(1600/3375): \tTrain Loss  0.0950\n",
            "Epoch(9/10)\t Batch(1800/3375): \tTrain Loss  0.0773\n",
            "Epoch(9/10)\t Batch(2000/3375): \tTrain Loss  0.1188\n",
            "Epoch(9/10)\t Batch(2200/3375): \tTrain Loss  0.1018\n",
            "Epoch(9/10)\t Batch(2400/3375): \tTrain Loss  0.1061\n",
            "Epoch(9/10)\t Batch(2600/3375): \tTrain Loss  0.1020\n",
            "Epoch(9/10)\t Batch(2800/3375): \tTrain Loss  0.0878\n",
            "Epoch(9/10)\t Batch(3000/3375): \tTrain Loss  0.0865\n",
            "Epoch(9/10)\t Batch(3200/3375): \tTrain Loss  0.1086\n",
            "End of Epoch: 9 Train Accuracy: 0.3040 Val Loss: 0.1065 Val Accuracy: 0.2982\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/3375): \tTrain Loss  0.1210\n",
            "Epoch(10/10)\t Batch(200/3375): \tTrain Loss  0.0920\n",
            "Epoch(10/10)\t Batch(400/3375): \tTrain Loss  0.1107\n",
            "Epoch(10/10)\t Batch(600/3375): \tTrain Loss  0.0963\n",
            "Epoch(10/10)\t Batch(800/3375): \tTrain Loss  0.1281\n",
            "Epoch(10/10)\t Batch(1000/3375): \tTrain Loss  0.1219\n",
            "Epoch(10/10)\t Batch(1200/3375): \tTrain Loss  0.1113\n",
            "Epoch(10/10)\t Batch(1400/3375): \tTrain Loss  0.1183\n",
            "Epoch(10/10)\t Batch(1600/3375): \tTrain Loss  0.0902\n",
            "Epoch(10/10)\t Batch(1800/3375): \tTrain Loss  0.1090\n",
            "Epoch(10/10)\t Batch(2000/3375): \tTrain Loss  0.1243\n",
            "Epoch(10/10)\t Batch(2200/3375): \tTrain Loss  0.1054\n",
            "Epoch(10/10)\t Batch(2400/3375): \tTrain Loss  0.1172\n",
            "Epoch(10/10)\t Batch(2600/3375): \tTrain Loss  0.1096\n",
            "Epoch(10/10)\t Batch(2800/3375): \tTrain Loss  0.1029\n",
            "Epoch(10/10)\t Batch(3000/3375): \tTrain Loss  0.0925\n",
            "Epoch(10/10)\t Batch(3200/3375): \tTrain Loss  0.0926\n",
            "End of Epoch: 10 Train Accuracy: 0.3068 Val Loss: 0.1052 Val Accuracy: 0.2997\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▃▅▆▇▇▇████</td></tr><tr><td>Train Loss</td><td>█▄▄▄▇▆▃▁▃▃</td></tr><tr><td>Val Accuracy</td><td>▁▃▅▆▇▇█████</td></tr><tr><td>Val Loss</td><td>█▄▃▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.30681</td></tr><tr><td>Train Loss</td><td>0.10039</td></tr><tr><td>Val Accuracy</td><td>0.29967</td></tr><tr><td>Val Loss</td><td>0.10523</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_momentum|loss_mse|lr=0.0001|batch_16|act_sigmoid|hid_3|neurons_128|nrns_10|init_Random792</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bqb1qk21' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bqb1qk21</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_170301-bqb1qk21/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cb0bp92p with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_170537-cb0bp92p</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/cb0bp92p' target=\"_blank\">visionary-sweep-25</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/cb0bp92p' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/cb0bp92p</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.0998 Val Loss: 0.0914 Val Accuracy: 0.1022\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  0.0930\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  0.0900\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  0.0900\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  0.0898\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  0.0900\n",
            "End of Epoch: 1 Train Accuracy: 0.1002 Val Loss: 0.0900 Val Accuracy: 0.0983\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  0.0900\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  0.0900\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  0.0901\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  0.0899\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  0.0901\n",
            "End of Epoch: 2 Train Accuracy: 0.0991 Val Loss: 0.0900 Val Accuracy: 0.1080\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  0.0902\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  0.0900\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  0.0901\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  0.0900\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  0.0864\n",
            "End of Epoch: 3 Train Accuracy: 0.1998 Val Loss: 0.0853 Val Accuracy: 0.2045\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  0.0854\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  0.0812\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  0.0752\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  0.0768\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  0.0746\n",
            "End of Epoch: 4 Train Accuracy: 0.3554 Val Loss: 0.0716 Val Accuracy: 0.3518\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  0.0720\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  0.0715\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  0.0614\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  0.0631\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  0.0559\n",
            "End of Epoch: 5 Train Accuracy: 0.5333 Val Loss: 0.0584 Val Accuracy: 0.5318\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  0.0563\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  0.0540\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  0.0515\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  0.0484\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  0.0528\n",
            "End of Epoch: 6 Train Accuracy: 0.6534 Val Loss: 0.0485 Val Accuracy: 0.6490\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  0.0551\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  0.0473\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  0.0383\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  0.0535\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  0.0456\n",
            "End of Epoch: 7 Train Accuracy: 0.6961 Val Loss: 0.0425 Val Accuracy: 0.6922\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  0.0489\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  0.0428\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  0.0458\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  0.0351\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  0.0363\n",
            "End of Epoch: 8 Train Accuracy: 0.7215 Val Loss: 0.0387 Val Accuracy: 0.7185\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  0.0368\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  0.0439\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  0.0327\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  0.0370\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  0.0316\n",
            "End of Epoch: 9 Train Accuracy: 0.7274 Val Loss: 0.0359 Val Accuracy: 0.7217\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  0.0359\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  0.0256\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  0.0361\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  0.0299\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  0.0340\n",
            "End of Epoch: 10 Train Accuracy: 0.7734 Val Loss: 0.0338 Val Accuracy: 0.7692\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▁▁▂▄▆▇▇▇██</td></tr><tr><td>Train Loss</td><td>███▇▄▃▄▃▂▁</td></tr><tr><td>Val Accuracy</td><td>▁▁▁▂▄▆▇▇▇██</td></tr><tr><td>Val Loss</td><td>███▇▆▄▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.77344</td></tr><tr><td>Train Loss</td><td>0.02337</td></tr><tr><td>Val Accuracy</td><td>0.76917</td></tr><tr><td>Val Loss</td><td>0.03385</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nadam|loss_mse|lr=0.0001|batch_64|act_sigmoid|hid_5|neurons_128|nrns_10|init_Xavier98</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/cb0bp92p' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/cb0bp92p</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_170537-cb0bp92p/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2dlu3mr5 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_170808-2dlu3mr5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/2dlu3mr5' target=\"_blank\">soft-sweep-26</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/2dlu3mr5' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/2dlu3mr5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.0996 Val Loss: 2.3939 Val Accuracy: 0.1037\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/3375): \tTrain Loss  2.4540\n",
            "Epoch(1/10)\t Batch(200/3375): \tTrain Loss  2.2222\n",
            "Epoch(1/10)\t Batch(400/3375): \tTrain Loss  2.3027\n",
            "Epoch(1/10)\t Batch(600/3375): \tTrain Loss  2.3376\n",
            "Epoch(1/10)\t Batch(800/3375): \tTrain Loss  2.3197\n",
            "Epoch(1/10)\t Batch(1000/3375): \tTrain Loss  2.2898\n",
            "Epoch(1/10)\t Batch(1200/3375): \tTrain Loss  2.2906\n",
            "Epoch(1/10)\t Batch(1400/3375): \tTrain Loss  2.3252\n",
            "Epoch(1/10)\t Batch(1600/3375): \tTrain Loss  2.3148\n",
            "Epoch(1/10)\t Batch(1800/3375): \tTrain Loss  2.3044\n",
            "Epoch(1/10)\t Batch(2000/3375): \tTrain Loss  2.2988\n",
            "Epoch(1/10)\t Batch(2200/3375): \tTrain Loss  2.3003\n",
            "Epoch(1/10)\t Batch(2400/3375): \tTrain Loss  2.2998\n",
            "Epoch(1/10)\t Batch(2600/3375): \tTrain Loss  2.3018\n",
            "Epoch(1/10)\t Batch(2800/3375): \tTrain Loss  2.2999\n",
            "Epoch(1/10)\t Batch(3000/3375): \tTrain Loss  2.3017\n",
            "Epoch(1/10)\t Batch(3200/3375): \tTrain Loss  2.3015\n",
            "End of Epoch: 1 Train Accuracy: 0.1001 Val Loss: 2.3016 Val Accuracy: 0.1013\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/3375): \tTrain Loss  2.2999\n",
            "Epoch(2/10)\t Batch(200/3375): \tTrain Loss  2.3010\n",
            "Epoch(2/10)\t Batch(400/3375): \tTrain Loss  2.3003\n",
            "Epoch(2/10)\t Batch(600/3375): \tTrain Loss  2.3008\n",
            "Epoch(2/10)\t Batch(800/3375): \tTrain Loss  2.3018\n",
            "Epoch(2/10)\t Batch(1000/3375): \tTrain Loss  2.3018\n",
            "Epoch(2/10)\t Batch(1200/3375): \tTrain Loss  2.2995\n",
            "Epoch(2/10)\t Batch(1400/3375): \tTrain Loss  2.2993\n",
            "Epoch(2/10)\t Batch(1600/3375): \tTrain Loss  2.3009\n",
            "Epoch(2/10)\t Batch(1800/3375): \tTrain Loss  2.2997\n",
            "Epoch(2/10)\t Batch(2000/3375): \tTrain Loss  2.2998\n",
            "Epoch(2/10)\t Batch(2200/3375): \tTrain Loss  2.2990\n",
            "Epoch(2/10)\t Batch(2400/3375): \tTrain Loss  2.3007\n",
            "Epoch(2/10)\t Batch(2600/3375): \tTrain Loss  2.2989\n",
            "Epoch(2/10)\t Batch(2800/3375): \tTrain Loss  2.2977\n",
            "Epoch(2/10)\t Batch(3000/3375): \tTrain Loss  2.2964\n",
            "Epoch(2/10)\t Batch(3200/3375): \tTrain Loss  2.2993\n",
            "End of Epoch: 2 Train Accuracy: 0.2750 Val Loss: 2.2972 Val Accuracy: 0.2758\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/3375): \tTrain Loss  2.2982\n",
            "Epoch(3/10)\t Batch(200/3375): \tTrain Loss  2.2963\n",
            "Epoch(3/10)\t Batch(400/3375): \tTrain Loss  2.2968\n",
            "Epoch(3/10)\t Batch(600/3375): \tTrain Loss  2.2951\n",
            "Epoch(3/10)\t Batch(800/3375): \tTrain Loss  2.2943\n",
            "Epoch(3/10)\t Batch(1000/3375): \tTrain Loss  2.2958\n",
            "Epoch(3/10)\t Batch(1200/3375): \tTrain Loss  2.2962\n",
            "Epoch(3/10)\t Batch(1400/3375): \tTrain Loss  2.2934\n",
            "Epoch(3/10)\t Batch(1600/3375): \tTrain Loss  2.2901\n",
            "Epoch(3/10)\t Batch(1800/3375): \tTrain Loss  2.2913\n",
            "Epoch(3/10)\t Batch(2000/3375): \tTrain Loss  2.2901\n",
            "Epoch(3/10)\t Batch(2200/3375): \tTrain Loss  2.2900\n",
            "Epoch(3/10)\t Batch(2400/3375): \tTrain Loss  2.2903\n",
            "Epoch(3/10)\t Batch(2600/3375): \tTrain Loss  2.2840\n",
            "Epoch(3/10)\t Batch(2800/3375): \tTrain Loss  2.2866\n",
            "Epoch(3/10)\t Batch(3000/3375): \tTrain Loss  2.2892\n",
            "Epoch(3/10)\t Batch(3200/3375): \tTrain Loss  2.2799\n",
            "End of Epoch: 3 Train Accuracy: 0.2018 Val Loss: 2.2804 Val Accuracy: 0.1997\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/3375): \tTrain Loss  2.2808\n",
            "Epoch(4/10)\t Batch(200/3375): \tTrain Loss  2.2776\n",
            "Epoch(4/10)\t Batch(400/3375): \tTrain Loss  2.2813\n",
            "Epoch(4/10)\t Batch(600/3375): \tTrain Loss  2.2715\n",
            "Epoch(4/10)\t Batch(800/3375): \tTrain Loss  2.2740\n",
            "Epoch(4/10)\t Batch(1000/3375): \tTrain Loss  2.2759\n",
            "Epoch(4/10)\t Batch(1200/3375): \tTrain Loss  2.2677\n",
            "Epoch(4/10)\t Batch(1400/3375): \tTrain Loss  2.2664\n",
            "Epoch(4/10)\t Batch(1600/3375): \tTrain Loss  2.2570\n",
            "Epoch(4/10)\t Batch(1800/3375): \tTrain Loss  2.2690\n",
            "Epoch(4/10)\t Batch(2000/3375): \tTrain Loss  2.2537\n",
            "Epoch(4/10)\t Batch(2200/3375): \tTrain Loss  2.2652\n",
            "Epoch(4/10)\t Batch(2400/3375): \tTrain Loss  2.2529\n",
            "Epoch(4/10)\t Batch(2600/3375): \tTrain Loss  2.2424\n",
            "Epoch(4/10)\t Batch(2800/3375): \tTrain Loss  2.2453\n",
            "Epoch(4/10)\t Batch(3000/3375): \tTrain Loss  2.2465\n",
            "Epoch(4/10)\t Batch(3200/3375): \tTrain Loss  2.2384\n",
            "End of Epoch: 4 Train Accuracy: 0.1946 Val Loss: 2.2437 Val Accuracy: 0.1942\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/3375): \tTrain Loss  2.2391\n",
            "Epoch(5/10)\t Batch(200/3375): \tTrain Loss  2.2330\n",
            "Epoch(5/10)\t Batch(400/3375): \tTrain Loss  2.2376\n",
            "Epoch(5/10)\t Batch(600/3375): \tTrain Loss  2.2376\n",
            "Epoch(5/10)\t Batch(800/3375): \tTrain Loss  2.2699\n",
            "Epoch(5/10)\t Batch(1000/3375): \tTrain Loss  2.2377\n",
            "Epoch(5/10)\t Batch(1200/3375): \tTrain Loss  2.2158\n",
            "Epoch(5/10)\t Batch(1400/3375): \tTrain Loss  2.2322\n",
            "Epoch(5/10)\t Batch(1600/3375): \tTrain Loss  2.2246\n",
            "Epoch(5/10)\t Batch(1800/3375): \tTrain Loss  2.2350\n",
            "Epoch(5/10)\t Batch(2000/3375): \tTrain Loss  2.2180\n",
            "Epoch(5/10)\t Batch(2200/3375): \tTrain Loss  2.2238\n",
            "Epoch(5/10)\t Batch(2400/3375): \tTrain Loss  2.2362\n",
            "Epoch(5/10)\t Batch(2600/3375): \tTrain Loss  2.2217\n",
            "Epoch(5/10)\t Batch(2800/3375): \tTrain Loss  2.2038\n",
            "Epoch(5/10)\t Batch(3000/3375): \tTrain Loss  2.2317\n",
            "Epoch(5/10)\t Batch(3200/3375): \tTrain Loss  2.2305\n",
            "End of Epoch: 5 Train Accuracy: 0.1999 Val Loss: 2.2180 Val Accuracy: 0.1968\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/3375): \tTrain Loss  2.2185\n",
            "Epoch(6/10)\t Batch(200/3375): \tTrain Loss  2.2078\n",
            "Epoch(6/10)\t Batch(400/3375): \tTrain Loss  2.2021\n",
            "Epoch(6/10)\t Batch(600/3375): \tTrain Loss  2.2098\n",
            "Epoch(6/10)\t Batch(800/3375): \tTrain Loss  2.2374\n",
            "Epoch(6/10)\t Batch(1000/3375): \tTrain Loss  2.2014\n",
            "Epoch(6/10)\t Batch(1200/3375): \tTrain Loss  2.2119\n",
            "Epoch(6/10)\t Batch(1400/3375): \tTrain Loss  2.2023\n",
            "Epoch(6/10)\t Batch(1600/3375): \tTrain Loss  2.2101\n",
            "Epoch(6/10)\t Batch(1800/3375): \tTrain Loss  2.2236\n",
            "Epoch(6/10)\t Batch(2000/3375): \tTrain Loss  2.2369\n",
            "Epoch(6/10)\t Batch(2200/3375): \tTrain Loss  2.2154\n",
            "Epoch(6/10)\t Batch(2400/3375): \tTrain Loss  2.2362\n",
            "Epoch(6/10)\t Batch(2600/3375): \tTrain Loss  2.2135\n",
            "Epoch(6/10)\t Batch(2800/3375): \tTrain Loss  2.2043\n",
            "Epoch(6/10)\t Batch(3000/3375): \tTrain Loss  2.2251\n",
            "Epoch(6/10)\t Batch(3200/3375): \tTrain Loss  2.2101\n",
            "End of Epoch: 6 Train Accuracy: 0.2001 Val Loss: 2.2125 Val Accuracy: 0.1970\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/3375): \tTrain Loss  2.2005\n",
            "Epoch(7/10)\t Batch(200/3375): \tTrain Loss  2.2114\n",
            "Epoch(7/10)\t Batch(400/3375): \tTrain Loss  2.2185\n",
            "Epoch(7/10)\t Batch(600/3375): \tTrain Loss  2.2209\n",
            "Epoch(7/10)\t Batch(800/3375): \tTrain Loss  2.2113\n",
            "Epoch(7/10)\t Batch(1000/3375): \tTrain Loss  2.2102\n",
            "Epoch(7/10)\t Batch(1200/3375): \tTrain Loss  2.2150\n",
            "Epoch(7/10)\t Batch(1400/3375): \tTrain Loss  2.2016\n",
            "Epoch(7/10)\t Batch(1600/3375): \tTrain Loss  2.2225\n",
            "Epoch(7/10)\t Batch(1800/3375): \tTrain Loss  2.2230\n",
            "Epoch(7/10)\t Batch(2000/3375): \tTrain Loss  2.2036\n",
            "Epoch(7/10)\t Batch(2200/3375): \tTrain Loss  2.2232\n",
            "Epoch(7/10)\t Batch(2400/3375): \tTrain Loss  2.2064\n",
            "Epoch(7/10)\t Batch(2600/3375): \tTrain Loss  2.1893\n",
            "Epoch(7/10)\t Batch(2800/3375): \tTrain Loss  2.2029\n",
            "Epoch(7/10)\t Batch(3000/3375): \tTrain Loss  2.2230\n",
            "Epoch(7/10)\t Batch(3200/3375): \tTrain Loss  2.2214\n",
            "End of Epoch: 7 Train Accuracy: 0.2016 Val Loss: 2.2112 Val Accuracy: 0.1990\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/3375): \tTrain Loss  2.1898\n",
            "Epoch(8/10)\t Batch(200/3375): \tTrain Loss  2.1940\n",
            "Epoch(8/10)\t Batch(400/3375): \tTrain Loss  2.1976\n",
            "Epoch(8/10)\t Batch(600/3375): \tTrain Loss  2.2098\n",
            "Epoch(8/10)\t Batch(800/3375): \tTrain Loss  2.2183\n",
            "Epoch(8/10)\t Batch(1000/3375): \tTrain Loss  2.2028\n",
            "Epoch(8/10)\t Batch(1200/3375): \tTrain Loss  2.2116\n",
            "Epoch(8/10)\t Batch(1400/3375): \tTrain Loss  2.2160\n",
            "Epoch(8/10)\t Batch(1600/3375): \tTrain Loss  2.2070\n",
            "Epoch(8/10)\t Batch(1800/3375): \tTrain Loss  2.2084\n",
            "Epoch(8/10)\t Batch(2000/3375): \tTrain Loss  2.2189\n",
            "Epoch(8/10)\t Batch(2200/3375): \tTrain Loss  2.2046\n",
            "Epoch(8/10)\t Batch(2400/3375): \tTrain Loss  2.2032\n",
            "Epoch(8/10)\t Batch(2600/3375): \tTrain Loss  2.2207\n",
            "Epoch(8/10)\t Batch(2800/3375): \tTrain Loss  2.2157\n",
            "Epoch(8/10)\t Batch(3000/3375): \tTrain Loss  2.2201\n",
            "Epoch(8/10)\t Batch(3200/3375): \tTrain Loss  2.2149\n",
            "End of Epoch: 8 Train Accuracy: 0.2164 Val Loss: 2.2103 Val Accuracy: 0.2082\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/3375): \tTrain Loss  2.2133\n",
            "Epoch(9/10)\t Batch(200/3375): \tTrain Loss  2.2102\n",
            "Epoch(9/10)\t Batch(400/3375): \tTrain Loss  2.2071\n",
            "Epoch(9/10)\t Batch(600/3375): \tTrain Loss  2.2205\n",
            "Epoch(9/10)\t Batch(800/3375): \tTrain Loss  2.2033\n",
            "Epoch(9/10)\t Batch(1000/3375): \tTrain Loss  2.2251\n",
            "Epoch(9/10)\t Batch(1200/3375): \tTrain Loss  2.2241\n",
            "Epoch(9/10)\t Batch(1400/3375): \tTrain Loss  2.2044\n",
            "Epoch(9/10)\t Batch(1600/3375): \tTrain Loss  2.2126\n",
            "Epoch(9/10)\t Batch(1800/3375): \tTrain Loss  2.2121\n",
            "Epoch(9/10)\t Batch(2000/3375): \tTrain Loss  2.2016\n",
            "Epoch(9/10)\t Batch(2200/3375): \tTrain Loss  2.2072\n",
            "Epoch(9/10)\t Batch(2400/3375): \tTrain Loss  2.1954\n",
            "Epoch(9/10)\t Batch(2600/3375): \tTrain Loss  2.2092\n",
            "Epoch(9/10)\t Batch(2800/3375): \tTrain Loss  2.2174\n",
            "Epoch(9/10)\t Batch(3000/3375): \tTrain Loss  2.1988\n",
            "Epoch(9/10)\t Batch(3200/3375): \tTrain Loss  2.2063\n",
            "End of Epoch: 9 Train Accuracy: 0.2746 Val Loss: 2.2083 Val Accuracy: 0.2718\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/3375): \tTrain Loss  2.2068\n",
            "Epoch(10/10)\t Batch(200/3375): \tTrain Loss  2.2082\n",
            "Epoch(10/10)\t Batch(400/3375): \tTrain Loss  2.1970\n",
            "Epoch(10/10)\t Batch(600/3375): \tTrain Loss  2.1968\n",
            "Epoch(10/10)\t Batch(800/3375): \tTrain Loss  2.1995\n",
            "Epoch(10/10)\t Batch(1000/3375): \tTrain Loss  2.2061\n",
            "Epoch(10/10)\t Batch(1200/3375): \tTrain Loss  2.2019\n",
            "Epoch(10/10)\t Batch(1400/3375): \tTrain Loss  2.2056\n",
            "Epoch(10/10)\t Batch(1600/3375): \tTrain Loss  2.2167\n",
            "Epoch(10/10)\t Batch(1800/3375): \tTrain Loss  2.2166\n",
            "Epoch(10/10)\t Batch(2000/3375): \tTrain Loss  2.2114\n",
            "Epoch(10/10)\t Batch(2200/3375): \tTrain Loss  2.1917\n",
            "Epoch(10/10)\t Batch(2400/3375): \tTrain Loss  2.2262\n",
            "Epoch(10/10)\t Batch(2600/3375): \tTrain Loss  2.2087\n",
            "Epoch(10/10)\t Batch(2800/3375): \tTrain Loss  2.2057\n",
            "Epoch(10/10)\t Batch(3000/3375): \tTrain Loss  2.1920\n",
            "Epoch(10/10)\t Batch(3200/3375): \tTrain Loss  2.1979\n",
            "End of Epoch: 10 Train Accuracy: 0.2901 Val Loss: 2.2030 Val Accuracy: 0.2852\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▁▇▅▄▅▅▅▅▇█</td></tr><tr><td>Train Loss</td><td>██▇▄▂▃▃▁▂▂</td></tr><tr><td>Val Accuracy</td><td>▁▁█▅▅▅▅▅▅▇█</td></tr><tr><td>Val Loss</td><td>█▅▄▄▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.29015</td></tr><tr><td>Train Loss</td><td>2.19545</td></tr><tr><td>Val Accuracy</td><td>0.28517</td></tr><tr><td>Val Loss</td><td>2.20298</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_adam|loss_cross_entropy|lr=0.0001|batch_16|act_sigmoid|hid_5|neurons_32|nrns_10|init_Xavier214</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/2dlu3mr5' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/2dlu3mr5</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_170808-2dlu3mr5/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3e3sms79 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nag\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_170944-3e3sms79</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/3e3sms79' target=\"_blank\">visionary-sweep-27</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/3e3sms79' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/3e3sms79</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0936 Val Loss: 0.1809 Val Accuracy: 0.0953\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  0.1812\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  0.1844\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  0.1781\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  0.1812\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  0.1812\n",
            "End of Epoch: 1 Train Accuracy: 0.0986 Val Loss: 0.1803 Val Accuracy: 0.0983\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  0.1781\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  0.1844\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  0.1750\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  0.1812\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  0.1781\n",
            "End of Epoch: 2 Train Accuracy: 0.1000 Val Loss: 0.1801 Val Accuracy: 0.0997\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  0.1687\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  0.1781\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  0.1812\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  0.1906\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  0.1844\n",
            "End of Epoch: 3 Train Accuracy: 0.1000 Val Loss: 0.1801 Val Accuracy: 0.0997\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  0.1937\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  0.1750\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  0.1906\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  0.1750\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  0.1687\n",
            "End of Epoch: 4 Train Accuracy: 0.1000 Val Loss: 0.1801 Val Accuracy: 0.0997\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  0.1875\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  0.1844\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  0.1781\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  0.1875\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  0.1906\n",
            "End of Epoch: 5 Train Accuracy: 0.1000 Val Loss: 0.1800 Val Accuracy: 0.0997\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  0.1750\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  0.0970\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  0.0733\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  0.0612\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  0.0665\n",
            "End of Epoch: 6 Train Accuracy: 0.5749 Val Loss: 0.0631 Val Accuracy: 0.5825\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  0.0611\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  0.0616\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  0.0609\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  0.0603\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  0.0624\n",
            "End of Epoch: 7 Train Accuracy: 0.5646 Val Loss: 0.0596 Val Accuracy: 0.5672\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  0.0691\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  0.0645\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  0.0644\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  0.0593\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  0.0539\n",
            "End of Epoch: 8 Train Accuracy: 0.5684 Val Loss: 0.0584 Val Accuracy: 0.5682\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  0.0551\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  0.0644\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  0.0606\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  0.0601\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  0.0562\n",
            "End of Epoch: 9 Train Accuracy: 0.5647 Val Loss: 0.0576 Val Accuracy: 0.5658\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  0.0596\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  0.0565\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  0.0578\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  0.0563\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  0.0563\n",
            "End of Epoch: 10 Train Accuracy: 0.5951 Val Loss: 0.0568 Val Accuracy: 0.5895\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▁▁▁▁▁█████</td></tr><tr><td>Train Loss</td><td>▆██▇█▁▁▁▁▁</td></tr><tr><td>Val Accuracy</td><td>▁▁▁▁▁▁█████</td></tr><tr><td>Val Loss</td><td>██████▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.59515</td></tr><tr><td>Train Loss</td><td>0.05676</td></tr><tr><td>Val Accuracy</td><td>0.5895</td></tr><tr><td>Val Loss</td><td>0.05681</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nag|loss_mse|lr=0.001|batch_64|act_relu|hid_5|neurons_128|nrns_10|init_Random650</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/3e3sms79' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/3e3sms79</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_170944-3e3sms79/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gw1v4tgo with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: momentum\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_171134-gw1v4tgo</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/gw1v4tgo' target=\"_blank\">classic-sweep-28</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/gw1v4tgo' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/gw1v4tgo</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.0994 Val Loss: 12.1198 Val Accuracy: 0.1038\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/3375): \tTrain Loss  13.9426\n",
            "Epoch(1/10)\t Batch(200/3375): \tTrain Loss  2.3025\n",
            "Epoch(1/10)\t Batch(400/3375): \tTrain Loss  2.2394\n",
            "Epoch(1/10)\t Batch(600/3375): \tTrain Loss  2.2481\n",
            "Epoch(1/10)\t Batch(800/3375): \tTrain Loss  2.1864\n",
            "Epoch(1/10)\t Batch(1000/3375): \tTrain Loss  2.2006\n",
            "Epoch(1/10)\t Batch(1200/3375): \tTrain Loss  2.2017\n",
            "Epoch(1/10)\t Batch(1400/3375): \tTrain Loss  2.1876\n",
            "Epoch(1/10)\t Batch(1600/3375): \tTrain Loss  2.2057\n",
            "Epoch(1/10)\t Batch(1800/3375): \tTrain Loss  2.2420\n",
            "Epoch(1/10)\t Batch(2000/3375): \tTrain Loss  2.1769\n",
            "Epoch(1/10)\t Batch(2200/3375): \tTrain Loss  2.1921\n",
            "Epoch(1/10)\t Batch(2400/3375): \tTrain Loss  2.2315\n",
            "Epoch(1/10)\t Batch(2600/3375): \tTrain Loss  2.2204\n",
            "Epoch(1/10)\t Batch(2800/3375): \tTrain Loss  2.2228\n",
            "Epoch(1/10)\t Batch(3000/3375): \tTrain Loss  2.2507\n",
            "Epoch(1/10)\t Batch(3200/3375): \tTrain Loss  2.2487\n",
            "End of Epoch: 1 Train Accuracy: 0.1900 Val Loss: 2.2743 Val Accuracy: 0.1905\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/3375): \tTrain Loss  2.2679\n",
            "Epoch(2/10)\t Batch(200/3375): \tTrain Loss  2.2701\n",
            "Epoch(2/10)\t Batch(400/3375): \tTrain Loss  2.2888\n",
            "Epoch(2/10)\t Batch(600/3375): \tTrain Loss  2.2865\n",
            "Epoch(2/10)\t Batch(800/3375): \tTrain Loss  2.2973\n",
            "Epoch(2/10)\t Batch(1000/3375): \tTrain Loss  2.3076\n",
            "Epoch(2/10)\t Batch(1200/3375): \tTrain Loss  2.2853\n",
            "Epoch(2/10)\t Batch(1400/3375): \tTrain Loss  2.2878\n",
            "Epoch(2/10)\t Batch(1600/3375): \tTrain Loss  2.3026\n",
            "Epoch(2/10)\t Batch(1800/3375): \tTrain Loss  2.2965\n",
            "Epoch(2/10)\t Batch(2000/3375): \tTrain Loss  2.3034\n",
            "Epoch(2/10)\t Batch(2200/3375): \tTrain Loss  2.3084\n",
            "Epoch(2/10)\t Batch(2400/3375): \tTrain Loss  2.2970\n",
            "Epoch(2/10)\t Batch(2600/3375): \tTrain Loss  2.3167\n",
            "Epoch(2/10)\t Batch(2800/3375): \tTrain Loss  2.3113\n",
            "Epoch(2/10)\t Batch(3000/3375): \tTrain Loss  2.2993\n",
            "Epoch(2/10)\t Batch(3200/3375): \tTrain Loss  2.3024\n",
            "End of Epoch: 2 Train Accuracy: 0.0998 Val Loss: 2.3028 Val Accuracy: 0.1020\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/3375): \tTrain Loss  2.2925\n",
            "Epoch(3/10)\t Batch(200/3375): \tTrain Loss  2.2990\n",
            "Epoch(3/10)\t Batch(400/3375): \tTrain Loss  2.3135\n",
            "Epoch(3/10)\t Batch(600/3375): \tTrain Loss  2.3051\n",
            "Epoch(3/10)\t Batch(800/3375): \tTrain Loss  2.3005\n",
            "Epoch(3/10)\t Batch(1000/3375): \tTrain Loss  2.3051\n",
            "Epoch(3/10)\t Batch(1200/3375): \tTrain Loss  2.2840\n",
            "Epoch(3/10)\t Batch(1400/3375): \tTrain Loss  2.3201\n",
            "Epoch(3/10)\t Batch(1600/3375): \tTrain Loss  2.3189\n",
            "Epoch(3/10)\t Batch(1800/3375): \tTrain Loss  2.3010\n",
            "Epoch(3/10)\t Batch(2000/3375): \tTrain Loss  2.2971\n",
            "Epoch(3/10)\t Batch(2200/3375): \tTrain Loss  2.2973\n",
            "Epoch(3/10)\t Batch(2400/3375): \tTrain Loss  2.3033\n",
            "Epoch(3/10)\t Batch(2600/3375): \tTrain Loss  2.3030\n",
            "Epoch(3/10)\t Batch(2800/3375): \tTrain Loss  2.3059\n",
            "Epoch(3/10)\t Batch(3000/3375): \tTrain Loss  2.2927\n",
            "Epoch(3/10)\t Batch(3200/3375): \tTrain Loss  2.3022\n",
            "End of Epoch: 3 Train Accuracy: 0.1000 Val Loss: 2.3036 Val Accuracy: 0.1003\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/3375): \tTrain Loss  2.2951\n",
            "Epoch(4/10)\t Batch(200/3375): \tTrain Loss  2.3155\n",
            "Epoch(4/10)\t Batch(400/3375): \tTrain Loss  2.3037\n",
            "Epoch(4/10)\t Batch(600/3375): \tTrain Loss  2.3002\n",
            "Epoch(4/10)\t Batch(800/3375): \tTrain Loss  2.3031\n",
            "Epoch(4/10)\t Batch(1000/3375): \tTrain Loss  2.3005\n",
            "Epoch(4/10)\t Batch(1200/3375): \tTrain Loss  2.2892\n",
            "Epoch(4/10)\t Batch(1400/3375): \tTrain Loss  2.2916\n",
            "Epoch(4/10)\t Batch(1600/3375): \tTrain Loss  2.2983\n",
            "Epoch(4/10)\t Batch(1800/3375): \tTrain Loss  2.3102\n",
            "Epoch(4/10)\t Batch(2000/3375): \tTrain Loss  2.2941\n",
            "Epoch(4/10)\t Batch(2200/3375): \tTrain Loss  2.3157\n",
            "Epoch(4/10)\t Batch(2400/3375): \tTrain Loss  2.2991\n",
            "Epoch(4/10)\t Batch(2600/3375): \tTrain Loss  2.3091\n",
            "Epoch(4/10)\t Batch(2800/3375): \tTrain Loss  2.3114\n",
            "Epoch(4/10)\t Batch(3000/3375): \tTrain Loss  2.3146\n",
            "Epoch(4/10)\t Batch(3200/3375): \tTrain Loss  2.2931\n",
            "End of Epoch: 4 Train Accuracy: 0.1000 Val Loss: 2.3030 Val Accuracy: 0.1002\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/3375): \tTrain Loss  2.3137\n",
            "Epoch(5/10)\t Batch(200/3375): \tTrain Loss  2.3094\n",
            "Epoch(5/10)\t Batch(400/3375): \tTrain Loss  2.2925\n",
            "Epoch(5/10)\t Batch(600/3375): \tTrain Loss  2.3087\n",
            "Epoch(5/10)\t Batch(800/3375): \tTrain Loss  2.3038\n",
            "Epoch(5/10)\t Batch(1000/3375): \tTrain Loss  2.3125\n",
            "Epoch(5/10)\t Batch(1200/3375): \tTrain Loss  2.3151\n",
            "Epoch(5/10)\t Batch(1400/3375): \tTrain Loss  2.2999\n",
            "Epoch(5/10)\t Batch(1600/3375): \tTrain Loss  2.2998\n",
            "Epoch(5/10)\t Batch(1800/3375): \tTrain Loss  2.3055\n",
            "Epoch(5/10)\t Batch(2000/3375): \tTrain Loss  2.2972\n",
            "Epoch(5/10)\t Batch(2200/3375): \tTrain Loss  2.3014\n",
            "Epoch(5/10)\t Batch(2400/3375): \tTrain Loss  2.3032\n",
            "Epoch(5/10)\t Batch(2600/3375): \tTrain Loss  2.3060\n",
            "Epoch(5/10)\t Batch(2800/3375): \tTrain Loss  2.3083\n",
            "Epoch(5/10)\t Batch(3000/3375): \tTrain Loss  2.3104\n",
            "Epoch(5/10)\t Batch(3200/3375): \tTrain Loss  2.3070\n",
            "End of Epoch: 5 Train Accuracy: 0.1000 Val Loss: 2.3032 Val Accuracy: 0.1002\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/3375): \tTrain Loss  2.2943\n",
            "Epoch(6/10)\t Batch(200/3375): \tTrain Loss  2.3038\n",
            "Epoch(6/10)\t Batch(400/3375): \tTrain Loss  2.3126\n",
            "Epoch(6/10)\t Batch(600/3375): \tTrain Loss  2.3025\n",
            "Epoch(6/10)\t Batch(800/3375): \tTrain Loss  2.3039\n",
            "Epoch(6/10)\t Batch(1000/3375): \tTrain Loss  2.2986\n",
            "Epoch(6/10)\t Batch(1200/3375): \tTrain Loss  2.3032\n",
            "Epoch(6/10)\t Batch(1400/3375): \tTrain Loss  2.2922\n",
            "Epoch(6/10)\t Batch(1600/3375): \tTrain Loss  2.3077\n",
            "Epoch(6/10)\t Batch(1800/3375): \tTrain Loss  2.2950\n",
            "Epoch(6/10)\t Batch(2000/3375): \tTrain Loss  2.2935\n",
            "Epoch(6/10)\t Batch(2200/3375): \tTrain Loss  2.3026\n",
            "Epoch(6/10)\t Batch(2400/3375): \tTrain Loss  2.3082\n",
            "Epoch(6/10)\t Batch(2600/3375): \tTrain Loss  2.3149\n",
            "Epoch(6/10)\t Batch(2800/3375): \tTrain Loss  2.3090\n",
            "Epoch(6/10)\t Batch(3000/3375): \tTrain Loss  2.2967\n",
            "Epoch(6/10)\t Batch(3200/3375): \tTrain Loss  2.2906\n",
            "End of Epoch: 6 Train Accuracy: 0.1001 Val Loss: 2.3032 Val Accuracy: 0.0988\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/3375): \tTrain Loss  2.3001\n",
            "Epoch(7/10)\t Batch(200/3375): \tTrain Loss  2.3045\n",
            "Epoch(7/10)\t Batch(400/3375): \tTrain Loss  2.3026\n",
            "Epoch(7/10)\t Batch(600/3375): \tTrain Loss  2.3055\n",
            "Epoch(7/10)\t Batch(800/3375): \tTrain Loss  2.3016\n",
            "Epoch(7/10)\t Batch(1000/3375): \tTrain Loss  2.2978\n",
            "Epoch(7/10)\t Batch(1200/3375): \tTrain Loss  2.2911\n",
            "Epoch(7/10)\t Batch(1400/3375): \tTrain Loss  2.3012\n",
            "Epoch(7/10)\t Batch(1600/3375): \tTrain Loss  2.3035\n",
            "Epoch(7/10)\t Batch(1800/3375): \tTrain Loss  2.2944\n",
            "Epoch(7/10)\t Batch(2000/3375): \tTrain Loss  2.3044\n",
            "Epoch(7/10)\t Batch(2200/3375): \tTrain Loss  2.3029\n",
            "Epoch(7/10)\t Batch(2400/3375): \tTrain Loss  2.3010\n",
            "Epoch(7/10)\t Batch(2600/3375): \tTrain Loss  2.3030\n",
            "Epoch(7/10)\t Batch(2800/3375): \tTrain Loss  2.3009\n",
            "Epoch(7/10)\t Batch(3000/3375): \tTrain Loss  2.2871\n",
            "Epoch(7/10)\t Batch(3200/3375): \tTrain Loss  2.3005\n",
            "End of Epoch: 7 Train Accuracy: 0.1001 Val Loss: 2.3029 Val Accuracy: 0.0988\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/3375): \tTrain Loss  2.2962\n",
            "Epoch(8/10)\t Batch(200/3375): \tTrain Loss  2.3096\n",
            "Epoch(8/10)\t Batch(400/3375): \tTrain Loss  2.3012\n",
            "Epoch(8/10)\t Batch(600/3375): \tTrain Loss  2.3030\n",
            "Epoch(8/10)\t Batch(800/3375): \tTrain Loss  2.2957\n",
            "Epoch(8/10)\t Batch(1000/3375): \tTrain Loss  2.2859\n",
            "Epoch(8/10)\t Batch(1200/3375): \tTrain Loss  2.3111\n",
            "Epoch(8/10)\t Batch(1400/3375): \tTrain Loss  2.3040\n",
            "Epoch(8/10)\t Batch(1600/3375): \tTrain Loss  2.3101\n",
            "Epoch(8/10)\t Batch(1800/3375): \tTrain Loss  2.3057\n",
            "Epoch(8/10)\t Batch(2000/3375): \tTrain Loss  2.2947\n",
            "Epoch(8/10)\t Batch(2200/3375): \tTrain Loss  2.3078\n",
            "Epoch(8/10)\t Batch(2400/3375): \tTrain Loss  2.2911\n",
            "Epoch(8/10)\t Batch(2600/3375): \tTrain Loss  2.2922\n",
            "Epoch(8/10)\t Batch(2800/3375): \tTrain Loss  2.2849\n",
            "Epoch(8/10)\t Batch(3000/3375): \tTrain Loss  2.3027\n",
            "Epoch(8/10)\t Batch(3200/3375): \tTrain Loss  2.3028\n",
            "End of Epoch: 8 Train Accuracy: 0.1000 Val Loss: 2.3030 Val Accuracy: 0.1003\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/3375): \tTrain Loss  2.3161\n",
            "Epoch(9/10)\t Batch(200/3375): \tTrain Loss  2.3003\n",
            "Epoch(9/10)\t Batch(400/3375): \tTrain Loss  2.3166\n",
            "Epoch(9/10)\t Batch(600/3375): \tTrain Loss  2.3080\n",
            "Epoch(9/10)\t Batch(800/3375): \tTrain Loss  2.3002\n",
            "Epoch(9/10)\t Batch(1000/3375): \tTrain Loss  2.3156\n",
            "Epoch(9/10)\t Batch(1200/3375): \tTrain Loss  2.3163\n",
            "Epoch(9/10)\t Batch(1400/3375): \tTrain Loss  2.3112\n",
            "Epoch(9/10)\t Batch(1600/3375): \tTrain Loss  2.2924\n",
            "Epoch(9/10)\t Batch(1800/3375): \tTrain Loss  2.2864\n",
            "Epoch(9/10)\t Batch(2000/3375): \tTrain Loss  2.2935\n",
            "Epoch(9/10)\t Batch(2200/3375): \tTrain Loss  2.3007\n",
            "Epoch(9/10)\t Batch(2400/3375): \tTrain Loss  2.3113\n",
            "Epoch(9/10)\t Batch(2600/3375): \tTrain Loss  2.3051\n",
            "Epoch(9/10)\t Batch(2800/3375): \tTrain Loss  2.3029\n",
            "Epoch(9/10)\t Batch(3000/3375): \tTrain Loss  2.2984\n",
            "Epoch(9/10)\t Batch(3200/3375): \tTrain Loss  2.3009\n",
            "End of Epoch: 9 Train Accuracy: 0.0998 Val Loss: 2.3036 Val Accuracy: 0.1020\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/3375): \tTrain Loss  2.3035\n",
            "Epoch(10/10)\t Batch(200/3375): \tTrain Loss  2.3005\n",
            "Epoch(10/10)\t Batch(400/3375): \tTrain Loss  2.2930\n",
            "Epoch(10/10)\t Batch(600/3375): \tTrain Loss  2.3088\n",
            "Epoch(10/10)\t Batch(800/3375): \tTrain Loss  2.2948\n",
            "Epoch(10/10)\t Batch(1000/3375): \tTrain Loss  2.3057\n",
            "Epoch(10/10)\t Batch(1200/3375): \tTrain Loss  2.2974\n",
            "Epoch(10/10)\t Batch(1400/3375): \tTrain Loss  2.2980\n",
            "Epoch(10/10)\t Batch(1600/3375): \tTrain Loss  2.3040\n",
            "Epoch(10/10)\t Batch(1800/3375): \tTrain Loss  2.3096\n",
            "Epoch(10/10)\t Batch(2000/3375): \tTrain Loss  2.3084\n",
            "Epoch(10/10)\t Batch(2200/3375): \tTrain Loss  2.2926\n",
            "Epoch(10/10)\t Batch(2400/3375): \tTrain Loss  2.3106\n",
            "Epoch(10/10)\t Batch(2600/3375): \tTrain Loss  2.3084\n",
            "Epoch(10/10)\t Batch(2800/3375): \tTrain Loss  2.2954\n",
            "Epoch(10/10)\t Batch(3000/3375): \tTrain Loss  2.2927\n",
            "Epoch(10/10)\t Batch(3200/3375): \tTrain Loss  2.2874\n",
            "End of Epoch: 10 Train Accuracy: 0.1001 Val Loss: 2.3032 Val Accuracy: 0.0988\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train Loss</td><td>▁▆█▆▅█▆▇▆▅</td></tr><tr><td>Val Accuracy</td><td>▁█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.10013</td></tr><tr><td>Train Loss</td><td>2.29201</td></tr><tr><td>Val Accuracy</td><td>0.09883</td></tr><tr><td>Val Loss</td><td>2.30319</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_momentum|loss_cross_entropy|lr=0.001|batch_16|act_sigmoid|hid_5|neurons_128|nrns_10|init_Random902</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/gw1v4tgo' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/gw1v4tgo</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_171134-gw1v4tgo/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j3ur5fh3 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_171425-j3ur5fh3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/j3ur5fh3' target=\"_blank\">avid-sweep-29</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/j3ur5fh3' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/j3ur5fh3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.1104 Val Loss: 0.1775 Val Accuracy: 0.1117\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/3375): \tTrain Loss  0.2000\n",
            "Epoch(1/5)\t Batch(200/3375): \tTrain Loss  0.1500\n",
            "Epoch(1/5)\t Batch(400/3375): \tTrain Loss  0.2000\n",
            "Epoch(1/5)\t Batch(600/3375): \tTrain Loss  0.1625\n",
            "Epoch(1/5)\t Batch(800/3375): \tTrain Loss  0.1500\n",
            "Epoch(1/5)\t Batch(1000/3375): \tTrain Loss  0.1875\n",
            "Epoch(1/5)\t Batch(1200/3375): \tTrain Loss  0.1875\n",
            "Epoch(1/5)\t Batch(1400/3375): \tTrain Loss  0.1625\n",
            "Epoch(1/5)\t Batch(1600/3375): \tTrain Loss  0.1625\n",
            "Epoch(1/5)\t Batch(1800/3375): \tTrain Loss  0.2000\n",
            "Epoch(1/5)\t Batch(2000/3375): \tTrain Loss  0.1375\n",
            "Epoch(1/5)\t Batch(2200/3375): \tTrain Loss  0.2000\n",
            "Epoch(1/5)\t Batch(2400/3375): \tTrain Loss  0.2000\n",
            "Epoch(1/5)\t Batch(2600/3375): \tTrain Loss  0.2000\n",
            "Epoch(1/5)\t Batch(2800/3375): \tTrain Loss  0.1750\n",
            "Epoch(1/5)\t Batch(3000/3375): \tTrain Loss  0.1875\n",
            "Epoch(1/5)\t Batch(3200/3375): \tTrain Loss  0.1625\n",
            "End of Epoch: 1 Train Accuracy: 0.1181 Val Loss: 0.1761 Val Accuracy: 0.1190\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/3375): \tTrain Loss  0.1750\n",
            "Epoch(2/5)\t Batch(200/3375): \tTrain Loss  0.2000\n",
            "Epoch(2/5)\t Batch(400/3375): \tTrain Loss  0.1875\n",
            "Epoch(2/5)\t Batch(600/3375): \tTrain Loss  0.1625\n",
            "Epoch(2/5)\t Batch(800/3375): \tTrain Loss  0.1997\n",
            "Epoch(2/5)\t Batch(1000/3375): \tTrain Loss  0.2000\n",
            "Epoch(2/5)\t Batch(1200/3375): \tTrain Loss  0.2000\n",
            "Epoch(2/5)\t Batch(1400/3375): \tTrain Loss  0.2000\n",
            "Epoch(2/5)\t Batch(1600/3375): \tTrain Loss  0.1750\n",
            "Epoch(2/5)\t Batch(1800/3375): \tTrain Loss  0.2000\n",
            "Epoch(2/5)\t Batch(2000/3375): \tTrain Loss  0.1875\n",
            "Epoch(2/5)\t Batch(2200/3375): \tTrain Loss  0.1875\n",
            "Epoch(2/5)\t Batch(2400/3375): \tTrain Loss  0.1875\n",
            "Epoch(2/5)\t Batch(2600/3375): \tTrain Loss  0.2000\n",
            "Epoch(2/5)\t Batch(2800/3375): \tTrain Loss  0.1750\n",
            "Epoch(2/5)\t Batch(3000/3375): \tTrain Loss  0.1625\n",
            "Epoch(2/5)\t Batch(3200/3375): \tTrain Loss  0.1750\n",
            "End of Epoch: 2 Train Accuracy: 0.1344 Val Loss: 0.1728 Val Accuracy: 0.1355\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/3375): \tTrain Loss  0.1750\n",
            "Epoch(3/5)\t Batch(200/3375): \tTrain Loss  0.1625\n",
            "Epoch(3/5)\t Batch(400/3375): \tTrain Loss  0.1750\n",
            "Epoch(3/5)\t Batch(600/3375): \tTrain Loss  0.1625\n",
            "Epoch(3/5)\t Batch(800/3375): \tTrain Loss  0.1729\n",
            "Epoch(3/5)\t Batch(1000/3375): \tTrain Loss  0.1875\n",
            "Epoch(3/5)\t Batch(1200/3375): \tTrain Loss  0.1625\n",
            "Epoch(3/5)\t Batch(1400/3375): \tTrain Loss  0.1875\n",
            "Epoch(3/5)\t Batch(1600/3375): \tTrain Loss  0.1500\n",
            "Epoch(3/5)\t Batch(1800/3375): \tTrain Loss  0.1625\n",
            "Epoch(3/5)\t Batch(2000/3375): \tTrain Loss  0.1500\n",
            "Epoch(3/5)\t Batch(2200/3375): \tTrain Loss  0.1875\n",
            "Epoch(3/5)\t Batch(2400/3375): \tTrain Loss  0.1500\n",
            "Epoch(3/5)\t Batch(2600/3375): \tTrain Loss  0.2000\n",
            "Epoch(3/5)\t Batch(2800/3375): \tTrain Loss  0.1875\n",
            "Epoch(3/5)\t Batch(3000/3375): \tTrain Loss  0.1355\n",
            "Epoch(3/5)\t Batch(3200/3375): \tTrain Loss  0.1875\n",
            "End of Epoch: 3 Train Accuracy: 0.1507 Val Loss: 0.1697 Val Accuracy: 0.1512\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/3375): \tTrain Loss  0.1746\n",
            "Epoch(4/5)\t Batch(200/3375): \tTrain Loss  0.1875\n",
            "Epoch(4/5)\t Batch(400/3375): \tTrain Loss  0.1875\n",
            "Epoch(4/5)\t Batch(600/3375): \tTrain Loss  0.1749\n",
            "Epoch(4/5)\t Batch(800/3375): \tTrain Loss  0.1625\n",
            "Epoch(4/5)\t Batch(1000/3375): \tTrain Loss  0.1625\n",
            "Epoch(4/5)\t Batch(1200/3375): \tTrain Loss  0.1625\n",
            "Epoch(4/5)\t Batch(1400/3375): \tTrain Loss  0.1500\n",
            "Epoch(4/5)\t Batch(1600/3375): \tTrain Loss  0.1750\n",
            "Epoch(4/5)\t Batch(1800/3375): \tTrain Loss  0.1750\n",
            "Epoch(4/5)\t Batch(2000/3375): \tTrain Loss  0.1750\n",
            "Epoch(4/5)\t Batch(2200/3375): \tTrain Loss  0.1750\n",
            "Epoch(4/5)\t Batch(2400/3375): \tTrain Loss  0.1875\n",
            "Epoch(4/5)\t Batch(2600/3375): \tTrain Loss  0.1624\n",
            "Epoch(4/5)\t Batch(2800/3375): \tTrain Loss  0.1500\n",
            "Epoch(4/5)\t Batch(3000/3375): \tTrain Loss  0.1750\n",
            "Epoch(4/5)\t Batch(3200/3375): \tTrain Loss  0.1250\n",
            "End of Epoch: 4 Train Accuracy: 0.1549 Val Loss: 0.1694 Val Accuracy: 0.1527\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/3375): \tTrain Loss  0.1844\n",
            "Epoch(5/5)\t Batch(200/3375): \tTrain Loss  0.1750\n",
            "Epoch(5/5)\t Batch(400/3375): \tTrain Loss  0.1500\n",
            "Epoch(5/5)\t Batch(600/3375): \tTrain Loss  0.1375\n",
            "Epoch(5/5)\t Batch(800/3375): \tTrain Loss  0.1625\n",
            "Epoch(5/5)\t Batch(1000/3375): \tTrain Loss  0.1500\n",
            "Epoch(5/5)\t Batch(1200/3375): \tTrain Loss  0.1500\n",
            "Epoch(5/5)\t Batch(1400/3375): \tTrain Loss  0.1875\n",
            "Epoch(5/5)\t Batch(1600/3375): \tTrain Loss  0.1500\n",
            "Epoch(5/5)\t Batch(1800/3375): \tTrain Loss  0.2000\n",
            "Epoch(5/5)\t Batch(2000/3375): \tTrain Loss  0.1625\n",
            "Epoch(5/5)\t Batch(2200/3375): \tTrain Loss  0.1625\n",
            "Epoch(5/5)\t Batch(2400/3375): \tTrain Loss  0.1375\n",
            "Epoch(5/5)\t Batch(2600/3375): \tTrain Loss  0.1625\n",
            "Epoch(5/5)\t Batch(2800/3375): \tTrain Loss  0.1875\n",
            "Epoch(5/5)\t Batch(3000/3375): \tTrain Loss  0.1375\n",
            "Epoch(5/5)\t Batch(3200/3375): \tTrain Loss  0.1625\n",
            "End of Epoch: 5 Train Accuracy: 0.1692 Val Loss: 0.1667 Val Accuracy: 0.1660\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▂▄▆▆█</td></tr><tr><td>Train Loss</td><td>▁▃▁█▆</td></tr><tr><td>Val Accuracy</td><td>▁▂▄▆▆█</td></tr><tr><td>Val Loss</td><td>█▇▅▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.16922</td></tr><tr><td>Train Loss</td><td>0.1875</td></tr><tr><td>Val Accuracy</td><td>0.166</td></tr><tr><td>Val Loss</td><td>0.16672</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_mse|lr=0.0001|batch_16|act_relu|hid_4|neurons_32|nrns_5|init_Random570</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/j3ur5fh3' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/j3ur5fh3</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_171425-j3ur5fh3/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: da15jzs1 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_171516-da15jzs1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/da15jzs1' target=\"_blank\">magic-sweep-30</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/da15jzs1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/da15jzs1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0731 Val Loss: 25.4856 Val Accuracy: 0.0775\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/1687): \tTrain Loss  26.7676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-4f78d507ffd0>:24: RuntimeWarning: overflow encountered in matmul\n",
            "  self.out = x @ self.weight\n",
            "<ipython-input-2-4f78d507ffd0>:24: RuntimeWarning: invalid value encountered in matmul\n",
            "  self.out = x @ self.weight\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch(1/5)\t Batch(200/1687): \tTrain Loss  nan\n",
            "Epoch(1/5)\t Batch(400/1687): \tTrain Loss  nan\n",
            "Epoch(1/5)\t Batch(600/1687): \tTrain Loss  nan\n",
            "Epoch(1/5)\t Batch(800/1687): \tTrain Loss  nan\n",
            "Epoch(1/5)\t Batch(1000/1687): \tTrain Loss  nan\n",
            "Epoch(1/5)\t Batch(1200/1687): \tTrain Loss  nan\n",
            "Epoch(1/5)\t Batch(1400/1687): \tTrain Loss  nan\n",
            "Epoch(1/5)\t Batch(1600/1687): \tTrain Loss  nan\n",
            "End of Epoch: 1 Train Accuracy: 0.1004 Val Loss: nan Val Accuracy: 0.0963\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/1687): \tTrain Loss  nan\n",
            "Epoch(2/5)\t Batch(200/1687): \tTrain Loss  nan\n",
            "Epoch(2/5)\t Batch(400/1687): \tTrain Loss  nan\n",
            "Epoch(2/5)\t Batch(600/1687): \tTrain Loss  nan\n",
            "Epoch(2/5)\t Batch(800/1687): \tTrain Loss  nan\n",
            "Epoch(2/5)\t Batch(1000/1687): \tTrain Loss  nan\n",
            "Epoch(2/5)\t Batch(1200/1687): \tTrain Loss  nan\n",
            "Epoch(2/5)\t Batch(1400/1687): \tTrain Loss  nan\n",
            "Epoch(2/5)\t Batch(1600/1687): \tTrain Loss  nan\n",
            "End of Epoch: 2 Train Accuracy: 0.1004 Val Loss: nan Val Accuracy: 0.0963\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/1687): \tTrain Loss  nan\n",
            "Epoch(3/5)\t Batch(200/1687): \tTrain Loss  nan\n",
            "Epoch(3/5)\t Batch(400/1687): \tTrain Loss  nan\n",
            "Epoch(3/5)\t Batch(600/1687): \tTrain Loss  nan\n",
            "Epoch(3/5)\t Batch(800/1687): \tTrain Loss  nan\n",
            "Epoch(3/5)\t Batch(1000/1687): \tTrain Loss  nan\n",
            "Epoch(3/5)\t Batch(1200/1687): \tTrain Loss  nan\n",
            "Epoch(3/5)\t Batch(1400/1687): \tTrain Loss  nan\n",
            "Epoch(3/5)\t Batch(1600/1687): \tTrain Loss  nan\n",
            "End of Epoch: 3 Train Accuracy: 0.1004 Val Loss: nan Val Accuracy: 0.0963\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/1687): \tTrain Loss  nan\n",
            "Epoch(4/5)\t Batch(200/1687): \tTrain Loss  nan\n",
            "Epoch(4/5)\t Batch(400/1687): \tTrain Loss  nan\n",
            "Epoch(4/5)\t Batch(600/1687): \tTrain Loss  nan\n",
            "Epoch(4/5)\t Batch(800/1687): \tTrain Loss  nan\n",
            "Epoch(4/5)\t Batch(1000/1687): \tTrain Loss  nan\n",
            "Epoch(4/5)\t Batch(1200/1687): \tTrain Loss  nan\n",
            "Epoch(4/5)\t Batch(1400/1687): \tTrain Loss  nan\n",
            "Epoch(4/5)\t Batch(1600/1687): \tTrain Loss  nan\n",
            "End of Epoch: 4 Train Accuracy: 0.1004 Val Loss: nan Val Accuracy: 0.0963\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/1687): \tTrain Loss  nan\n",
            "Epoch(5/5)\t Batch(200/1687): \tTrain Loss  nan\n",
            "Epoch(5/5)\t Batch(400/1687): \tTrain Loss  nan\n",
            "Epoch(5/5)\t Batch(600/1687): \tTrain Loss  nan\n",
            "Epoch(5/5)\t Batch(800/1687): \tTrain Loss  nan\n",
            "Epoch(5/5)\t Batch(1000/1687): \tTrain Loss  nan\n",
            "Epoch(5/5)\t Batch(1200/1687): \tTrain Loss  nan\n",
            "Epoch(5/5)\t Batch(1400/1687): \tTrain Loss  nan\n",
            "Epoch(5/5)\t Batch(1600/1687): \tTrain Loss  nan\n",
            "End of Epoch: 5 Train Accuracy: 0.1004 Val Loss: nan Val Accuracy: 0.0963\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁█████</td></tr><tr><td>Val Accuracy</td><td>▁█████</td></tr><tr><td>Val Loss</td><td>▁     </td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.10041</td></tr><tr><td>Train Loss</td><td>nan</td></tr><tr><td>Val Accuracy</td><td>0.09633</td></tr><tr><td>Val Loss</td><td>nan</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_sgd|loss_cross_entropy|lr=0.001|batch_32|act_relu|hid_4|neurons_128|nrns_5|init_Random38</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/da15jzs1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/da15jzs1</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_171516-da15jzs1/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jc5vyl6n with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_171601-jc5vyl6n</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/jc5vyl6n' target=\"_blank\">dandy-sweep-31</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/jc5vyl6n' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/jc5vyl6n</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.0995 Val Loss: 2.4414 Val Accuracy: 0.1045\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/3375): \tTrain Loss  2.6012\n",
            "Epoch(1/10)\t Batch(200/3375): \tTrain Loss  2.3026\n",
            "Epoch(1/10)\t Batch(400/3375): \tTrain Loss  2.3015\n",
            "Epoch(1/10)\t Batch(600/3375): \tTrain Loss  2.3071\n",
            "Epoch(1/10)\t Batch(800/3375): \tTrain Loss  2.3020\n",
            "Epoch(1/10)\t Batch(1000/3375): \tTrain Loss  2.3010\n",
            "Epoch(1/10)\t Batch(1200/3375): \tTrain Loss  2.3000\n",
            "Epoch(1/10)\t Batch(1400/3375): \tTrain Loss  2.3059\n",
            "Epoch(1/10)\t Batch(1600/3375): \tTrain Loss  2.3033\n",
            "Epoch(1/10)\t Batch(1800/3375): \tTrain Loss  2.3072\n",
            "Epoch(1/10)\t Batch(2000/3375): \tTrain Loss  2.3035\n",
            "Epoch(1/10)\t Batch(2200/3375): \tTrain Loss  2.3044\n",
            "Epoch(1/10)\t Batch(2400/3375): \tTrain Loss  2.3031\n",
            "Epoch(1/10)\t Batch(2600/3375): \tTrain Loss  2.2999\n",
            "Epoch(1/10)\t Batch(2800/3375): \tTrain Loss  2.3030\n",
            "Epoch(1/10)\t Batch(3000/3375): \tTrain Loss  2.3073\n",
            "Epoch(1/10)\t Batch(3200/3375): \tTrain Loss  2.2997\n",
            "End of Epoch: 1 Train Accuracy: 0.0989 Val Loss: 2.3006 Val Accuracy: 0.1100\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/3375): \tTrain Loss  2.3021\n",
            "Epoch(2/10)\t Batch(200/3375): \tTrain Loss  2.2983\n",
            "Epoch(2/10)\t Batch(400/3375): \tTrain Loss  2.2986\n",
            "Epoch(2/10)\t Batch(600/3375): \tTrain Loss  2.2953\n",
            "Epoch(2/10)\t Batch(800/3375): \tTrain Loss  2.2967\n",
            "Epoch(2/10)\t Batch(1000/3375): \tTrain Loss  2.2844\n",
            "Epoch(2/10)\t Batch(1200/3375): \tTrain Loss  2.2505\n",
            "Epoch(2/10)\t Batch(1400/3375): \tTrain Loss  2.2345\n",
            "Epoch(2/10)\t Batch(1600/3375): \tTrain Loss  2.2357\n",
            "Epoch(2/10)\t Batch(1800/3375): \tTrain Loss  2.2160\n",
            "Epoch(2/10)\t Batch(2000/3375): \tTrain Loss  2.2129\n",
            "Epoch(2/10)\t Batch(2200/3375): \tTrain Loss  2.1923\n",
            "Epoch(2/10)\t Batch(2400/3375): \tTrain Loss  2.2162\n",
            "Epoch(2/10)\t Batch(2600/3375): \tTrain Loss  2.2021\n",
            "Epoch(2/10)\t Batch(2800/3375): \tTrain Loss  2.1951\n",
            "Epoch(2/10)\t Batch(3000/3375): \tTrain Loss  2.1866\n",
            "Epoch(2/10)\t Batch(3200/3375): \tTrain Loss  2.1770\n",
            "End of Epoch: 2 Train Accuracy: 0.2424 Val Loss: 2.2058 Val Accuracy: 0.2392\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/3375): \tTrain Loss  2.1996\n",
            "Epoch(3/10)\t Batch(200/3375): \tTrain Loss  2.1895\n",
            "Epoch(3/10)\t Batch(400/3375): \tTrain Loss  2.2100\n",
            "Epoch(3/10)\t Batch(600/3375): \tTrain Loss  2.2010\n",
            "Epoch(3/10)\t Batch(800/3375): \tTrain Loss  2.1992\n",
            "Epoch(3/10)\t Batch(1000/3375): \tTrain Loss  2.2000\n",
            "Epoch(3/10)\t Batch(1200/3375): \tTrain Loss  2.1840\n",
            "Epoch(3/10)\t Batch(1400/3375): \tTrain Loss  2.2138\n",
            "Epoch(3/10)\t Batch(1600/3375): \tTrain Loss  2.1992\n",
            "Epoch(3/10)\t Batch(1800/3375): \tTrain Loss  2.1822\n",
            "Epoch(3/10)\t Batch(2000/3375): \tTrain Loss  2.1983\n",
            "Epoch(3/10)\t Batch(2200/3375): \tTrain Loss  2.1839\n",
            "Epoch(3/10)\t Batch(2400/3375): \tTrain Loss  2.1366\n",
            "Epoch(3/10)\t Batch(2600/3375): \tTrain Loss  2.2115\n",
            "Epoch(3/10)\t Batch(2800/3375): \tTrain Loss  2.1519\n",
            "Epoch(3/10)\t Batch(3000/3375): \tTrain Loss  2.1880\n",
            "Epoch(3/10)\t Batch(3200/3375): \tTrain Loss  2.1929\n",
            "End of Epoch: 3 Train Accuracy: 0.3417 Val Loss: 2.1465 Val Accuracy: 0.3523\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/3375): \tTrain Loss  2.0844\n",
            "Epoch(4/10)\t Batch(200/3375): \tTrain Loss  2.1457\n",
            "Epoch(4/10)\t Batch(400/3375): \tTrain Loss  2.1938\n",
            "Epoch(4/10)\t Batch(600/3375): \tTrain Loss  2.1990\n",
            "Epoch(4/10)\t Batch(800/3375): \tTrain Loss  2.1805\n",
            "Epoch(4/10)\t Batch(1000/3375): \tTrain Loss  2.0618\n",
            "Epoch(4/10)\t Batch(1200/3375): \tTrain Loss  2.1510\n",
            "Epoch(4/10)\t Batch(1400/3375): \tTrain Loss  2.1620\n",
            "Epoch(4/10)\t Batch(1600/3375): \tTrain Loss  2.1735\n",
            "Epoch(4/10)\t Batch(1800/3375): \tTrain Loss  2.1030\n",
            "Epoch(4/10)\t Batch(2000/3375): \tTrain Loss  2.1648\n",
            "Epoch(4/10)\t Batch(2200/3375): \tTrain Loss  2.1114\n",
            "Epoch(4/10)\t Batch(2400/3375): \tTrain Loss  2.1510\n",
            "Epoch(4/10)\t Batch(2600/3375): \tTrain Loss  2.1831\n",
            "Epoch(4/10)\t Batch(2800/3375): \tTrain Loss  2.1362\n",
            "Epoch(4/10)\t Batch(3000/3375): \tTrain Loss  2.0763\n",
            "Epoch(4/10)\t Batch(3200/3375): \tTrain Loss  2.1210\n",
            "End of Epoch: 4 Train Accuracy: 0.4539 Val Loss: 2.0969 Val Accuracy: 0.4560\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/3375): \tTrain Loss  2.1412\n",
            "Epoch(5/10)\t Batch(200/3375): \tTrain Loss  2.1407\n",
            "Epoch(5/10)\t Batch(400/3375): \tTrain Loss  2.1301\n",
            "Epoch(5/10)\t Batch(600/3375): \tTrain Loss  2.1622\n",
            "Epoch(5/10)\t Batch(800/3375): \tTrain Loss  2.1346\n",
            "Epoch(5/10)\t Batch(1000/3375): \tTrain Loss  2.1121\n",
            "Epoch(5/10)\t Batch(1200/3375): \tTrain Loss  2.0664\n",
            "Epoch(5/10)\t Batch(1400/3375): \tTrain Loss  2.1043\n",
            "Epoch(5/10)\t Batch(1600/3375): \tTrain Loss  2.1122\n",
            "Epoch(5/10)\t Batch(1800/3375): \tTrain Loss  2.0380\n",
            "Epoch(5/10)\t Batch(2000/3375): \tTrain Loss  2.1006\n",
            "Epoch(5/10)\t Batch(2200/3375): \tTrain Loss  2.0923\n",
            "Epoch(5/10)\t Batch(2400/3375): \tTrain Loss  2.0143\n",
            "Epoch(5/10)\t Batch(2600/3375): \tTrain Loss  1.9765\n",
            "Epoch(5/10)\t Batch(2800/3375): \tTrain Loss  2.0620\n",
            "Epoch(5/10)\t Batch(3000/3375): \tTrain Loss  2.1101\n",
            "Epoch(5/10)\t Batch(3200/3375): \tTrain Loss  2.0019\n",
            "End of Epoch: 5 Train Accuracy: 0.5007 Val Loss: 2.0426 Val Accuracy: 0.5023\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/3375): \tTrain Loss  2.1327\n",
            "Epoch(6/10)\t Batch(200/3375): \tTrain Loss  2.0574\n",
            "Epoch(6/10)\t Batch(400/3375): \tTrain Loss  1.9594\n",
            "Epoch(6/10)\t Batch(600/3375): \tTrain Loss  2.0040\n",
            "Epoch(6/10)\t Batch(800/3375): \tTrain Loss  2.0487\n",
            "Epoch(6/10)\t Batch(1000/3375): \tTrain Loss  2.0333\n",
            "Epoch(6/10)\t Batch(1200/3375): \tTrain Loss  2.0612\n",
            "Epoch(6/10)\t Batch(1400/3375): \tTrain Loss  2.0308\n",
            "Epoch(6/10)\t Batch(1600/3375): \tTrain Loss  1.9809\n",
            "Epoch(6/10)\t Batch(1800/3375): \tTrain Loss  1.9340\n",
            "Epoch(6/10)\t Batch(2000/3375): \tTrain Loss  1.9638\n",
            "Epoch(6/10)\t Batch(2200/3375): \tTrain Loss  2.0041\n",
            "Epoch(6/10)\t Batch(2400/3375): \tTrain Loss  2.0559\n",
            "Epoch(6/10)\t Batch(2600/3375): \tTrain Loss  2.0630\n",
            "Epoch(6/10)\t Batch(2800/3375): \tTrain Loss  1.9786\n",
            "Epoch(6/10)\t Batch(3000/3375): \tTrain Loss  2.0261\n",
            "Epoch(6/10)\t Batch(3200/3375): \tTrain Loss  1.9648\n",
            "End of Epoch: 6 Train Accuracy: 0.5681 Val Loss: 1.9787 Val Accuracy: 0.5717\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/3375): \tTrain Loss  1.9964\n",
            "Epoch(7/10)\t Batch(200/3375): \tTrain Loss  1.9604\n",
            "Epoch(7/10)\t Batch(400/3375): \tTrain Loss  2.0615\n",
            "Epoch(7/10)\t Batch(600/3375): \tTrain Loss  2.0523\n",
            "Epoch(7/10)\t Batch(800/3375): \tTrain Loss  2.0163\n",
            "Epoch(7/10)\t Batch(1000/3375): \tTrain Loss  1.9489\n",
            "Epoch(7/10)\t Batch(1200/3375): \tTrain Loss  1.8841\n",
            "Epoch(7/10)\t Batch(1400/3375): \tTrain Loss  1.9496\n",
            "Epoch(7/10)\t Batch(1600/3375): \tTrain Loss  1.9661\n",
            "Epoch(7/10)\t Batch(1800/3375): \tTrain Loss  1.8649\n",
            "Epoch(7/10)\t Batch(2000/3375): \tTrain Loss  1.9135\n",
            "Epoch(7/10)\t Batch(2200/3375): \tTrain Loss  1.8508\n",
            "Epoch(7/10)\t Batch(2400/3375): \tTrain Loss  1.7650\n",
            "Epoch(7/10)\t Batch(2600/3375): \tTrain Loss  1.9356\n",
            "Epoch(7/10)\t Batch(2800/3375): \tTrain Loss  1.7962\n",
            "Epoch(7/10)\t Batch(3000/3375): \tTrain Loss  1.8815\n",
            "Epoch(7/10)\t Batch(3200/3375): \tTrain Loss  1.8418\n",
            "End of Epoch: 7 Train Accuracy: 0.6932 Val Loss: 1.8726 Val Accuracy: 0.7027\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/3375): \tTrain Loss  1.8204\n",
            "Epoch(8/10)\t Batch(200/3375): \tTrain Loss  1.8075\n",
            "Epoch(8/10)\t Batch(400/3375): \tTrain Loss  1.8900\n",
            "Epoch(8/10)\t Batch(600/3375): \tTrain Loss  1.7601\n",
            "Epoch(8/10)\t Batch(800/3375): \tTrain Loss  1.7150\n",
            "Epoch(8/10)\t Batch(1000/3375): \tTrain Loss  1.8700\n",
            "Epoch(8/10)\t Batch(1200/3375): \tTrain Loss  1.9245\n",
            "Epoch(8/10)\t Batch(1400/3375): \tTrain Loss  1.8007\n",
            "Epoch(8/10)\t Batch(1600/3375): \tTrain Loss  2.0260\n",
            "Epoch(8/10)\t Batch(1800/3375): \tTrain Loss  1.9180\n",
            "Epoch(8/10)\t Batch(2000/3375): \tTrain Loss  1.9579\n",
            "Epoch(8/10)\t Batch(2200/3375): \tTrain Loss  1.8597\n",
            "Epoch(8/10)\t Batch(2400/3375): \tTrain Loss  1.7344\n",
            "Epoch(8/10)\t Batch(2600/3375): \tTrain Loss  1.8062\n",
            "Epoch(8/10)\t Batch(2800/3375): \tTrain Loss  1.8541\n",
            "Epoch(8/10)\t Batch(3000/3375): \tTrain Loss  1.8192\n",
            "Epoch(8/10)\t Batch(3200/3375): \tTrain Loss  1.7797\n",
            "End of Epoch: 8 Train Accuracy: 0.7186 Val Loss: 1.8228 Val Accuracy: 0.7277\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/3375): \tTrain Loss  1.7260\n",
            "Epoch(9/10)\t Batch(200/3375): \tTrain Loss  1.8388\n",
            "Epoch(9/10)\t Batch(400/3375): \tTrain Loss  1.6869\n",
            "Epoch(9/10)\t Batch(600/3375): \tTrain Loss  1.7325\n",
            "Epoch(9/10)\t Batch(800/3375): \tTrain Loss  1.7817\n",
            "Epoch(9/10)\t Batch(1000/3375): \tTrain Loss  1.8261\n",
            "Epoch(9/10)\t Batch(1200/3375): \tTrain Loss  1.8891\n",
            "Epoch(9/10)\t Batch(1400/3375): \tTrain Loss  1.8069\n",
            "Epoch(9/10)\t Batch(1600/3375): \tTrain Loss  1.7437\n",
            "Epoch(9/10)\t Batch(1800/3375): \tTrain Loss  1.8566\n",
            "Epoch(9/10)\t Batch(2000/3375): \tTrain Loss  1.8199\n",
            "Epoch(9/10)\t Batch(2200/3375): \tTrain Loss  1.7035\n",
            "Epoch(9/10)\t Batch(2400/3375): \tTrain Loss  1.8231\n",
            "Epoch(9/10)\t Batch(2600/3375): \tTrain Loss  1.7435\n",
            "Epoch(9/10)\t Batch(2800/3375): \tTrain Loss  1.8475\n",
            "Epoch(9/10)\t Batch(3000/3375): \tTrain Loss  1.7800\n",
            "Epoch(9/10)\t Batch(3200/3375): \tTrain Loss  1.8415\n",
            "End of Epoch: 9 Train Accuracy: 0.7246 Val Loss: 1.7884 Val Accuracy: 0.7338\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/3375): \tTrain Loss  1.8451\n",
            "Epoch(10/10)\t Batch(200/3375): \tTrain Loss  1.8176\n",
            "Epoch(10/10)\t Batch(400/3375): \tTrain Loss  1.8555\n",
            "Epoch(10/10)\t Batch(600/3375): \tTrain Loss  1.7851\n",
            "Epoch(10/10)\t Batch(800/3375): \tTrain Loss  1.7105\n",
            "Epoch(10/10)\t Batch(1000/3375): \tTrain Loss  1.8291\n",
            "Epoch(10/10)\t Batch(1200/3375): \tTrain Loss  1.8083\n",
            "Epoch(10/10)\t Batch(1400/3375): \tTrain Loss  1.7862\n",
            "Epoch(10/10)\t Batch(1600/3375): \tTrain Loss  1.7256\n",
            "Epoch(10/10)\t Batch(1800/3375): \tTrain Loss  1.7884\n",
            "Epoch(10/10)\t Batch(2000/3375): \tTrain Loss  1.7390\n",
            "Epoch(10/10)\t Batch(2200/3375): \tTrain Loss  1.8822\n",
            "Epoch(10/10)\t Batch(2400/3375): \tTrain Loss  1.8243\n",
            "Epoch(10/10)\t Batch(2600/3375): \tTrain Loss  1.7432\n",
            "Epoch(10/10)\t Batch(2800/3375): \tTrain Loss  1.8180\n",
            "Epoch(10/10)\t Batch(3000/3375): \tTrain Loss  1.7074\n",
            "Epoch(10/10)\t Batch(3200/3375): \tTrain Loss  1.7457\n",
            "End of Epoch: 10 Train Accuracy: 0.7319 Val Loss: 1.7649 Val Accuracy: 0.7403\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▁▃▄▅▅▆████</td></tr><tr><td>Train Loss</td><td>█▇▆▅▆▄▃▂▃▁</td></tr><tr><td>Val Accuracy</td><td>▁▁▂▄▅▅▆████</td></tr><tr><td>Val Loss</td><td>█▇▆▅▄▄▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.73191</td></tr><tr><td>Train Loss</td><td>1.73433</td></tr><tr><td>Val Accuracy</td><td>0.74033</td></tr><tr><td>Val Loss</td><td>1.76487</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_cross_entropy|lr=0.0001|batch_16|act_sigmoid|hid_5|neurons_64|nrns_10|init_Xavier163</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/jc5vyl6n' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/jc5vyl6n</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_171601-jc5vyl6n/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zwqmuzhm with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_171757-zwqmuzhm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/zwqmuzhm' target=\"_blank\">glowing-sweep-32</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/zwqmuzhm' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/zwqmuzhm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.1047 Val Loss: 2.3037 Val Accuracy: 0.1000\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/1687): \tTrain Loss  2.3014\n",
            "Epoch(1/5)\t Batch(200/1687): \tTrain Loss  2.2943\n",
            "Epoch(1/5)\t Batch(400/1687): \tTrain Loss  2.2956\n",
            "Epoch(1/5)\t Batch(600/1687): \tTrain Loss  2.2916\n",
            "Epoch(1/5)\t Batch(800/1687): \tTrain Loss  2.2839\n",
            "Epoch(1/5)\t Batch(1000/1687): \tTrain Loss  2.2809\n",
            "Epoch(1/5)\t Batch(1200/1687): \tTrain Loss  2.2816\n",
            "Epoch(1/5)\t Batch(1400/1687): \tTrain Loss  2.2715\n",
            "Epoch(1/5)\t Batch(1600/1687): \tTrain Loss  2.2667\n",
            "End of Epoch: 1 Train Accuracy: 0.3979 Val Loss: 2.2701 Val Accuracy: 0.3903\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/1687): \tTrain Loss  2.2785\n",
            "Epoch(2/5)\t Batch(200/1687): \tTrain Loss  2.2589\n",
            "Epoch(2/5)\t Batch(400/1687): \tTrain Loss  2.2606\n",
            "Epoch(2/5)\t Batch(600/1687): \tTrain Loss  2.2553\n",
            "Epoch(2/5)\t Batch(800/1687): \tTrain Loss  2.2449\n",
            "Epoch(2/5)\t Batch(1000/1687): \tTrain Loss  2.2463\n",
            "Epoch(2/5)\t Batch(1200/1687): \tTrain Loss  2.2470\n",
            "Epoch(2/5)\t Batch(1400/1687): \tTrain Loss  2.2338\n",
            "Epoch(2/5)\t Batch(1600/1687): \tTrain Loss  2.2315\n",
            "End of Epoch: 2 Train Accuracy: 0.5285 Val Loss: 2.2288 Val Accuracy: 0.5190\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/1687): \tTrain Loss  2.2431\n",
            "Epoch(3/5)\t Batch(200/1687): \tTrain Loss  2.2297\n",
            "Epoch(3/5)\t Batch(400/1687): \tTrain Loss  2.2177\n",
            "Epoch(3/5)\t Batch(600/1687): \tTrain Loss  2.2169\n",
            "Epoch(3/5)\t Batch(800/1687): \tTrain Loss  2.2033\n",
            "Epoch(3/5)\t Batch(1000/1687): \tTrain Loss  2.1975\n",
            "Epoch(3/5)\t Batch(1200/1687): \tTrain Loss  2.2055\n",
            "Epoch(3/5)\t Batch(1400/1687): \tTrain Loss  2.1632\n",
            "Epoch(3/5)\t Batch(1600/1687): \tTrain Loss  2.1747\n",
            "End of Epoch: 3 Train Accuracy: 0.5471 Val Loss: 2.1795 Val Accuracy: 0.5373\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/1687): \tTrain Loss  2.1554\n",
            "Epoch(4/5)\t Batch(200/1687): \tTrain Loss  2.1637\n",
            "Epoch(4/5)\t Batch(400/1687): \tTrain Loss  2.1459\n",
            "Epoch(4/5)\t Batch(600/1687): \tTrain Loss  2.1668\n",
            "Epoch(4/5)\t Batch(800/1687): \tTrain Loss  2.1390\n",
            "Epoch(4/5)\t Batch(1000/1687): \tTrain Loss  2.1665\n",
            "Epoch(4/5)\t Batch(1200/1687): \tTrain Loss  2.1370\n",
            "Epoch(4/5)\t Batch(1400/1687): \tTrain Loss  2.1380\n",
            "Epoch(4/5)\t Batch(1600/1687): \tTrain Loss  2.1377\n",
            "End of Epoch: 4 Train Accuracy: 0.5617 Val Loss: 2.1354 Val Accuracy: 0.5555\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/1687): \tTrain Loss  2.1077\n",
            "Epoch(5/5)\t Batch(200/1687): \tTrain Loss  2.1305\n",
            "Epoch(5/5)\t Batch(400/1687): \tTrain Loss  2.0460\n",
            "Epoch(5/5)\t Batch(600/1687): \tTrain Loss  2.0935\n",
            "Epoch(5/5)\t Batch(800/1687): \tTrain Loss  2.0650\n",
            "Epoch(5/5)\t Batch(1000/1687): \tTrain Loss  2.1367\n",
            "Epoch(5/5)\t Batch(1200/1687): \tTrain Loss  2.1061\n",
            "Epoch(5/5)\t Batch(1400/1687): \tTrain Loss  2.0473\n",
            "Epoch(5/5)\t Batch(1600/1687): \tTrain Loss  2.1043\n",
            "End of Epoch: 5 Train Accuracy: 0.5855 Val Loss: 2.1001 Val Accuracy: 0.5807\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▅▇▇██</td></tr><tr><td>Train Loss</td><td>█▇▆▅▁</td></tr><tr><td>Val Accuracy</td><td>▁▅▇▇██</td></tr><tr><td>Val Loss</td><td>█▇▅▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.58552</td></tr><tr><td>Train Loss</td><td>2.0641</td></tr><tr><td>Val Accuracy</td><td>0.58067</td></tr><tr><td>Val Loss</td><td>2.10011</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_sgd|loss_cross_entropy|lr=0.001|batch_32|act_relu|hid_4|neurons_32|nrns_5|init_Xavier685</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/zwqmuzhm' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/zwqmuzhm</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_171757-zwqmuzhm/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k7l57kyi with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_171822-k7l57kyi</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/k7l57kyi' target=\"_blank\">good-sweep-33</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/k7l57kyi' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/k7l57kyi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.0484 Val Loss: 0.0903 Val Accuracy: 0.0515\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/1687): \tTrain Loss  0.0902\n",
            "Epoch(1/10)\t Batch(200/1687): \tTrain Loss  0.0902\n",
            "Epoch(1/10)\t Batch(400/1687): \tTrain Loss  0.0897\n",
            "Epoch(1/10)\t Batch(600/1687): \tTrain Loss  0.0903\n",
            "Epoch(1/10)\t Batch(800/1687): \tTrain Loss  0.0903\n",
            "Epoch(1/10)\t Batch(1000/1687): \tTrain Loss  0.0902\n",
            "Epoch(1/10)\t Batch(1200/1687): \tTrain Loss  0.0899\n",
            "Epoch(1/10)\t Batch(1400/1687): \tTrain Loss  0.0899\n",
            "Epoch(1/10)\t Batch(1600/1687): \tTrain Loss  0.0898\n",
            "End of Epoch: 1 Train Accuracy: 0.0688 Val Loss: 0.0901 Val Accuracy: 0.0707\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/1687): \tTrain Loss  0.0898\n",
            "Epoch(2/10)\t Batch(200/1687): \tTrain Loss  0.0901\n",
            "Epoch(2/10)\t Batch(400/1687): \tTrain Loss  0.0896\n",
            "Epoch(2/10)\t Batch(600/1687): \tTrain Loss  0.0898\n",
            "Epoch(2/10)\t Batch(800/1687): \tTrain Loss  0.0899\n",
            "Epoch(2/10)\t Batch(1000/1687): \tTrain Loss  0.0897\n",
            "Epoch(2/10)\t Batch(1200/1687): \tTrain Loss  0.0898\n",
            "Epoch(2/10)\t Batch(1400/1687): \tTrain Loss  0.0897\n",
            "Epoch(2/10)\t Batch(1600/1687): \tTrain Loss  0.0897\n",
            "End of Epoch: 2 Train Accuracy: 0.0918 Val Loss: 0.0899 Val Accuracy: 0.0958\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/1687): \tTrain Loss  0.0897\n",
            "Epoch(3/10)\t Batch(200/1687): \tTrain Loss  0.0897\n",
            "Epoch(3/10)\t Batch(400/1687): \tTrain Loss  0.0902\n",
            "Epoch(3/10)\t Batch(600/1687): \tTrain Loss  0.0899\n",
            "Epoch(3/10)\t Batch(800/1687): \tTrain Loss  0.0901\n",
            "Epoch(3/10)\t Batch(1000/1687): \tTrain Loss  0.0894\n",
            "Epoch(3/10)\t Batch(1200/1687): \tTrain Loss  0.0898\n",
            "Epoch(3/10)\t Batch(1400/1687): \tTrain Loss  0.0895\n",
            "Epoch(3/10)\t Batch(1600/1687): \tTrain Loss  0.0897\n",
            "End of Epoch: 3 Train Accuracy: 0.1116 Val Loss: 0.0897 Val Accuracy: 0.1132\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/1687): \tTrain Loss  0.0899\n",
            "Epoch(4/10)\t Batch(200/1687): \tTrain Loss  0.0897\n",
            "Epoch(4/10)\t Batch(400/1687): \tTrain Loss  0.0894\n",
            "Epoch(4/10)\t Batch(600/1687): \tTrain Loss  0.0897\n",
            "Epoch(4/10)\t Batch(800/1687): \tTrain Loss  0.0897\n",
            "Epoch(4/10)\t Batch(1000/1687): \tTrain Loss  0.0897\n",
            "Epoch(4/10)\t Batch(1200/1687): \tTrain Loss  0.0897\n",
            "Epoch(4/10)\t Batch(1400/1687): \tTrain Loss  0.0897\n",
            "Epoch(4/10)\t Batch(1600/1687): \tTrain Loss  0.0898\n",
            "End of Epoch: 4 Train Accuracy: 0.1378 Val Loss: 0.0896 Val Accuracy: 0.1432\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/1687): \tTrain Loss  0.0895\n",
            "Epoch(5/10)\t Batch(200/1687): \tTrain Loss  0.0897\n",
            "Epoch(5/10)\t Batch(400/1687): \tTrain Loss  0.0895\n",
            "Epoch(5/10)\t Batch(600/1687): \tTrain Loss  0.0897\n",
            "Epoch(5/10)\t Batch(800/1687): \tTrain Loss  0.0896\n",
            "Epoch(5/10)\t Batch(1000/1687): \tTrain Loss  0.0896\n",
            "Epoch(5/10)\t Batch(1200/1687): \tTrain Loss  0.0894\n",
            "Epoch(5/10)\t Batch(1400/1687): \tTrain Loss  0.0895\n",
            "Epoch(5/10)\t Batch(1600/1687): \tTrain Loss  0.0897\n",
            "End of Epoch: 5 Train Accuracy: 0.1719 Val Loss: 0.0894 Val Accuracy: 0.1733\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/1687): \tTrain Loss  0.0891\n",
            "Epoch(6/10)\t Batch(200/1687): \tTrain Loss  0.0896\n",
            "Epoch(6/10)\t Batch(400/1687): \tTrain Loss  0.0893\n",
            "Epoch(6/10)\t Batch(600/1687): \tTrain Loss  0.0891\n",
            "Epoch(6/10)\t Batch(800/1687): \tTrain Loss  0.0894\n",
            "Epoch(6/10)\t Batch(1000/1687): \tTrain Loss  0.0894\n",
            "Epoch(6/10)\t Batch(1200/1687): \tTrain Loss  0.0893\n",
            "Epoch(6/10)\t Batch(1400/1687): \tTrain Loss  0.0893\n",
            "Epoch(6/10)\t Batch(1600/1687): \tTrain Loss  0.0894\n",
            "End of Epoch: 6 Train Accuracy: 0.2102 Val Loss: 0.0892 Val Accuracy: 0.2113\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/1687): \tTrain Loss  0.0895\n",
            "Epoch(7/10)\t Batch(200/1687): \tTrain Loss  0.0892\n",
            "Epoch(7/10)\t Batch(400/1687): \tTrain Loss  0.0893\n",
            "Epoch(7/10)\t Batch(600/1687): \tTrain Loss  0.0890\n",
            "Epoch(7/10)\t Batch(800/1687): \tTrain Loss  0.0894\n",
            "Epoch(7/10)\t Batch(1000/1687): \tTrain Loss  0.0887\n",
            "Epoch(7/10)\t Batch(1200/1687): \tTrain Loss  0.0891\n",
            "Epoch(7/10)\t Batch(1400/1687): \tTrain Loss  0.0891\n",
            "Epoch(7/10)\t Batch(1600/1687): \tTrain Loss  0.0887\n",
            "End of Epoch: 7 Train Accuracy: 0.2473 Val Loss: 0.0891 Val Accuracy: 0.2508\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/1687): \tTrain Loss  0.0895\n",
            "Epoch(8/10)\t Batch(200/1687): \tTrain Loss  0.0890\n",
            "Epoch(8/10)\t Batch(400/1687): \tTrain Loss  0.0894\n",
            "Epoch(8/10)\t Batch(600/1687): \tTrain Loss  0.0891\n",
            "Epoch(8/10)\t Batch(800/1687): \tTrain Loss  0.0891\n",
            "Epoch(8/10)\t Batch(1000/1687): \tTrain Loss  0.0889\n",
            "Epoch(8/10)\t Batch(1200/1687): \tTrain Loss  0.0893\n",
            "Epoch(8/10)\t Batch(1400/1687): \tTrain Loss  0.0889\n",
            "Epoch(8/10)\t Batch(1600/1687): \tTrain Loss  0.0891\n",
            "End of Epoch: 8 Train Accuracy: 0.2818 Val Loss: 0.0889 Val Accuracy: 0.2832\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/1687): \tTrain Loss  0.0886\n",
            "Epoch(9/10)\t Batch(200/1687): \tTrain Loss  0.0888\n",
            "Epoch(9/10)\t Batch(400/1687): \tTrain Loss  0.0892\n",
            "Epoch(9/10)\t Batch(600/1687): \tTrain Loss  0.0886\n",
            "Epoch(9/10)\t Batch(800/1687): \tTrain Loss  0.0884\n",
            "Epoch(9/10)\t Batch(1000/1687): \tTrain Loss  0.0891\n",
            "Epoch(9/10)\t Batch(1200/1687): \tTrain Loss  0.0889\n",
            "Epoch(9/10)\t Batch(1400/1687): \tTrain Loss  0.0889\n",
            "Epoch(9/10)\t Batch(1600/1687): \tTrain Loss  0.0882\n",
            "End of Epoch: 9 Train Accuracy: 0.3091 Val Loss: 0.0887 Val Accuracy: 0.3137\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/1687): \tTrain Loss  0.0888\n",
            "Epoch(10/10)\t Batch(200/1687): \tTrain Loss  0.0884\n",
            "Epoch(10/10)\t Batch(400/1687): \tTrain Loss  0.0882\n",
            "Epoch(10/10)\t Batch(600/1687): \tTrain Loss  0.0887\n",
            "Epoch(10/10)\t Batch(800/1687): \tTrain Loss  0.0886\n",
            "Epoch(10/10)\t Batch(1000/1687): \tTrain Loss  0.0883\n",
            "Epoch(10/10)\t Batch(1200/1687): \tTrain Loss  0.0886\n",
            "Epoch(10/10)\t Batch(1400/1687): \tTrain Loss  0.0883\n",
            "Epoch(10/10)\t Batch(1600/1687): \tTrain Loss  0.0885\n",
            "End of Epoch: 10 Train Accuracy: 0.3303 Val Loss: 0.0885 Val Accuracy: 0.3370\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▃▃▄▅▆▇▇█</td></tr><tr><td>Train Loss</td><td>█▆▅▅▇▂▃▁▃▃</td></tr><tr><td>Val Accuracy</td><td>▁▁▂▃▃▄▅▆▇▇█</td></tr><tr><td>Val Loss</td><td>█▇▇▆▅▅▄▃▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.33033</td></tr><tr><td>Train Loss</td><td>0.08896</td></tr><tr><td>Val Accuracy</td><td>0.337</td></tr><tr><td>Val Loss</td><td>0.08849</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_sgd|loss_mse|lr=0.0001|batch_32|act_tanh|hid_5|neurons_128|nrns_10|init_Xavier703</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/k7l57kyi' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/k7l57kyi</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_171822-k7l57kyi/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: clula0x9 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_172114-clula0x9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/clula0x9' target=\"_blank\">toasty-sweep-34</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/clula0x9' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/clula0x9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.0991 Val Loss: 0.1733 Val Accuracy: 0.1107\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/1687): \tTrain Loss  0.1747\n",
            "Epoch(1/5)\t Batch(200/1687): \tTrain Loss  0.1179\n",
            "Epoch(1/5)\t Batch(400/1687): \tTrain Loss  0.0920\n",
            "Epoch(1/5)\t Batch(600/1687): \tTrain Loss  0.0992\n",
            "Epoch(1/5)\t Batch(800/1687): \tTrain Loss  0.0676\n",
            "Epoch(1/5)\t Batch(1000/1687): \tTrain Loss  0.0739\n",
            "Epoch(1/5)\t Batch(1200/1687): \tTrain Loss  0.0640\n",
            "Epoch(1/5)\t Batch(1400/1687): \tTrain Loss  0.0672\n",
            "Epoch(1/5)\t Batch(1600/1687): \tTrain Loss  0.0625\n",
            "End of Epoch: 1 Train Accuracy: 0.5827 Val Loss: 0.0578 Val Accuracy: 0.5717\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/1687): \tTrain Loss  0.0530\n",
            "Epoch(2/5)\t Batch(200/1687): \tTrain Loss  0.0782\n",
            "Epoch(2/5)\t Batch(400/1687): \tTrain Loss  0.0436\n",
            "Epoch(2/5)\t Batch(600/1687): \tTrain Loss  0.0398\n",
            "Epoch(2/5)\t Batch(800/1687): \tTrain Loss  0.0295\n",
            "Epoch(2/5)\t Batch(1000/1687): \tTrain Loss  0.0517\n",
            "Epoch(2/5)\t Batch(1200/1687): \tTrain Loss  0.0431\n",
            "Epoch(2/5)\t Batch(1400/1687): \tTrain Loss  0.0459\n",
            "Epoch(2/5)\t Batch(1600/1687): \tTrain Loss  0.0506\n",
            "End of Epoch: 2 Train Accuracy: 0.6900 Val Loss: 0.0440 Val Accuracy: 0.6768\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/1687): \tTrain Loss  0.0267\n",
            "Epoch(3/5)\t Batch(200/1687): \tTrain Loss  0.0444\n",
            "Epoch(3/5)\t Batch(400/1687): \tTrain Loss  0.0395\n",
            "Epoch(3/5)\t Batch(600/1687): \tTrain Loss  0.0633\n",
            "Epoch(3/5)\t Batch(800/1687): \tTrain Loss  0.0396\n",
            "Epoch(3/5)\t Batch(1000/1687): \tTrain Loss  0.0337\n",
            "Epoch(3/5)\t Batch(1200/1687): \tTrain Loss  0.0499\n",
            "Epoch(3/5)\t Batch(1400/1687): \tTrain Loss  0.0366\n",
            "Epoch(3/5)\t Batch(1600/1687): \tTrain Loss  0.0319\n",
            "End of Epoch: 3 Train Accuracy: 0.7185 Val Loss: 0.0399 Val Accuracy: 0.7083\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/1687): \tTrain Loss  0.0479\n",
            "Epoch(4/5)\t Batch(200/1687): \tTrain Loss  0.0407\n",
            "Epoch(4/5)\t Batch(400/1687): \tTrain Loss  0.0182\n",
            "Epoch(4/5)\t Batch(600/1687): \tTrain Loss  0.0288\n",
            "Epoch(4/5)\t Batch(800/1687): \tTrain Loss  0.0325\n",
            "Epoch(4/5)\t Batch(1000/1687): \tTrain Loss  0.0226\n",
            "Epoch(4/5)\t Batch(1200/1687): \tTrain Loss  0.0169\n",
            "Epoch(4/5)\t Batch(1400/1687): \tTrain Loss  0.0356\n",
            "Epoch(4/5)\t Batch(1600/1687): \tTrain Loss  0.0346\n",
            "End of Epoch: 4 Train Accuracy: 0.7330 Val Loss: 0.0380 Val Accuracy: 0.7237\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/1687): \tTrain Loss  0.0430\n",
            "Epoch(5/5)\t Batch(200/1687): \tTrain Loss  0.0409\n",
            "Epoch(5/5)\t Batch(400/1687): \tTrain Loss  0.0459\n",
            "Epoch(5/5)\t Batch(600/1687): \tTrain Loss  0.0505\n",
            "Epoch(5/5)\t Batch(800/1687): \tTrain Loss  0.0377\n",
            "Epoch(5/5)\t Batch(1000/1687): \tTrain Loss  0.0339\n",
            "Epoch(5/5)\t Batch(1200/1687): \tTrain Loss  0.0438\n",
            "Epoch(5/5)\t Batch(1400/1687): \tTrain Loss  0.0322\n",
            "Epoch(5/5)\t Batch(1600/1687): \tTrain Loss  0.0551\n",
            "End of Epoch: 5 Train Accuracy: 0.7434 Val Loss: 0.0367 Val Accuracy: 0.7335\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▆▇███</td></tr><tr><td>Train Loss</td><td>▆▁█▁▅</td></tr><tr><td>Val Accuracy</td><td>▁▆▇███</td></tr><tr><td>Val Loss</td><td>█▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.74341</td></tr><tr><td>Train Loss</td><td>0.04267</td></tr><tr><td>Val Accuracy</td><td>0.7335</td></tr><tr><td>Val Loss</td><td>0.03674</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nadam|loss_mse|lr=0.0001|batch_32|act_sigmoid|hid_3|neurons_64|nrns_5|init_Random358</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/clula0x9' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/clula0x9</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_172114-clula0x9/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nskmw8z2 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_172209-nskmw8z2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/nskmw8z2' target=\"_blank\">glad-sweep-35</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/nskmw8z2' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/nskmw8z2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.1077 Val Loss: 2.2971 Val Accuracy: 0.1025\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/1687): \tTrain Loss  2.2997\n",
            "Epoch(1/10)\t Batch(200/1687): \tTrain Loss  2.0940\n",
            "Epoch(1/10)\t Batch(400/1687): \tTrain Loss  1.9053\n",
            "Epoch(1/10)\t Batch(600/1687): \tTrain Loss  1.8733\n",
            "Epoch(1/10)\t Batch(800/1687): \tTrain Loss  1.8991\n",
            "Epoch(1/10)\t Batch(1000/1687): \tTrain Loss  1.8939\n",
            "Epoch(1/10)\t Batch(1200/1687): \tTrain Loss  1.7808\n",
            "Epoch(1/10)\t Batch(1400/1687): \tTrain Loss  1.7337\n",
            "Epoch(1/10)\t Batch(1600/1687): \tTrain Loss  1.7846\n",
            "End of Epoch: 1 Train Accuracy: 0.7947 Val Loss: 1.7755 Val Accuracy: 0.7992\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/1687): \tTrain Loss  1.7620\n",
            "Epoch(2/10)\t Batch(200/1687): \tTrain Loss  1.7877\n",
            "Epoch(2/10)\t Batch(400/1687): \tTrain Loss  1.7407\n",
            "Epoch(2/10)\t Batch(600/1687): \tTrain Loss  1.7596\n",
            "Epoch(2/10)\t Batch(800/1687): \tTrain Loss  1.8423\n",
            "Epoch(2/10)\t Batch(1000/1687): \tTrain Loss  1.7297\n",
            "Epoch(2/10)\t Batch(1200/1687): \tTrain Loss  1.6954\n",
            "Epoch(2/10)\t Batch(1400/1687): \tTrain Loss  1.7049\n",
            "Epoch(2/10)\t Batch(1600/1687): \tTrain Loss  1.8151\n",
            "End of Epoch: 2 Train Accuracy: 0.8297 Val Loss: 1.7268 Val Accuracy: 0.8313\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/1687): \tTrain Loss  1.7834\n",
            "Epoch(3/10)\t Batch(200/1687): \tTrain Loss  1.7174\n",
            "Epoch(3/10)\t Batch(400/1687): \tTrain Loss  1.7579\n",
            "Epoch(3/10)\t Batch(600/1687): \tTrain Loss  1.7051\n",
            "Epoch(3/10)\t Batch(800/1687): \tTrain Loss  1.7882\n",
            "Epoch(3/10)\t Batch(1000/1687): \tTrain Loss  1.7119\n",
            "Epoch(3/10)\t Batch(1200/1687): \tTrain Loss  1.8046\n",
            "Epoch(3/10)\t Batch(1400/1687): \tTrain Loss  1.7798\n",
            "Epoch(3/10)\t Batch(1600/1687): \tTrain Loss  1.6996\n",
            "End of Epoch: 3 Train Accuracy: 0.8388 Val Loss: 1.7070 Val Accuracy: 0.8378\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/1687): \tTrain Loss  1.7428\n",
            "Epoch(4/10)\t Batch(200/1687): \tTrain Loss  1.7176\n",
            "Epoch(4/10)\t Batch(400/1687): \tTrain Loss  1.7297\n",
            "Epoch(4/10)\t Batch(600/1687): \tTrain Loss  1.7185\n",
            "Epoch(4/10)\t Batch(800/1687): \tTrain Loss  1.6840\n",
            "Epoch(4/10)\t Batch(1000/1687): \tTrain Loss  1.7224\n",
            "Epoch(4/10)\t Batch(1200/1687): \tTrain Loss  1.6711\n",
            "Epoch(4/10)\t Batch(1400/1687): \tTrain Loss  1.7679\n",
            "Epoch(4/10)\t Batch(1600/1687): \tTrain Loss  1.7057\n",
            "End of Epoch: 4 Train Accuracy: 0.8467 Val Loss: 1.6986 Val Accuracy: 0.8440\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/1687): \tTrain Loss  1.7429\n",
            "Epoch(5/10)\t Batch(200/1687): \tTrain Loss  1.6849\n",
            "Epoch(5/10)\t Batch(400/1687): \tTrain Loss  1.7040\n",
            "Epoch(5/10)\t Batch(600/1687): \tTrain Loss  1.6745\n",
            "Epoch(5/10)\t Batch(800/1687): \tTrain Loss  1.7191\n",
            "Epoch(5/10)\t Batch(1000/1687): \tTrain Loss  1.6816\n",
            "Epoch(5/10)\t Batch(1200/1687): \tTrain Loss  1.7037\n",
            "Epoch(5/10)\t Batch(1400/1687): \tTrain Loss  1.7240\n",
            "Epoch(5/10)\t Batch(1600/1687): \tTrain Loss  1.7177\n",
            "End of Epoch: 5 Train Accuracy: 0.8528 Val Loss: 1.6833 Val Accuracy: 0.8498\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/1687): \tTrain Loss  1.6827\n",
            "Epoch(6/10)\t Batch(200/1687): \tTrain Loss  1.6698\n",
            "Epoch(6/10)\t Batch(400/1687): \tTrain Loss  1.5995\n",
            "Epoch(6/10)\t Batch(600/1687): \tTrain Loss  1.6741\n",
            "Epoch(6/10)\t Batch(800/1687): \tTrain Loss  1.6317\n",
            "Epoch(6/10)\t Batch(1000/1687): \tTrain Loss  1.6989\n",
            "Epoch(6/10)\t Batch(1200/1687): \tTrain Loss  1.6439\n",
            "Epoch(6/10)\t Batch(1400/1687): \tTrain Loss  1.7099\n",
            "Epoch(6/10)\t Batch(1600/1687): \tTrain Loss  1.7122\n",
            "End of Epoch: 6 Train Accuracy: 0.8570 Val Loss: 1.6833 Val Accuracy: 0.8562\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/1687): \tTrain Loss  1.6904\n",
            "Epoch(7/10)\t Batch(200/1687): \tTrain Loss  1.6865\n",
            "Epoch(7/10)\t Batch(400/1687): \tTrain Loss  1.6554\n",
            "Epoch(7/10)\t Batch(600/1687): \tTrain Loss  1.7251\n",
            "Epoch(7/10)\t Batch(800/1687): \tTrain Loss  1.6622\n",
            "Epoch(7/10)\t Batch(1000/1687): \tTrain Loss  1.6471\n",
            "Epoch(7/10)\t Batch(1200/1687): \tTrain Loss  1.6868\n",
            "Epoch(7/10)\t Batch(1400/1687): \tTrain Loss  1.6911\n",
            "Epoch(7/10)\t Batch(1600/1687): \tTrain Loss  1.6655\n",
            "End of Epoch: 7 Train Accuracy: 0.8613 Val Loss: 1.6700 Val Accuracy: 0.8580\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/1687): \tTrain Loss  1.6374\n",
            "Epoch(8/10)\t Batch(200/1687): \tTrain Loss  1.6423\n",
            "Epoch(8/10)\t Batch(400/1687): \tTrain Loss  1.6609\n",
            "Epoch(8/10)\t Batch(600/1687): \tTrain Loss  1.7008\n",
            "Epoch(8/10)\t Batch(800/1687): \tTrain Loss  1.6157\n",
            "Epoch(8/10)\t Batch(1000/1687): \tTrain Loss  1.6630\n",
            "Epoch(8/10)\t Batch(1200/1687): \tTrain Loss  1.6425\n",
            "Epoch(8/10)\t Batch(1400/1687): \tTrain Loss  1.6529\n",
            "Epoch(8/10)\t Batch(1600/1687): \tTrain Loss  1.7425\n",
            "End of Epoch: 8 Train Accuracy: 0.8613 Val Loss: 1.6648 Val Accuracy: 0.8577\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/1687): \tTrain Loss  1.6678\n",
            "Epoch(9/10)\t Batch(200/1687): \tTrain Loss  1.7253\n",
            "Epoch(9/10)\t Batch(400/1687): \tTrain Loss  1.6549\n",
            "Epoch(9/10)\t Batch(600/1687): \tTrain Loss  1.7040\n",
            "Epoch(9/10)\t Batch(800/1687): \tTrain Loss  1.5918\n",
            "Epoch(9/10)\t Batch(1000/1687): \tTrain Loss  1.6516\n",
            "Epoch(9/10)\t Batch(1200/1687): \tTrain Loss  1.6120\n",
            "Epoch(9/10)\t Batch(1400/1687): \tTrain Loss  1.6647\n",
            "Epoch(9/10)\t Batch(1600/1687): \tTrain Loss  1.6772\n",
            "End of Epoch: 9 Train Accuracy: 0.8641 Val Loss: 1.6654 Val Accuracy: 0.8588\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/1687): \tTrain Loss  1.7219\n",
            "Epoch(10/10)\t Batch(200/1687): \tTrain Loss  1.6841\n",
            "Epoch(10/10)\t Batch(400/1687): \tTrain Loss  1.6994\n",
            "Epoch(10/10)\t Batch(600/1687): \tTrain Loss  1.6534\n",
            "Epoch(10/10)\t Batch(800/1687): \tTrain Loss  1.5846\n",
            "Epoch(10/10)\t Batch(1000/1687): \tTrain Loss  1.7492\n",
            "Epoch(10/10)\t Batch(1200/1687): \tTrain Loss  1.6426\n",
            "Epoch(10/10)\t Batch(1400/1687): \tTrain Loss  1.6765\n",
            "Epoch(10/10)\t Batch(1600/1687): \tTrain Loss  1.6854\n",
            "End of Epoch: 10 Train Accuracy: 0.8685 Val Loss: 1.6628 Val Accuracy: 0.8612\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▇█████████</td></tr><tr><td>Train Loss</td><td>█▄▄▄▅▅▁▄▅▂</td></tr><tr><td>Val Accuracy</td><td>▁▇█████████</td></tr><tr><td>Val Loss</td><td>█▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.8685</td></tr><tr><td>Train Loss</td><td>1.67257</td></tr><tr><td>Val Accuracy</td><td>0.86117</td></tr><tr><td>Val Loss</td><td>1.66285</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_cross_entropy|lr=0.0001|batch_32|act_tanh|hid_4|neurons_32|nrns_10|init_Xavier180</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/nskmw8z2' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/nskmw8z2</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_172209-nskmw8z2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d5qpknvc with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_172304-d5qpknvc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/d5qpknvc' target=\"_blank\">winter-sweep-36</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/d5qpknvc' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/d5qpknvc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.0996 Val Loss: 0.0926 Val Accuracy: 0.1040\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/3375): \tTrain Loss  0.0952\n",
            "Epoch(1/5)\t Batch(200/3375): \tTrain Loss  0.0858\n",
            "Epoch(1/5)\t Batch(400/3375): \tTrain Loss  0.0692\n",
            "Epoch(1/5)\t Batch(600/3375): \tTrain Loss  0.0598\n",
            "Epoch(1/5)\t Batch(800/3375): \tTrain Loss  0.0288\n",
            "Epoch(1/5)\t Batch(1000/3375): \tTrain Loss  0.0117\n",
            "Epoch(1/5)\t Batch(1200/3375): \tTrain Loss  0.0376\n",
            "Epoch(1/5)\t Batch(1400/3375): \tTrain Loss  0.0339\n",
            "Epoch(1/5)\t Batch(1600/3375): \tTrain Loss  0.0450\n",
            "Epoch(1/5)\t Batch(1800/3375): \tTrain Loss  0.0200\n",
            "Epoch(1/5)\t Batch(2000/3375): \tTrain Loss  0.0296\n",
            "Epoch(1/5)\t Batch(2200/3375): \tTrain Loss  0.0332\n",
            "Epoch(1/5)\t Batch(2400/3375): \tTrain Loss  0.0280\n",
            "Epoch(1/5)\t Batch(2600/3375): \tTrain Loss  0.0378\n",
            "Epoch(1/5)\t Batch(2800/3375): \tTrain Loss  0.0323\n",
            "Epoch(1/5)\t Batch(3000/3375): \tTrain Loss  0.0318\n",
            "Epoch(1/5)\t Batch(3200/3375): \tTrain Loss  0.0283\n",
            "End of Epoch: 1 Train Accuracy: 0.8161 Val Loss: 0.0290 Val Accuracy: 0.8152\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/3375): \tTrain Loss  0.0452\n",
            "Epoch(2/5)\t Batch(200/3375): \tTrain Loss  0.0364\n",
            "Epoch(2/5)\t Batch(400/3375): \tTrain Loss  0.0242\n",
            "Epoch(2/5)\t Batch(600/3375): \tTrain Loss  0.0110\n",
            "Epoch(2/5)\t Batch(800/3375): \tTrain Loss  0.0213\n",
            "Epoch(2/5)\t Batch(1000/3375): \tTrain Loss  0.0305\n",
            "Epoch(2/5)\t Batch(1200/3375): \tTrain Loss  0.0331\n",
            "Epoch(2/5)\t Batch(1400/3375): \tTrain Loss  0.0295\n",
            "Epoch(2/5)\t Batch(1600/3375): \tTrain Loss  0.0183\n",
            "Epoch(2/5)\t Batch(1800/3375): \tTrain Loss  0.0137\n",
            "Epoch(2/5)\t Batch(2000/3375): \tTrain Loss  0.0271\n",
            "Epoch(2/5)\t Batch(2200/3375): \tTrain Loss  0.0354\n",
            "Epoch(2/5)\t Batch(2400/3375): \tTrain Loss  0.0282\n",
            "Epoch(2/5)\t Batch(2600/3375): \tTrain Loss  0.0369\n",
            "Epoch(2/5)\t Batch(2800/3375): \tTrain Loss  0.0146\n",
            "Epoch(2/5)\t Batch(3000/3375): \tTrain Loss  0.0290\n",
            "Epoch(2/5)\t Batch(3200/3375): \tTrain Loss  0.0327\n",
            "End of Epoch: 2 Train Accuracy: 0.8271 Val Loss: 0.0281 Val Accuracy: 0.8333\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/3375): \tTrain Loss  0.0434\n",
            "Epoch(3/5)\t Batch(200/3375): \tTrain Loss  0.0327\n",
            "Epoch(3/5)\t Batch(400/3375): \tTrain Loss  0.0222\n",
            "Epoch(3/5)\t Batch(600/3375): \tTrain Loss  0.0209\n",
            "Epoch(3/5)\t Batch(800/3375): \tTrain Loss  0.0288\n",
            "Epoch(3/5)\t Batch(1000/3375): \tTrain Loss  0.0227\n",
            "Epoch(3/5)\t Batch(1200/3375): \tTrain Loss  0.0318\n",
            "Epoch(3/5)\t Batch(1400/3375): \tTrain Loss  0.0353\n",
            "Epoch(3/5)\t Batch(1600/3375): \tTrain Loss  0.0307\n",
            "Epoch(3/5)\t Batch(1800/3375): \tTrain Loss  0.0530\n",
            "Epoch(3/5)\t Batch(2000/3375): \tTrain Loss  0.0299\n",
            "Epoch(3/5)\t Batch(2200/3375): \tTrain Loss  0.0174\n",
            "Epoch(3/5)\t Batch(2400/3375): \tTrain Loss  0.0116\n",
            "Epoch(3/5)\t Batch(2600/3375): \tTrain Loss  0.0393\n",
            "Epoch(3/5)\t Batch(2800/3375): \tTrain Loss  0.0323\n",
            "Epoch(3/5)\t Batch(3000/3375): \tTrain Loss  0.0386\n",
            "Epoch(3/5)\t Batch(3200/3375): \tTrain Loss  0.0320\n",
            "End of Epoch: 3 Train Accuracy: 0.8265 Val Loss: 0.0277 Val Accuracy: 0.8235\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/3375): \tTrain Loss  0.0170\n",
            "Epoch(4/5)\t Batch(200/3375): \tTrain Loss  0.0328\n",
            "Epoch(4/5)\t Batch(400/3375): \tTrain Loss  0.0193\n",
            "Epoch(4/5)\t Batch(600/3375): \tTrain Loss  0.0362\n",
            "Epoch(4/5)\t Batch(800/3375): \tTrain Loss  0.0287\n",
            "Epoch(4/5)\t Batch(1000/3375): \tTrain Loss  0.0324\n",
            "Epoch(4/5)\t Batch(1200/3375): \tTrain Loss  0.0288\n",
            "Epoch(4/5)\t Batch(1400/3375): \tTrain Loss  0.0277\n",
            "Epoch(4/5)\t Batch(1600/3375): \tTrain Loss  0.0212\n",
            "Epoch(4/5)\t Batch(1800/3375): \tTrain Loss  0.0351\n",
            "Epoch(4/5)\t Batch(2000/3375): \tTrain Loss  0.0146\n",
            "Epoch(4/5)\t Batch(2200/3375): \tTrain Loss  0.0379\n",
            "Epoch(4/5)\t Batch(2400/3375): \tTrain Loss  0.0262\n",
            "Epoch(4/5)\t Batch(2600/3375): \tTrain Loss  0.0172\n",
            "Epoch(4/5)\t Batch(2800/3375): \tTrain Loss  0.0139\n",
            "Epoch(4/5)\t Batch(3000/3375): \tTrain Loss  0.0294\n",
            "Epoch(4/5)\t Batch(3200/3375): \tTrain Loss  0.0317\n",
            "End of Epoch: 4 Train Accuracy: 0.8297 Val Loss: 0.0272 Val Accuracy: 0.8292\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/3375): \tTrain Loss  0.0313\n",
            "Epoch(5/5)\t Batch(200/3375): \tTrain Loss  0.0130\n",
            "Epoch(5/5)\t Batch(400/3375): \tTrain Loss  0.0210\n",
            "Epoch(5/5)\t Batch(600/3375): \tTrain Loss  0.0228\n",
            "Epoch(5/5)\t Batch(800/3375): \tTrain Loss  0.0419\n",
            "Epoch(5/5)\t Batch(1000/3375): \tTrain Loss  0.0319\n",
            "Epoch(5/5)\t Batch(1200/3375): \tTrain Loss  0.0232\n",
            "Epoch(5/5)\t Batch(1400/3375): \tTrain Loss  0.0187\n",
            "Epoch(5/5)\t Batch(1600/3375): \tTrain Loss  0.0203\n",
            "Epoch(5/5)\t Batch(1800/3375): \tTrain Loss  0.0308\n",
            "Epoch(5/5)\t Batch(2000/3375): \tTrain Loss  0.0394\n",
            "Epoch(5/5)\t Batch(2200/3375): \tTrain Loss  0.0449\n",
            "Epoch(5/5)\t Batch(2400/3375): \tTrain Loss  0.0285\n",
            "Epoch(5/5)\t Batch(2600/3375): \tTrain Loss  0.0426\n",
            "Epoch(5/5)\t Batch(2800/3375): \tTrain Loss  0.0216\n",
            "Epoch(5/5)\t Batch(3000/3375): \tTrain Loss  0.0293\n",
            "Epoch(5/5)\t Batch(3200/3375): \tTrain Loss  0.0311\n",
            "End of Epoch: 5 Train Accuracy: 0.8260 Val Loss: 0.0276 Val Accuracy: 0.8298\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁█████</td></tr><tr><td>Train Loss</td><td>█▃▂█▁</td></tr><tr><td>Val Accuracy</td><td>▁█████</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.82598</td></tr><tr><td>Train Loss</td><td>0.02091</td></tr><tr><td>Val Accuracy</td><td>0.82983</td></tr><tr><td>Val Loss</td><td>0.02762</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nadam|loss_mse|lr=0.001|batch_16|act_sigmoid|hid_3|neurons_32|nrns_5|init_Xavier427</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/d5qpknvc' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/d5qpknvc</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_172304-d5qpknvc/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8p36sarj with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nag\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_172405-8p36sarj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8p36sarj' target=\"_blank\">likely-sweep-37</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8p36sarj' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8p36sarj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.0863 Val Loss: 15.2489 Val Accuracy: 0.0885\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  13.9413\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  4.3066\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  2.6917\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  2.4369\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  2.4687\n",
            "End of Epoch: 1 Train Accuracy: 0.1059 Val Loss: 2.3915 Val Accuracy: 0.1062\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  2.4557\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  2.3110\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  2.4283\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  2.2917\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  2.2960\n",
            "End of Epoch: 2 Train Accuracy: 0.1039 Val Loss: 2.3342 Val Accuracy: 0.0985\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  2.3022\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  2.3172\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  2.4115\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  2.2865\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  2.3063\n",
            "End of Epoch: 3 Train Accuracy: 0.1051 Val Loss: 2.3196 Val Accuracy: 0.1052\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  2.3021\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  2.3015\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  2.3127\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  2.2990\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  2.2884\n",
            "End of Epoch: 4 Train Accuracy: 0.1031 Val Loss: 2.3172 Val Accuracy: 0.1038\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  2.3692\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  2.3163\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  2.2984\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  2.4335\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  2.3771\n",
            "End of Epoch: 5 Train Accuracy: 0.0958 Val Loss: 2.3115 Val Accuracy: 0.0958\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  2.3043\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  2.3323\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  2.2939\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  2.3004\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  2.3028\n",
            "End of Epoch: 6 Train Accuracy: 0.1035 Val Loss: 2.3093 Val Accuracy: 0.0948\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  2.3078\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  2.3017\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  2.3021\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  2.3998\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  2.3041\n",
            "End of Epoch: 7 Train Accuracy: 0.1053 Val Loss: 2.3074 Val Accuracy: 0.1013\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  2.3047\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  2.3028\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  2.2999\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  2.3033\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  2.3014\n",
            "End of Epoch: 8 Train Accuracy: 0.0870 Val Loss: 2.3059 Val Accuracy: 0.0910\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  2.3013\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  2.3032\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  2.3021\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  2.3005\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  2.2935\n",
            "End of Epoch: 9 Train Accuracy: 0.0788 Val Loss: 2.3052 Val Accuracy: 0.0782\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  2.2960\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  2.3043\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  2.3041\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  2.3019\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  2.3014\n",
            "End of Epoch: 10 Train Accuracy: 0.0768 Val Loss: 2.3044 Val Accuracy: 0.0745\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▃███▇▆▇█▃▁▁</td></tr><tr><td>Train Loss</td><td>▁▄▅▅█▅▄▅▅▅</td></tr><tr><td>Val Accuracy</td><td>▄█▆█▇▆▅▇▅▂▁</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.07681</td></tr><tr><td>Train Loss</td><td>2.30267</td></tr><tr><td>Val Accuracy</td><td>0.0745</td></tr><tr><td>Val Loss</td><td>2.30442</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nag|loss_cross_entropy|lr=0.0001|batch_64|act_tanh|hid_5|neurons_128|nrns_10|init_Random153</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8p36sarj' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8p36sarj</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_172405-8p36sarj/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: snwjarig with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: momentum\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_172626-snwjarig</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/snwjarig' target=\"_blank\">unique-sweep-38</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/snwjarig' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/snwjarig</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.1066 Val Loss: 12.4909 Val Accuracy: 0.1108\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/1687): \tTrain Loss  13.0618\n",
            "Epoch(1/10)\t Batch(200/1687): \tTrain Loss  2.5345\n",
            "Epoch(1/10)\t Batch(400/1687): \tTrain Loss  2.2914\n",
            "Epoch(1/10)\t Batch(600/1687): \tTrain Loss  2.4183\n",
            "Epoch(1/10)\t Batch(800/1687): \tTrain Loss  2.3366\n",
            "Epoch(1/10)\t Batch(1000/1687): \tTrain Loss  2.3510\n",
            "Epoch(1/10)\t Batch(1200/1687): \tTrain Loss  2.2916\n",
            "Epoch(1/10)\t Batch(1400/1687): \tTrain Loss  2.3204\n",
            "Epoch(1/10)\t Batch(1600/1687): \tTrain Loss  2.2637\n",
            "End of Epoch: 1 Train Accuracy: 0.1224 Val Loss: 2.2945 Val Accuracy: 0.1225\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/1687): \tTrain Loss  2.2616\n",
            "Epoch(2/10)\t Batch(200/1687): \tTrain Loss  2.2651\n",
            "Epoch(2/10)\t Batch(400/1687): \tTrain Loss  2.2943\n",
            "Epoch(2/10)\t Batch(600/1687): \tTrain Loss  2.2902\n",
            "Epoch(2/10)\t Batch(800/1687): \tTrain Loss  2.2962\n",
            "Epoch(2/10)\t Batch(1000/1687): \tTrain Loss  2.3012\n",
            "Epoch(2/10)\t Batch(1200/1687): \tTrain Loss  2.3063\n",
            "Epoch(2/10)\t Batch(1400/1687): \tTrain Loss  2.2772\n",
            "Epoch(2/10)\t Batch(1600/1687): \tTrain Loss  2.2873\n",
            "End of Epoch: 2 Train Accuracy: 0.1471 Val Loss: 2.2883 Val Accuracy: 0.1453\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/1687): \tTrain Loss  2.2728\n",
            "Epoch(3/10)\t Batch(200/1687): \tTrain Loss  2.3260\n",
            "Epoch(3/10)\t Batch(400/1687): \tTrain Loss  2.2782\n",
            "Epoch(3/10)\t Batch(600/1687): \tTrain Loss  2.3165\n",
            "Epoch(3/10)\t Batch(800/1687): \tTrain Loss  2.2784\n",
            "Epoch(3/10)\t Batch(1000/1687): \tTrain Loss  2.2792\n",
            "Epoch(3/10)\t Batch(1200/1687): \tTrain Loss  2.2736\n",
            "Epoch(3/10)\t Batch(1400/1687): \tTrain Loss  2.3143\n",
            "Epoch(3/10)\t Batch(1600/1687): \tTrain Loss  2.2774\n",
            "End of Epoch: 3 Train Accuracy: 0.1571 Val Loss: 2.2859 Val Accuracy: 0.1552\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/1687): \tTrain Loss  2.2996\n",
            "Epoch(4/10)\t Batch(200/1687): \tTrain Loss  2.3020\n",
            "Epoch(4/10)\t Batch(400/1687): \tTrain Loss  2.2833\n",
            "Epoch(4/10)\t Batch(600/1687): \tTrain Loss  2.3008\n",
            "Epoch(4/10)\t Batch(800/1687): \tTrain Loss  2.2599\n",
            "Epoch(4/10)\t Batch(1000/1687): \tTrain Loss  2.3019\n",
            "Epoch(4/10)\t Batch(1200/1687): \tTrain Loss  2.2693\n",
            "Epoch(4/10)\t Batch(1400/1687): \tTrain Loss  2.2797\n",
            "Epoch(4/10)\t Batch(1600/1687): \tTrain Loss  2.2677\n",
            "End of Epoch: 4 Train Accuracy: 0.1567 Val Loss: 2.2841 Val Accuracy: 0.1515\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/1687): \tTrain Loss  2.2900\n",
            "Epoch(5/10)\t Batch(200/1687): \tTrain Loss  2.2765\n",
            "Epoch(5/10)\t Batch(400/1687): \tTrain Loss  2.2921\n",
            "Epoch(5/10)\t Batch(600/1687): \tTrain Loss  2.3024\n",
            "Epoch(5/10)\t Batch(800/1687): \tTrain Loss  2.2759\n",
            "Epoch(5/10)\t Batch(1000/1687): \tTrain Loss  2.2877\n",
            "Epoch(5/10)\t Batch(1200/1687): \tTrain Loss  2.2813\n",
            "Epoch(5/10)\t Batch(1400/1687): \tTrain Loss  2.2887\n",
            "Epoch(5/10)\t Batch(1600/1687): \tTrain Loss  2.2667\n",
            "End of Epoch: 5 Train Accuracy: 0.1819 Val Loss: 2.2819 Val Accuracy: 0.1813\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/1687): \tTrain Loss  2.2923\n",
            "Epoch(6/10)\t Batch(200/1687): \tTrain Loss  2.2738\n",
            "Epoch(6/10)\t Batch(400/1687): \tTrain Loss  2.2933\n",
            "Epoch(6/10)\t Batch(600/1687): \tTrain Loss  2.2785\n",
            "Epoch(6/10)\t Batch(800/1687): \tTrain Loss  2.2847\n",
            "Epoch(6/10)\t Batch(1000/1687): \tTrain Loss  2.2913\n",
            "Epoch(6/10)\t Batch(1200/1687): \tTrain Loss  2.2894\n",
            "Epoch(6/10)\t Batch(1400/1687): \tTrain Loss  2.2948\n",
            "Epoch(6/10)\t Batch(1600/1687): \tTrain Loss  2.2800\n",
            "End of Epoch: 6 Train Accuracy: 0.1989 Val Loss: 2.2796 Val Accuracy: 0.1942\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/1687): \tTrain Loss  2.2742\n",
            "Epoch(7/10)\t Batch(200/1687): \tTrain Loss  2.2723\n",
            "Epoch(7/10)\t Batch(400/1687): \tTrain Loss  2.2871\n",
            "Epoch(7/10)\t Batch(600/1687): \tTrain Loss  2.2694\n",
            "Epoch(7/10)\t Batch(800/1687): \tTrain Loss  2.2753\n",
            "Epoch(7/10)\t Batch(1000/1687): \tTrain Loss  2.2867\n",
            "Epoch(7/10)\t Batch(1200/1687): \tTrain Loss  2.2951\n",
            "Epoch(7/10)\t Batch(1400/1687): \tTrain Loss  2.2701\n",
            "Epoch(7/10)\t Batch(1600/1687): \tTrain Loss  2.2884\n",
            "End of Epoch: 7 Train Accuracy: 0.2229 Val Loss: 2.2774 Val Accuracy: 0.2287\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/1687): \tTrain Loss  2.2766\n",
            "Epoch(8/10)\t Batch(200/1687): \tTrain Loss  2.2780\n",
            "Epoch(8/10)\t Batch(400/1687): \tTrain Loss  2.2815\n",
            "Epoch(8/10)\t Batch(600/1687): \tTrain Loss  2.2851\n",
            "Epoch(8/10)\t Batch(800/1687): \tTrain Loss  2.2694\n",
            "Epoch(8/10)\t Batch(1000/1687): \tTrain Loss  2.2664\n",
            "Epoch(8/10)\t Batch(1200/1687): \tTrain Loss  2.2661\n",
            "Epoch(8/10)\t Batch(1400/1687): \tTrain Loss  2.2638\n",
            "Epoch(8/10)\t Batch(1600/1687): \tTrain Loss  2.2819\n",
            "End of Epoch: 8 Train Accuracy: 0.2302 Val Loss: 2.2748 Val Accuracy: 0.2330\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/1687): \tTrain Loss  2.2712\n",
            "Epoch(9/10)\t Batch(200/1687): \tTrain Loss  2.2960\n",
            "Epoch(9/10)\t Batch(400/1687): \tTrain Loss  2.2699\n",
            "Epoch(9/10)\t Batch(600/1687): \tTrain Loss  2.2803\n",
            "Epoch(9/10)\t Batch(800/1687): \tTrain Loss  2.2550\n",
            "Epoch(9/10)\t Batch(1000/1687): \tTrain Loss  2.2617\n",
            "Epoch(9/10)\t Batch(1200/1687): \tTrain Loss  2.2770\n",
            "Epoch(9/10)\t Batch(1400/1687): \tTrain Loss  2.2565\n",
            "Epoch(9/10)\t Batch(1600/1687): \tTrain Loss  2.2868\n",
            "End of Epoch: 9 Train Accuracy: 0.2401 Val Loss: 2.2723 Val Accuracy: 0.2420\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/1687): \tTrain Loss  2.2500\n",
            "Epoch(10/10)\t Batch(200/1687): \tTrain Loss  2.2795\n",
            "Epoch(10/10)\t Batch(400/1687): \tTrain Loss  2.2828\n",
            "Epoch(10/10)\t Batch(600/1687): \tTrain Loss  2.2659\n",
            "Epoch(10/10)\t Batch(800/1687): \tTrain Loss  2.2761\n",
            "Epoch(10/10)\t Batch(1000/1687): \tTrain Loss  2.2837\n",
            "Epoch(10/10)\t Batch(1200/1687): \tTrain Loss  2.2797\n",
            "Epoch(10/10)\t Batch(1400/1687): \tTrain Loss  2.2873\n",
            "Epoch(10/10)\t Batch(1600/1687): \tTrain Loss  2.2648\n",
            "End of Epoch: 10 Train Accuracy: 0.2344 Val Loss: 2.2694 Val Accuracy: 0.2380\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▄▄▅▆▇▇██</td></tr><tr><td>Train Loss</td><td>▁█▃█▅▄▃▃▂▂</td></tr><tr><td>Val Accuracy</td><td>▁▂▃▃▃▅▅▇███</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.23443</td></tr><tr><td>Train Loss</td><td>2.2579</td></tr><tr><td>Val Accuracy</td><td>0.238</td></tr><tr><td>Val Loss</td><td>2.26943</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_momentum|loss_cross_entropy|lr=0.0001|batch_32|act_sigmoid|hid_5|neurons_128|nrns_10|init_Random913</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/snwjarig' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/snwjarig</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_172626-snwjarig/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tq9g4zia with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_172827-tq9g4zia</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/tq9g4zia' target=\"_blank\">effortless-sweep-39</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/tq9g4zia' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/tq9g4zia</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.1036 Val Loss: 0.1441 Val Accuracy: 0.1103\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  0.1487\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  0.1582\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  0.1478\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  0.1419\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  0.1461\n",
            "End of Epoch: 1 Train Accuracy: 0.1059 Val Loss: 0.1435 Val Accuracy: 0.1117\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  0.1520\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  0.1315\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  0.1455\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  0.1232\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  0.1344\n",
            "End of Epoch: 2 Train Accuracy: 0.1079 Val Loss: 0.1430 Val Accuracy: 0.1145\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  0.1455\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  0.1462\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  0.1421\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  0.1404\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  0.1506\n",
            "End of Epoch: 3 Train Accuracy: 0.1095 Val Loss: 0.1426 Val Accuracy: 0.1143\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  0.1475\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  0.1438\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  0.1512\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  0.1563\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  0.1409\n",
            "End of Epoch: 4 Train Accuracy: 0.1109 Val Loss: 0.1423 Val Accuracy: 0.1165\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  0.1450\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  0.1463\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  0.1432\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  0.1472\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  0.1367\n",
            "End of Epoch: 5 Train Accuracy: 0.1120 Val Loss: 0.1420 Val Accuracy: 0.1180\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  0.1452\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  0.1552\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  0.1429\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  0.1400\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  0.1487\n",
            "End of Epoch: 6 Train Accuracy: 0.1133 Val Loss: 0.1417 Val Accuracy: 0.1180\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  0.1390\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  0.1366\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  0.1375\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  0.1488\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  0.1495\n",
            "End of Epoch: 7 Train Accuracy: 0.1148 Val Loss: 0.1413 Val Accuracy: 0.1177\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  0.1387\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  0.1356\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  0.1303\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  0.1557\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  0.1424\n",
            "End of Epoch: 8 Train Accuracy: 0.1160 Val Loss: 0.1410 Val Accuracy: 0.1195\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  0.1338\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  0.1443\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  0.1368\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  0.1318\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  0.1369\n",
            "End of Epoch: 9 Train Accuracy: 0.1171 Val Loss: 0.1407 Val Accuracy: 0.1205\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  0.1453\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  0.1501\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  0.1346\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  0.1472\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  0.1369\n",
            "End of Epoch: 10 Train Accuracy: 0.1183 Val Loss: 0.1404 Val Accuracy: 0.1218\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▄▄▅▆▆▇▇█</td></tr><tr><td>Train Loss</td><td>█▅▆█▅▅▃▁▃▅</td></tr><tr><td>Val Accuracy</td><td>▁▂▄▃▅▆▆▅▇▇█</td></tr><tr><td>Val Loss</td><td>█▇▆▅▅▄▃▃▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.11833</td></tr><tr><td>Train Loss</td><td>0.14643</td></tr><tr><td>Val Accuracy</td><td>0.12183</td></tr><tr><td>Val Loss</td><td>0.14036</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_sgd|loss_mse|lr=0.0001|batch_64|act_tanh|hid_5|neurons_32|nrns_10|init_Random753</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/tq9g4zia' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/tq9g4zia</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_172827-tq9g4zia/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: aofzxy75 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nag\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_172943-aofzxy75</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/aofzxy75' target=\"_blank\">radiant-sweep-40</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/aofzxy75' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/aofzxy75</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0560 Val Loss: 2.2992 Val Accuracy: 0.0607\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/843): \tTrain Loss  2.3056\n",
            "Epoch(1/5)\t Batch(200/843): \tTrain Loss  2.2711\n",
            "Epoch(1/5)\t Batch(400/843): \tTrain Loss  2.2290\n",
            "Epoch(1/5)\t Batch(600/843): \tTrain Loss  2.1828\n",
            "Epoch(1/5)\t Batch(800/843): \tTrain Loss  2.1459\n",
            "End of Epoch: 1 Train Accuracy: 0.5579 Val Loss: 2.1276 Val Accuracy: 0.5543\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/843): \tTrain Loss  2.1152\n",
            "Epoch(2/5)\t Batch(200/843): \tTrain Loss  2.1125\n",
            "Epoch(2/5)\t Batch(400/843): \tTrain Loss  2.0798\n",
            "Epoch(2/5)\t Batch(600/843): \tTrain Loss  2.0463\n",
            "Epoch(2/5)\t Batch(800/843): \tTrain Loss  2.0388\n",
            "End of Epoch: 2 Train Accuracy: 0.6063 Val Loss: 1.9899 Val Accuracy: 0.6067\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/843): \tTrain Loss  2.0013\n",
            "Epoch(3/5)\t Batch(200/843): \tTrain Loss  1.9531\n",
            "Epoch(3/5)\t Batch(400/843): \tTrain Loss  1.9704\n",
            "Epoch(3/5)\t Batch(600/843): \tTrain Loss  1.9170\n",
            "Epoch(3/5)\t Batch(800/843): \tTrain Loss  1.9405\n",
            "End of Epoch: 3 Train Accuracy: 0.6993 Val Loss: 1.9249 Val Accuracy: 0.6950\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/843): \tTrain Loss  1.9616\n",
            "Epoch(4/5)\t Batch(200/843): \tTrain Loss  1.9266\n",
            "Epoch(4/5)\t Batch(400/843): \tTrain Loss  1.9276\n",
            "Epoch(4/5)\t Batch(600/843): \tTrain Loss  1.8603\n",
            "Epoch(4/5)\t Batch(800/843): \tTrain Loss  1.8690\n",
            "End of Epoch: 4 Train Accuracy: 0.7347 Val Loss: 1.8781 Val Accuracy: 0.7377\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/843): \tTrain Loss  1.8827\n",
            "Epoch(5/5)\t Batch(200/843): \tTrain Loss  1.8769\n",
            "Epoch(5/5)\t Batch(400/843): \tTrain Loss  1.8756\n",
            "Epoch(5/5)\t Batch(600/843): \tTrain Loss  1.8745\n",
            "Epoch(5/5)\t Batch(800/843): \tTrain Loss  1.8818\n",
            "End of Epoch: 5 Train Accuracy: 0.7500 Val Loss: 1.8500 Val Accuracy: 0.7537\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▆▇▇██</td></tr><tr><td>Train Loss</td><td>█▄▂▃▁</td></tr><tr><td>Val Accuracy</td><td>▁▆▇▇██</td></tr><tr><td>Val Loss</td><td>█▅▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.75</td></tr><tr><td>Train Loss</td><td>1.87613</td></tr><tr><td>Val Accuracy</td><td>0.75367</td></tr><tr><td>Val Loss</td><td>1.84997</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nag|loss_cross_entropy|lr=0.001|batch_64|act_relu|hid_4|neurons_32|nrns_5|init_Xavier635</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/aofzxy75' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/aofzxy75</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_172943-aofzxy75/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x66xxbbk with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nag\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_173003-x66xxbbk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/x66xxbbk' target=\"_blank\">radiant-sweep-41</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/x66xxbbk' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/x66xxbbk</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.0990 Val Loss: 0.0905 Val Accuracy: 0.1025\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/3375): \tTrain Loss  0.0908\n",
            "Epoch(1/10)\t Batch(200/3375): \tTrain Loss  0.0898\n",
            "Epoch(1/10)\t Batch(400/3375): \tTrain Loss  0.0898\n",
            "Epoch(1/10)\t Batch(600/3375): \tTrain Loss  0.0903\n",
            "Epoch(1/10)\t Batch(800/3375): \tTrain Loss  0.0901\n",
            "Epoch(1/10)\t Batch(1000/3375): \tTrain Loss  0.0899\n",
            "Epoch(1/10)\t Batch(1200/3375): \tTrain Loss  0.0892\n",
            "Epoch(1/10)\t Batch(1400/3375): \tTrain Loss  0.0889\n",
            "Epoch(1/10)\t Batch(1600/3375): \tTrain Loss  0.0883\n",
            "Epoch(1/10)\t Batch(1800/3375): \tTrain Loss  0.0891\n",
            "Epoch(1/10)\t Batch(2000/3375): \tTrain Loss  0.0885\n",
            "Epoch(1/10)\t Batch(2200/3375): \tTrain Loss  0.0886\n",
            "Epoch(1/10)\t Batch(2400/3375): \tTrain Loss  0.0871\n",
            "Epoch(1/10)\t Batch(2600/3375): \tTrain Loss  0.0874\n",
            "Epoch(1/10)\t Batch(2800/3375): \tTrain Loss  0.0863\n",
            "Epoch(1/10)\t Batch(3000/3375): \tTrain Loss  0.0870\n",
            "Epoch(1/10)\t Batch(3200/3375): \tTrain Loss  0.0855\n",
            "End of Epoch: 1 Train Accuracy: 0.3259 Val Loss: 0.0862 Val Accuracy: 0.3240\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/3375): \tTrain Loss  0.0868\n",
            "Epoch(2/10)\t Batch(200/3375): \tTrain Loss  0.0852\n",
            "Epoch(2/10)\t Batch(400/3375): \tTrain Loss  0.0855\n",
            "Epoch(2/10)\t Batch(600/3375): \tTrain Loss  0.0839\n",
            "Epoch(2/10)\t Batch(800/3375): \tTrain Loss  0.0836\n",
            "Epoch(2/10)\t Batch(1000/3375): \tTrain Loss  0.0841\n",
            "Epoch(2/10)\t Batch(1200/3375): \tTrain Loss  0.0853\n",
            "Epoch(2/10)\t Batch(1400/3375): \tTrain Loss  0.0837\n",
            "Epoch(2/10)\t Batch(1600/3375): \tTrain Loss  0.0840\n",
            "Epoch(2/10)\t Batch(1800/3375): \tTrain Loss  0.0834\n",
            "Epoch(2/10)\t Batch(2000/3375): \tTrain Loss  0.0822\n",
            "Epoch(2/10)\t Batch(2200/3375): \tTrain Loss  0.0845\n",
            "Epoch(2/10)\t Batch(2400/3375): \tTrain Loss  0.0824\n",
            "Epoch(2/10)\t Batch(2600/3375): \tTrain Loss  0.0797\n",
            "Epoch(2/10)\t Batch(2800/3375): \tTrain Loss  0.0759\n",
            "Epoch(2/10)\t Batch(3000/3375): \tTrain Loss  0.0830\n",
            "Epoch(2/10)\t Batch(3200/3375): \tTrain Loss  0.0814\n",
            "End of Epoch: 2 Train Accuracy: 0.4970 Val Loss: 0.0799 Val Accuracy: 0.4912\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/3375): \tTrain Loss  0.0822\n",
            "Epoch(3/10)\t Batch(200/3375): \tTrain Loss  0.0789\n",
            "Epoch(3/10)\t Batch(400/3375): \tTrain Loss  0.0781\n",
            "Epoch(3/10)\t Batch(600/3375): \tTrain Loss  0.0812\n",
            "Epoch(3/10)\t Batch(800/3375): \tTrain Loss  0.0719\n",
            "Epoch(3/10)\t Batch(1000/3375): \tTrain Loss  0.0767\n",
            "Epoch(3/10)\t Batch(1200/3375): \tTrain Loss  0.0771\n",
            "Epoch(3/10)\t Batch(1400/3375): \tTrain Loss  0.0777\n",
            "Epoch(3/10)\t Batch(1600/3375): \tTrain Loss  0.0759\n",
            "Epoch(3/10)\t Batch(1800/3375): \tTrain Loss  0.0744\n",
            "Epoch(3/10)\t Batch(2000/3375): \tTrain Loss  0.0760\n",
            "Epoch(3/10)\t Batch(2200/3375): \tTrain Loss  0.0829\n",
            "Epoch(3/10)\t Batch(2400/3375): \tTrain Loss  0.0706\n",
            "Epoch(3/10)\t Batch(2600/3375): \tTrain Loss  0.0698\n",
            "Epoch(3/10)\t Batch(2800/3375): \tTrain Loss  0.0719\n",
            "Epoch(3/10)\t Batch(3000/3375): \tTrain Loss  0.0665\n",
            "Epoch(3/10)\t Batch(3200/3375): \tTrain Loss  0.0706\n",
            "End of Epoch: 3 Train Accuracy: 0.5464 Val Loss: 0.0723 Val Accuracy: 0.5383\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/3375): \tTrain Loss  0.0736\n",
            "Epoch(4/10)\t Batch(200/3375): \tTrain Loss  0.0800\n",
            "Epoch(4/10)\t Batch(400/3375): \tTrain Loss  0.0699\n",
            "Epoch(4/10)\t Batch(600/3375): \tTrain Loss  0.0702\n",
            "Epoch(4/10)\t Batch(800/3375): \tTrain Loss  0.0715\n",
            "Epoch(4/10)\t Batch(1000/3375): \tTrain Loss  0.0717\n",
            "Epoch(4/10)\t Batch(1200/3375): \tTrain Loss  0.0709\n",
            "Epoch(4/10)\t Batch(1400/3375): \tTrain Loss  0.0690\n",
            "Epoch(4/10)\t Batch(1600/3375): \tTrain Loss  0.0660\n",
            "Epoch(4/10)\t Batch(1800/3375): \tTrain Loss  0.0598\n",
            "Epoch(4/10)\t Batch(2000/3375): \tTrain Loss  0.0675\n",
            "Epoch(4/10)\t Batch(2200/3375): \tTrain Loss  0.0734\n",
            "Epoch(4/10)\t Batch(2400/3375): \tTrain Loss  0.0675\n",
            "Epoch(4/10)\t Batch(2600/3375): \tTrain Loss  0.0588\n",
            "Epoch(4/10)\t Batch(2800/3375): \tTrain Loss  0.0662\n",
            "Epoch(4/10)\t Batch(3000/3375): \tTrain Loss  0.0600\n",
            "Epoch(4/10)\t Batch(3200/3375): \tTrain Loss  0.0626\n",
            "End of Epoch: 4 Train Accuracy: 0.5671 Val Loss: 0.0635 Val Accuracy: 0.5615\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/3375): \tTrain Loss  0.0647\n",
            "Epoch(5/10)\t Batch(200/3375): \tTrain Loss  0.0695\n",
            "Epoch(5/10)\t Batch(400/3375): \tTrain Loss  0.0725\n",
            "Epoch(5/10)\t Batch(600/3375): \tTrain Loss  0.0621\n",
            "Epoch(5/10)\t Batch(800/3375): \tTrain Loss  0.0557\n",
            "Epoch(5/10)\t Batch(1000/3375): \tTrain Loss  0.0676\n",
            "Epoch(5/10)\t Batch(1200/3375): \tTrain Loss  0.0601\n",
            "Epoch(5/10)\t Batch(1400/3375): \tTrain Loss  0.0567\n",
            "Epoch(5/10)\t Batch(1600/3375): \tTrain Loss  0.0466\n",
            "Epoch(5/10)\t Batch(1800/3375): \tTrain Loss  0.0588\n",
            "Epoch(5/10)\t Batch(2000/3375): \tTrain Loss  0.0562\n",
            "Epoch(5/10)\t Batch(2200/3375): \tTrain Loss  0.0533\n",
            "Epoch(5/10)\t Batch(2400/3375): \tTrain Loss  0.0583\n",
            "Epoch(5/10)\t Batch(2600/3375): \tTrain Loss  0.0480\n",
            "Epoch(5/10)\t Batch(2800/3375): \tTrain Loss  0.0699\n",
            "Epoch(5/10)\t Batch(3000/3375): \tTrain Loss  0.0501\n",
            "Epoch(5/10)\t Batch(3200/3375): \tTrain Loss  0.0504\n",
            "End of Epoch: 5 Train Accuracy: 0.6063 Val Loss: 0.0565 Val Accuracy: 0.5977\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/3375): \tTrain Loss  0.0521\n",
            "Epoch(6/10)\t Batch(200/3375): \tTrain Loss  0.0549\n",
            "Epoch(6/10)\t Batch(400/3375): \tTrain Loss  0.0378\n",
            "Epoch(6/10)\t Batch(600/3375): \tTrain Loss  0.0492\n",
            "Epoch(6/10)\t Batch(800/3375): \tTrain Loss  0.0670\n",
            "Epoch(6/10)\t Batch(1000/3375): \tTrain Loss  0.0366\n",
            "Epoch(6/10)\t Batch(1200/3375): \tTrain Loss  0.0593\n",
            "Epoch(6/10)\t Batch(1400/3375): \tTrain Loss  0.0387\n",
            "Epoch(6/10)\t Batch(1600/3375): \tTrain Loss  0.0462\n",
            "Epoch(6/10)\t Batch(1800/3375): \tTrain Loss  0.0515\n",
            "Epoch(6/10)\t Batch(2000/3375): \tTrain Loss  0.0505\n",
            "Epoch(6/10)\t Batch(2200/3375): \tTrain Loss  0.0545\n",
            "Epoch(6/10)\t Batch(2400/3375): \tTrain Loss  0.0512\n",
            "Epoch(6/10)\t Batch(2600/3375): \tTrain Loss  0.0473\n",
            "Epoch(6/10)\t Batch(2800/3375): \tTrain Loss  0.0504\n",
            "Epoch(6/10)\t Batch(3000/3375): \tTrain Loss  0.0479\n",
            "Epoch(6/10)\t Batch(3200/3375): \tTrain Loss  0.0466\n",
            "End of Epoch: 6 Train Accuracy: 0.6501 Val Loss: 0.0516 Val Accuracy: 0.6432\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/3375): \tTrain Loss  0.0632\n",
            "Epoch(7/10)\t Batch(200/3375): \tTrain Loss  0.0578\n",
            "Epoch(7/10)\t Batch(400/3375): \tTrain Loss  0.0343\n",
            "Epoch(7/10)\t Batch(600/3375): \tTrain Loss  0.0456\n",
            "Epoch(7/10)\t Batch(800/3375): \tTrain Loss  0.0419\n",
            "Epoch(7/10)\t Batch(1000/3375): \tTrain Loss  0.0652\n",
            "Epoch(7/10)\t Batch(1200/3375): \tTrain Loss  0.0505\n",
            "Epoch(7/10)\t Batch(1400/3375): \tTrain Loss  0.0467\n",
            "Epoch(7/10)\t Batch(1600/3375): \tTrain Loss  0.0522\n",
            "Epoch(7/10)\t Batch(1800/3375): \tTrain Loss  0.0589\n",
            "Epoch(7/10)\t Batch(2000/3375): \tTrain Loss  0.0467\n",
            "Epoch(7/10)\t Batch(2200/3375): \tTrain Loss  0.0433\n",
            "Epoch(7/10)\t Batch(2400/3375): \tTrain Loss  0.0514\n",
            "Epoch(7/10)\t Batch(2600/3375): \tTrain Loss  0.0437\n",
            "Epoch(7/10)\t Batch(2800/3375): \tTrain Loss  0.0457\n",
            "Epoch(7/10)\t Batch(3000/3375): \tTrain Loss  0.0511\n",
            "Epoch(7/10)\t Batch(3200/3375): \tTrain Loss  0.0354\n",
            "End of Epoch: 7 Train Accuracy: 0.6696 Val Loss: 0.0478 Val Accuracy: 0.6613\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/3375): \tTrain Loss  0.0486\n",
            "Epoch(8/10)\t Batch(200/3375): \tTrain Loss  0.0402\n",
            "Epoch(8/10)\t Batch(400/3375): \tTrain Loss  0.0460\n",
            "Epoch(8/10)\t Batch(600/3375): \tTrain Loss  0.0410\n",
            "Epoch(8/10)\t Batch(800/3375): \tTrain Loss  0.0500\n",
            "Epoch(8/10)\t Batch(1000/3375): \tTrain Loss  0.0544\n",
            "Epoch(8/10)\t Batch(1200/3375): \tTrain Loss  0.0444\n",
            "Epoch(8/10)\t Batch(1400/3375): \tTrain Loss  0.0414\n",
            "Epoch(8/10)\t Batch(1600/3375): \tTrain Loss  0.0602\n",
            "Epoch(8/10)\t Batch(1800/3375): \tTrain Loss  0.0319\n",
            "Epoch(8/10)\t Batch(2000/3375): \tTrain Loss  0.0352\n",
            "Epoch(8/10)\t Batch(2200/3375): \tTrain Loss  0.0369\n",
            "Epoch(8/10)\t Batch(2400/3375): \tTrain Loss  0.0270\n",
            "Epoch(8/10)\t Batch(2600/3375): \tTrain Loss  0.0440\n",
            "Epoch(8/10)\t Batch(2800/3375): \tTrain Loss  0.0456\n",
            "Epoch(8/10)\t Batch(3000/3375): \tTrain Loss  0.0706\n",
            "Epoch(8/10)\t Batch(3200/3375): \tTrain Loss  0.0423\n",
            "End of Epoch: 8 Train Accuracy: 0.6808 Val Loss: 0.0449 Val Accuracy: 0.6722\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/3375): \tTrain Loss  0.0560\n",
            "Epoch(9/10)\t Batch(200/3375): \tTrain Loss  0.0459\n",
            "Epoch(9/10)\t Batch(400/3375): \tTrain Loss  0.0477\n",
            "Epoch(9/10)\t Batch(600/3375): \tTrain Loss  0.0274\n",
            "Epoch(9/10)\t Batch(800/3375): \tTrain Loss  0.0344\n",
            "Epoch(9/10)\t Batch(1000/3375): \tTrain Loss  0.0621\n",
            "Epoch(9/10)\t Batch(1200/3375): \tTrain Loss  0.0343\n",
            "Epoch(9/10)\t Batch(1400/3375): \tTrain Loss  0.0508\n",
            "Epoch(9/10)\t Batch(1600/3375): \tTrain Loss  0.0390\n",
            "Epoch(9/10)\t Batch(1800/3375): \tTrain Loss  0.0359\n",
            "Epoch(9/10)\t Batch(2000/3375): \tTrain Loss  0.0511\n",
            "Epoch(9/10)\t Batch(2200/3375): \tTrain Loss  0.0367\n",
            "Epoch(9/10)\t Batch(2400/3375): \tTrain Loss  0.0484\n",
            "Epoch(9/10)\t Batch(2600/3375): \tTrain Loss  0.0416\n",
            "Epoch(9/10)\t Batch(2800/3375): \tTrain Loss  0.0428\n",
            "Epoch(9/10)\t Batch(3000/3375): \tTrain Loss  0.0281\n",
            "Epoch(9/10)\t Batch(3200/3375): \tTrain Loss  0.0428\n",
            "End of Epoch: 9 Train Accuracy: 0.6906 Val Loss: 0.0423 Val Accuracy: 0.6812\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/3375): \tTrain Loss  0.0418\n",
            "Epoch(10/10)\t Batch(200/3375): \tTrain Loss  0.0404\n",
            "Epoch(10/10)\t Batch(400/3375): \tTrain Loss  0.0381\n",
            "Epoch(10/10)\t Batch(600/3375): \tTrain Loss  0.0419\n",
            "Epoch(10/10)\t Batch(800/3375): \tTrain Loss  0.0327\n",
            "Epoch(10/10)\t Batch(1000/3375): \tTrain Loss  0.0486\n",
            "Epoch(10/10)\t Batch(1200/3375): \tTrain Loss  0.0569\n",
            "Epoch(10/10)\t Batch(1400/3375): \tTrain Loss  0.0647\n",
            "Epoch(10/10)\t Batch(1600/3375): \tTrain Loss  0.0299\n",
            "Epoch(10/10)\t Batch(1800/3375): \tTrain Loss  0.0623\n",
            "Epoch(10/10)\t Batch(2000/3375): \tTrain Loss  0.0381\n",
            "Epoch(10/10)\t Batch(2200/3375): \tTrain Loss  0.0296\n",
            "Epoch(10/10)\t Batch(2400/3375): \tTrain Loss  0.0363\n",
            "Epoch(10/10)\t Batch(2600/3375): \tTrain Loss  0.0636\n",
            "Epoch(10/10)\t Batch(2800/3375): \tTrain Loss  0.0312\n",
            "Epoch(10/10)\t Batch(3000/3375): \tTrain Loss  0.0451\n",
            "Epoch(10/10)\t Batch(3200/3375): \tTrain Loss  0.0458\n",
            "End of Epoch: 10 Train Accuracy: 0.7229 Val Loss: 0.0400 Val Accuracy: 0.7203\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▄▅▆▆▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▇▇▆▅▃▄▄▂▁</td></tr><tr><td>Val Accuracy</td><td>▁▄▅▆▆▇▇▇▇██</td></tr><tr><td>Val Loss</td><td>█▇▇▅▄▃▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.72285</td></tr><tr><td>Train Loss</td><td>0.02123</td></tr><tr><td>Val Accuracy</td><td>0.72033</td></tr><tr><td>Val Loss</td><td>0.03997</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nag|loss_mse|lr=0.0001|batch_16|act_tanh|hid_4|neurons_32|nrns_10|init_Xavier300</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/x66xxbbk' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/x66xxbbk</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_173003-x66xxbbk/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o0ugmppq with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nag\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_173144-o0ugmppq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/o0ugmppq' target=\"_blank\">wandering-sweep-42</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/o0ugmppq' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/o0ugmppq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.1033 Val Loss: 2.3036 Val Accuracy: 0.1063\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  2.3049\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  2.3036\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  2.2967\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  2.2998\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  2.2975\n",
            "End of Epoch: 1 Train Accuracy: 0.2116 Val Loss: 2.2938 Val Accuracy: 0.2142\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  2.2937\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  2.2909\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  2.2902\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  2.2891\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  2.2863\n",
            "End of Epoch: 2 Train Accuracy: 0.3329 Val Loss: 2.2870 Val Accuracy: 0.3303\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  2.2859\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  2.2884\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  2.2842\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  2.2825\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  2.2814\n",
            "End of Epoch: 3 Train Accuracy: 0.4476 Val Loss: 2.2817 Val Accuracy: 0.4478\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  2.2778\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  2.2803\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  2.2752\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  2.2801\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  2.2782\n",
            "End of Epoch: 4 Train Accuracy: 0.4892 Val Loss: 2.2766 Val Accuracy: 0.4920\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  2.2787\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  2.2778\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  2.2729\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  2.2747\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  2.2732\n",
            "End of Epoch: 5 Train Accuracy: 0.5052 Val Loss: 2.2716 Val Accuracy: 0.5087\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  2.2753\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  2.2692\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  2.2631\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  2.2696\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  2.2684\n",
            "End of Epoch: 6 Train Accuracy: 0.5183 Val Loss: 2.2666 Val Accuracy: 0.5182\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  2.2613\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  2.2637\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  2.2607\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  2.2677\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  2.2609\n",
            "End of Epoch: 7 Train Accuracy: 0.5316 Val Loss: 2.2620 Val Accuracy: 0.5343\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  2.2634\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  2.2581\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  2.2563\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  2.2549\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  2.2602\n",
            "End of Epoch: 8 Train Accuracy: 0.5447 Val Loss: 2.2578 Val Accuracy: 0.5463\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  2.2548\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  2.2522\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  2.2493\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  2.2530\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  2.2581\n",
            "End of Epoch: 9 Train Accuracy: 0.5520 Val Loss: 2.2541 Val Accuracy: 0.5505\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  2.2511\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  2.2557\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  2.2535\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  2.2498\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  2.2523\n",
            "End of Epoch: 10 Train Accuracy: 0.5508 Val Loss: 2.2510 Val Accuracy: 0.5493\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▃▅▆▇▇▇████</td></tr><tr><td>Train Loss</td><td>█▇▆▅▄▃▃▂▁▂</td></tr><tr><td>Val Accuracy</td><td>▁▃▅▆▇▇▇████</td></tr><tr><td>Val Loss</td><td>█▇▆▅▄▄▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.55076</td></tr><tr><td>Train Loss</td><td>2.25614</td></tr><tr><td>Val Accuracy</td><td>0.54933</td></tr><tr><td>Val Loss</td><td>2.25095</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nag|loss_cross_entropy|lr=0.0001|batch_64|act_relu|hid_4|neurons_64|nrns_10|init_Xavier738</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/o0ugmppq' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/o0ugmppq</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_173144-o0ugmppq/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: whjvxjxd with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_173229-whjvxjxd</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/whjvxjxd' target=\"_blank\">fragrant-sweep-43</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/whjvxjxd' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/whjvxjxd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.1004 Val Loss: 0.1719 Val Accuracy: 0.1042\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/1687): \tTrain Loss  0.1881\n",
            "Epoch(1/10)\t Batch(200/1687): \tTrain Loss  0.1604\n",
            "Epoch(1/10)\t Batch(400/1687): \tTrain Loss  0.1706\n",
            "Epoch(1/10)\t Batch(600/1687): \tTrain Loss  0.1548\n",
            "Epoch(1/10)\t Batch(800/1687): \tTrain Loss  0.1815\n",
            "Epoch(1/10)\t Batch(1000/1687): \tTrain Loss  0.1609\n",
            "Epoch(1/10)\t Batch(1200/1687): \tTrain Loss  0.1692\n",
            "Epoch(1/10)\t Batch(1400/1687): \tTrain Loss  0.1497\n",
            "Epoch(1/10)\t Batch(1600/1687): \tTrain Loss  0.1683\n",
            "End of Epoch: 1 Train Accuracy: 0.1044 Val Loss: 0.1637 Val Accuracy: 0.1082\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/1687): \tTrain Loss  0.1511\n",
            "Epoch(2/10)\t Batch(200/1687): \tTrain Loss  0.1697\n",
            "Epoch(2/10)\t Batch(400/1687): \tTrain Loss  0.1604\n",
            "Epoch(2/10)\t Batch(600/1687): \tTrain Loss  0.1653\n",
            "Epoch(2/10)\t Batch(800/1687): \tTrain Loss  0.1725\n",
            "Epoch(2/10)\t Batch(1000/1687): \tTrain Loss  0.1631\n",
            "Epoch(2/10)\t Batch(1200/1687): \tTrain Loss  0.1590\n",
            "Epoch(2/10)\t Batch(1400/1687): \tTrain Loss  0.1553\n",
            "Epoch(2/10)\t Batch(1600/1687): \tTrain Loss  0.1628\n",
            "End of Epoch: 2 Train Accuracy: 0.1034 Val Loss: 0.1507 Val Accuracy: 0.1037\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/1687): \tTrain Loss  0.1369\n",
            "Epoch(3/10)\t Batch(200/1687): \tTrain Loss  0.1313\n",
            "Epoch(3/10)\t Batch(400/1687): \tTrain Loss  0.1500\n",
            "Epoch(3/10)\t Batch(600/1687): \tTrain Loss  0.1528\n",
            "Epoch(3/10)\t Batch(800/1687): \tTrain Loss  0.1489\n",
            "Epoch(3/10)\t Batch(1000/1687): \tTrain Loss  0.1389\n",
            "Epoch(3/10)\t Batch(1200/1687): \tTrain Loss  0.1450\n",
            "Epoch(3/10)\t Batch(1400/1687): \tTrain Loss  0.1591\n",
            "Epoch(3/10)\t Batch(1600/1687): \tTrain Loss  0.1502\n",
            "End of Epoch: 3 Train Accuracy: 0.1024 Val Loss: 0.1419 Val Accuracy: 0.0975\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/1687): \tTrain Loss  0.1373\n",
            "Epoch(4/10)\t Batch(200/1687): \tTrain Loss  0.1402\n",
            "Epoch(4/10)\t Batch(400/1687): \tTrain Loss  0.1552\n",
            "Epoch(4/10)\t Batch(600/1687): \tTrain Loss  0.1422\n",
            "Epoch(4/10)\t Batch(800/1687): \tTrain Loss  0.1569\n",
            "Epoch(4/10)\t Batch(1000/1687): \tTrain Loss  0.1354\n",
            "Epoch(4/10)\t Batch(1200/1687): \tTrain Loss  0.1207\n",
            "Epoch(4/10)\t Batch(1400/1687): \tTrain Loss  0.1290\n",
            "Epoch(4/10)\t Batch(1600/1687): \tTrain Loss  0.1371\n",
            "End of Epoch: 4 Train Accuracy: 0.1035 Val Loss: 0.1380 Val Accuracy: 0.1017\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/1687): \tTrain Loss  0.1402\n",
            "Epoch(5/10)\t Batch(200/1687): \tTrain Loss  0.1319\n",
            "Epoch(5/10)\t Batch(400/1687): \tTrain Loss  0.1271\n",
            "Epoch(5/10)\t Batch(600/1687): \tTrain Loss  0.1308\n",
            "Epoch(5/10)\t Batch(800/1687): \tTrain Loss  0.1345\n",
            "Epoch(5/10)\t Batch(1000/1687): \tTrain Loss  0.1412\n",
            "Epoch(5/10)\t Batch(1200/1687): \tTrain Loss  0.1295\n",
            "Epoch(5/10)\t Batch(1400/1687): \tTrain Loss  0.1377\n",
            "Epoch(5/10)\t Batch(1600/1687): \tTrain Loss  0.1505\n",
            "End of Epoch: 5 Train Accuracy: 0.1055 Val Loss: 0.1359 Val Accuracy: 0.1023\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/1687): \tTrain Loss  0.1375\n",
            "Epoch(6/10)\t Batch(200/1687): \tTrain Loss  0.1294\n",
            "Epoch(6/10)\t Batch(400/1687): \tTrain Loss  0.1175\n",
            "Epoch(6/10)\t Batch(600/1687): \tTrain Loss  0.1356\n",
            "Epoch(6/10)\t Batch(800/1687): \tTrain Loss  0.1359\n",
            "Epoch(6/10)\t Batch(1000/1687): \tTrain Loss  0.1396\n",
            "Epoch(6/10)\t Batch(1200/1687): \tTrain Loss  0.1434\n",
            "Epoch(6/10)\t Batch(1400/1687): \tTrain Loss  0.1475\n",
            "Epoch(6/10)\t Batch(1600/1687): \tTrain Loss  0.1329\n",
            "End of Epoch: 6 Train Accuracy: 0.1084 Val Loss: 0.1343 Val Accuracy: 0.1065\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/1687): \tTrain Loss  0.1313\n",
            "Epoch(7/10)\t Batch(200/1687): \tTrain Loss  0.1350\n",
            "Epoch(7/10)\t Batch(400/1687): \tTrain Loss  0.1241\n",
            "Epoch(7/10)\t Batch(600/1687): \tTrain Loss  0.1443\n",
            "Epoch(7/10)\t Batch(800/1687): \tTrain Loss  0.1291\n",
            "Epoch(7/10)\t Batch(1000/1687): \tTrain Loss  0.1288\n",
            "Epoch(7/10)\t Batch(1200/1687): \tTrain Loss  0.1209\n",
            "Epoch(7/10)\t Batch(1400/1687): \tTrain Loss  0.1347\n",
            "Epoch(7/10)\t Batch(1600/1687): \tTrain Loss  0.1355\n",
            "End of Epoch: 7 Train Accuracy: 0.1116 Val Loss: 0.1329 Val Accuracy: 0.1100\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/1687): \tTrain Loss  0.1378\n",
            "Epoch(8/10)\t Batch(200/1687): \tTrain Loss  0.1337\n",
            "Epoch(8/10)\t Batch(400/1687): \tTrain Loss  0.1197\n",
            "Epoch(8/10)\t Batch(600/1687): \tTrain Loss  0.1326\n",
            "Epoch(8/10)\t Batch(800/1687): \tTrain Loss  0.1107\n",
            "Epoch(8/10)\t Batch(1000/1687): \tTrain Loss  0.1430\n",
            "Epoch(8/10)\t Batch(1200/1687): \tTrain Loss  0.1300\n",
            "Epoch(8/10)\t Batch(1400/1687): \tTrain Loss  0.1376\n",
            "Epoch(8/10)\t Batch(1600/1687): \tTrain Loss  0.1236\n",
            "End of Epoch: 8 Train Accuracy: 0.1158 Val Loss: 0.1316 Val Accuracy: 0.1128\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/1687): \tTrain Loss  0.1339\n",
            "Epoch(9/10)\t Batch(200/1687): \tTrain Loss  0.1272\n",
            "Epoch(9/10)\t Batch(400/1687): \tTrain Loss  0.1338\n",
            "Epoch(9/10)\t Batch(600/1687): \tTrain Loss  0.1312\n",
            "Epoch(9/10)\t Batch(800/1687): \tTrain Loss  0.1324\n",
            "Epoch(9/10)\t Batch(1000/1687): \tTrain Loss  0.1407\n",
            "Epoch(9/10)\t Batch(1200/1687): \tTrain Loss  0.1264\n",
            "Epoch(9/10)\t Batch(1400/1687): \tTrain Loss  0.1379\n",
            "Epoch(9/10)\t Batch(1600/1687): \tTrain Loss  0.1167\n",
            "End of Epoch: 9 Train Accuracy: 0.1197 Val Loss: 0.1303 Val Accuracy: 0.1160\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/1687): \tTrain Loss  0.1349\n",
            "Epoch(10/10)\t Batch(200/1687): \tTrain Loss  0.1348\n",
            "Epoch(10/10)\t Batch(400/1687): \tTrain Loss  0.1255\n",
            "Epoch(10/10)\t Batch(600/1687): \tTrain Loss  0.1383\n",
            "Epoch(10/10)\t Batch(800/1687): \tTrain Loss  0.1216\n",
            "Epoch(10/10)\t Batch(1000/1687): \tTrain Loss  0.1158\n",
            "Epoch(10/10)\t Batch(1200/1687): \tTrain Loss  0.1186\n",
            "Epoch(10/10)\t Batch(1400/1687): \tTrain Loss  0.1291\n",
            "Epoch(10/10)\t Batch(1600/1687): \tTrain Loss  0.1148\n",
            "End of Epoch: 10 Train Accuracy: 0.1240 Val Loss: 0.1289 Val Accuracy: 0.1195\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▂▂▂▂▃▃▄▆▇█</td></tr><tr><td>Train Loss</td><td>▇▅▅█▁▅▅▄▄▅</td></tr><tr><td>Val Accuracy</td><td>▃▄▃▁▂▃▄▅▆▇█</td></tr><tr><td>Val Loss</td><td>█▇▅▃▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.12396</td></tr><tr><td>Train Loss</td><td>0.14047</td></tr><tr><td>Val Accuracy</td><td>0.1195</td></tr><tr><td>Val Loss</td><td>0.1289</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_sgd|loss_mse|lr=0.0001|batch_32|act_sigmoid|hid_5|neurons_128|nrns_10|init_Random257</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/whjvxjxd' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/whjvxjxd</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_173229-whjvxjxd/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8z8zxz1r with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_173444-8z8zxz1r</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8z8zxz1r' target=\"_blank\">playful-sweep-44</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8z8zxz1r' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8z8zxz1r</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0916 Val Loss: 24.9591 Val Accuracy: 0.0965\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/843): \tTrain Loss  25.9041\n",
            "Epoch(1/5)\t Batch(200/843): \tTrain Loss  24.5135\n",
            "Epoch(1/5)\t Batch(400/843): \tTrain Loss  25.4723\n",
            "Epoch(1/5)\t Batch(600/843): \tTrain Loss  25.2535\n",
            "Epoch(1/5)\t Batch(800/843): \tTrain Loss  25.0406\n",
            "End of Epoch: 1 Train Accuracy: 0.0686 Val Loss: 25.5798 Val Accuracy: 0.0712\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/843): \tTrain Loss  25.8340\n",
            "Epoch(2/5)\t Batch(200/843): \tTrain Loss  23.5342\n",
            "Epoch(2/5)\t Batch(400/843): \tTrain Loss  25.9041\n",
            "Epoch(2/5)\t Batch(600/843): \tTrain Loss  27.1993\n",
            "Epoch(2/5)\t Batch(800/843): \tTrain Loss  25.9041\n",
            "End of Epoch: 2 Train Accuracy: 0.0738 Val Loss: 25.4968 Val Accuracy: 0.0725\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/843): \tTrain Loss  25.4843\n",
            "Epoch(3/5)\t Batch(200/843): \tTrain Loss  25.9041\n",
            "Epoch(3/5)\t Batch(400/843): \tTrain Loss  26.3358\n",
            "Epoch(3/5)\t Batch(600/843): \tTrain Loss  25.4723\n",
            "Epoch(3/5)\t Batch(800/843): \tTrain Loss  26.3358\n",
            "End of Epoch: 3 Train Accuracy: 0.0734 Val Loss: 25.4691 Val Accuracy: 0.0740\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/843): \tTrain Loss  26.1699\n",
            "Epoch(4/5)\t Batch(200/843): \tTrain Loss  26.5789\n",
            "Epoch(4/5)\t Batch(400/843): \tTrain Loss  25.4711\n",
            "Epoch(4/5)\t Batch(600/843): \tTrain Loss  26.7676\n",
            "Epoch(4/5)\t Batch(800/843): \tTrain Loss  25.3586\n",
            "End of Epoch: 4 Train Accuracy: 0.0720 Val Loss: 25.4115 Val Accuracy: 0.0743\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/843): \tTrain Loss  25.8728\n",
            "Epoch(5/5)\t Batch(200/843): \tTrain Loss  24.6089\n",
            "Epoch(5/5)\t Batch(400/843): \tTrain Loss  24.6089\n",
            "Epoch(5/5)\t Batch(600/843): \tTrain Loss  26.3358\n",
            "Epoch(5/5)\t Batch(800/843): \tTrain Loss  25.0406\n",
            "End of Epoch: 5 Train Accuracy: 0.0760 Val Loss: 25.3234 Val Accuracy: 0.0772\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>█▁▃▂▂▃</td></tr><tr><td>Train Loss</td><td>▅▅█▅▁</td></tr><tr><td>Val Accuracy</td><td>█▁▁▂▂▃</td></tr><tr><td>Val Loss</td><td>▁█▇▇▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.07598</td></tr><tr><td>Train Loss</td><td>24.62563</td></tr><tr><td>Val Accuracy</td><td>0.07717</td></tr><tr><td>Val Loss</td><td>25.32345</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_adam|loss_cross_entropy|lr=0.0001|batch_64|act_relu|hid_3|neurons_64|nrns_5|init_Random100</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8z8zxz1r' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8z8zxz1r</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_173444-8z8zxz1r/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yyymli44 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_173515-yyymli44</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/yyymli44' target=\"_blank\">zesty-sweep-45</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/yyymli44' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/yyymli44</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.1073 Val Loss: 7.2546 Val Accuracy: 0.1073\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/1687): \tTrain Loss  6.3169\n",
            "Epoch(1/5)\t Batch(200/1687): \tTrain Loss  4.5814\n",
            "Epoch(1/5)\t Batch(400/1687): \tTrain Loss  4.3756\n",
            "Epoch(1/5)\t Batch(600/1687): \tTrain Loss  3.5274\n",
            "Epoch(1/5)\t Batch(800/1687): \tTrain Loss  3.6406\n",
            "Epoch(1/5)\t Batch(1000/1687): \tTrain Loss  2.9116\n",
            "Epoch(1/5)\t Batch(1200/1687): \tTrain Loss  2.8112\n",
            "Epoch(1/5)\t Batch(1400/1687): \tTrain Loss  2.8471\n",
            "Epoch(1/5)\t Batch(1600/1687): \tTrain Loss  2.4817\n",
            "End of Epoch: 1 Train Accuracy: 0.1136 Val Loss: 2.6222 Val Accuracy: 0.1090\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/1687): \tTrain Loss  2.8400\n",
            "Epoch(2/5)\t Batch(200/1687): \tTrain Loss  2.3444\n",
            "Epoch(2/5)\t Batch(400/1687): \tTrain Loss  2.5834\n",
            "Epoch(2/5)\t Batch(600/1687): \tTrain Loss  2.3575\n",
            "Epoch(2/5)\t Batch(800/1687): \tTrain Loss  2.3381\n",
            "Epoch(2/5)\t Batch(1000/1687): \tTrain Loss  2.4268\n",
            "Epoch(2/5)\t Batch(1200/1687): \tTrain Loss  2.3391\n",
            "Epoch(2/5)\t Batch(1400/1687): \tTrain Loss  2.4132\n",
            "Epoch(2/5)\t Batch(1600/1687): \tTrain Loss  2.2725\n",
            "End of Epoch: 2 Train Accuracy: 0.1486 Val Loss: 2.3025 Val Accuracy: 0.1485\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/1687): \tTrain Loss  2.3310\n",
            "Epoch(3/5)\t Batch(200/1687): \tTrain Loss  2.2832\n",
            "Epoch(3/5)\t Batch(400/1687): \tTrain Loss  2.2183\n",
            "Epoch(3/5)\t Batch(600/1687): \tTrain Loss  2.1535\n",
            "Epoch(3/5)\t Batch(800/1687): \tTrain Loss  2.1701\n",
            "Epoch(3/5)\t Batch(1000/1687): \tTrain Loss  2.2005\n",
            "Epoch(3/5)\t Batch(1200/1687): \tTrain Loss  2.0887\n",
            "Epoch(3/5)\t Batch(1400/1687): \tTrain Loss  2.1775\n",
            "Epoch(3/5)\t Batch(1600/1687): \tTrain Loss  2.3287\n",
            "End of Epoch: 3 Train Accuracy: 0.2007 Val Loss: 2.2154 Val Accuracy: 0.1967\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/1687): \tTrain Loss  2.0774\n",
            "Epoch(4/5)\t Batch(200/1687): \tTrain Loss  2.1163\n",
            "Epoch(4/5)\t Batch(400/1687): \tTrain Loss  2.2326\n",
            "Epoch(4/5)\t Batch(600/1687): \tTrain Loss  2.1547\n",
            "Epoch(4/5)\t Batch(800/1687): \tTrain Loss  2.2481\n",
            "Epoch(4/5)\t Batch(1000/1687): \tTrain Loss  2.3101\n",
            "Epoch(4/5)\t Batch(1200/1687): \tTrain Loss  2.2045\n",
            "Epoch(4/5)\t Batch(1400/1687): \tTrain Loss  2.1901\n",
            "Epoch(4/5)\t Batch(1600/1687): \tTrain Loss  2.2123\n",
            "End of Epoch: 4 Train Accuracy: 0.2436 Val Loss: 2.1899 Val Accuracy: 0.2395\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/1687): \tTrain Loss  2.0913\n",
            "Epoch(5/5)\t Batch(200/1687): \tTrain Loss  2.1640\n",
            "Epoch(5/5)\t Batch(400/1687): \tTrain Loss  2.1733\n",
            "Epoch(5/5)\t Batch(600/1687): \tTrain Loss  2.2352\n",
            "Epoch(5/5)\t Batch(800/1687): \tTrain Loss  2.2658\n",
            "Epoch(5/5)\t Batch(1000/1687): \tTrain Loss  2.2172\n",
            "Epoch(5/5)\t Batch(1200/1687): \tTrain Loss  2.2111\n",
            "Epoch(5/5)\t Batch(1400/1687): \tTrain Loss  2.1965\n",
            "Epoch(5/5)\t Batch(1600/1687): \tTrain Loss  2.1262\n",
            "End of Epoch: 5 Train Accuracy: 0.2740 Val Loss: 2.1746 Val Accuracy: 0.2768\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▁▃▅▇█</td></tr><tr><td>Train Loss</td><td>▁█▁▄▃</td></tr><tr><td>Val Accuracy</td><td>▁▁▃▅▆█</td></tr><tr><td>Val Loss</td><td>█▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.27396</td></tr><tr><td>Train Loss</td><td>2.2205</td></tr><tr><td>Val Accuracy</td><td>0.27683</td></tr><tr><td>Val Loss</td><td>2.17457</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_adam|loss_cross_entropy|lr=0.0001|batch_32|act_sigmoid|hid_4|neurons_64|nrns_5|init_Random699</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/yyymli44' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/yyymli44</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_173515-yyymli44/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bs8eimqv with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_173603-bs8eimqv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bs8eimqv' target=\"_blank\">summer-sweep-46</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bs8eimqv' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bs8eimqv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.1242 Val Loss: 2.3056 Val Accuracy: 0.1192\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/1687): \tTrain Loss  2.3046\n",
            "Epoch(1/5)\t Batch(200/1687): \tTrain Loss  2.1663\n",
            "Epoch(1/5)\t Batch(400/1687): \tTrain Loss  2.1156\n",
            "Epoch(1/5)\t Batch(600/1687): \tTrain Loss  1.9733\n",
            "Epoch(1/5)\t Batch(800/1687): \tTrain Loss  1.9432\n",
            "Epoch(1/5)\t Batch(1000/1687): \tTrain Loss  1.9517\n",
            "Epoch(1/5)\t Batch(1200/1687): \tTrain Loss  1.9801\n",
            "Epoch(1/5)\t Batch(1400/1687): \tTrain Loss  1.9242\n",
            "Epoch(1/5)\t Batch(1600/1687): \tTrain Loss  1.9385\n",
            "End of Epoch: 1 Train Accuracy: 0.7678 Val Loss: 1.8645 Val Accuracy: 0.7700\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/1687): \tTrain Loss  1.8290\n",
            "Epoch(2/5)\t Batch(200/1687): \tTrain Loss  1.7499\n",
            "Epoch(2/5)\t Batch(400/1687): \tTrain Loss  1.8497\n",
            "Epoch(2/5)\t Batch(600/1687): \tTrain Loss  1.9017\n",
            "Epoch(2/5)\t Batch(800/1687): \tTrain Loss  1.8131\n",
            "Epoch(2/5)\t Batch(1000/1687): \tTrain Loss  1.7778\n",
            "Epoch(2/5)\t Batch(1200/1687): \tTrain Loss  1.7634\n",
            "Epoch(2/5)\t Batch(1400/1687): \tTrain Loss  1.8790\n",
            "Epoch(2/5)\t Batch(1600/1687): \tTrain Loss  1.7852\n",
            "End of Epoch: 2 Train Accuracy: 0.7984 Val Loss: 1.7923 Val Accuracy: 0.7965\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/1687): \tTrain Loss  1.7962\n",
            "Epoch(3/5)\t Batch(200/1687): \tTrain Loss  1.8176\n",
            "Epoch(3/5)\t Batch(400/1687): \tTrain Loss  1.7908\n",
            "Epoch(3/5)\t Batch(600/1687): \tTrain Loss  1.8890\n",
            "Epoch(3/5)\t Batch(800/1687): \tTrain Loss  1.8179\n",
            "Epoch(3/5)\t Batch(1000/1687): \tTrain Loss  1.7524\n",
            "Epoch(3/5)\t Batch(1200/1687): \tTrain Loss  1.7978\n",
            "Epoch(3/5)\t Batch(1400/1687): \tTrain Loss  1.8403\n",
            "Epoch(3/5)\t Batch(1600/1687): \tTrain Loss  1.7119\n",
            "End of Epoch: 3 Train Accuracy: 0.8127 Val Loss: 1.7633 Val Accuracy: 0.8132\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/1687): \tTrain Loss  1.8651\n",
            "Epoch(4/5)\t Batch(200/1687): \tTrain Loss  1.7898\n",
            "Epoch(4/5)\t Batch(400/1687): \tTrain Loss  1.7843\n",
            "Epoch(4/5)\t Batch(600/1687): \tTrain Loss  1.7358\n",
            "Epoch(4/5)\t Batch(800/1687): \tTrain Loss  1.7038\n",
            "Epoch(4/5)\t Batch(1000/1687): \tTrain Loss  1.7644\n",
            "Epoch(4/5)\t Batch(1200/1687): \tTrain Loss  1.7429\n",
            "Epoch(4/5)\t Batch(1400/1687): \tTrain Loss  1.7379\n",
            "Epoch(4/5)\t Batch(1600/1687): \tTrain Loss  1.6605\n",
            "End of Epoch: 4 Train Accuracy: 0.8216 Val Loss: 1.7457 Val Accuracy: 0.8230\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/1687): \tTrain Loss  1.7309\n",
            "Epoch(5/5)\t Batch(200/1687): \tTrain Loss  1.6915\n",
            "Epoch(5/5)\t Batch(400/1687): \tTrain Loss  1.7589\n",
            "Epoch(5/5)\t Batch(600/1687): \tTrain Loss  1.7602\n",
            "Epoch(5/5)\t Batch(800/1687): \tTrain Loss  1.7509\n",
            "Epoch(5/5)\t Batch(1000/1687): \tTrain Loss  1.7168\n",
            "Epoch(5/5)\t Batch(1200/1687): \tTrain Loss  1.7003\n",
            "Epoch(5/5)\t Batch(1400/1687): \tTrain Loss  1.7139\n",
            "Epoch(5/5)\t Batch(1600/1687): \tTrain Loss  1.7052\n",
            "End of Epoch: 5 Train Accuracy: 0.8301 Val Loss: 1.7316 Val Accuracy: 0.8292\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▇████</td></tr><tr><td>Train Loss</td><td>▇█▁▆▃</td></tr><tr><td>Val Accuracy</td><td>▁▇████</td></tr><tr><td>Val Loss</td><td>█▃▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.83006</td></tr><tr><td>Train Loss</td><td>1.73536</td></tr><tr><td>Val Accuracy</td><td>0.82917</td></tr><tr><td>Val Loss</td><td>1.73155</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_adam|loss_cross_entropy|lr=0.0001|batch_32|act_relu|hid_5|neurons_64|nrns_5|init_Xavier785</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bs8eimqv' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bs8eimqv</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_173603-bs8eimqv/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f8wjog2f with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_173656-f8wjog2f</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/f8wjog2f' target=\"_blank\">sweet-sweep-47</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/f8wjog2f' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/f8wjog2f</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.1300 Val Loss: 0.0899 Val Accuracy: 0.1283\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/3375): \tTrain Loss  0.0901\n",
            "Epoch(1/5)\t Batch(200/3375): \tTrain Loss  0.0640\n",
            "Epoch(1/5)\t Batch(400/3375): \tTrain Loss  0.0315\n",
            "Epoch(1/5)\t Batch(600/3375): \tTrain Loss  0.0188\n",
            "Epoch(1/5)\t Batch(800/3375): \tTrain Loss  0.0419\n",
            "Epoch(1/5)\t Batch(1000/3375): \tTrain Loss  0.0270\n",
            "Epoch(1/5)\t Batch(1200/3375): \tTrain Loss  0.0364\n",
            "Epoch(1/5)\t Batch(1400/3375): \tTrain Loss  0.0310\n",
            "Epoch(1/5)\t Batch(1600/3375): \tTrain Loss  0.0231\n",
            "Epoch(1/5)\t Batch(1800/3375): \tTrain Loss  0.0211\n",
            "Epoch(1/5)\t Batch(2000/3375): \tTrain Loss  0.0142\n",
            "Epoch(1/5)\t Batch(2200/3375): \tTrain Loss  0.0484\n",
            "Epoch(1/5)\t Batch(2400/3375): \tTrain Loss  0.0196\n",
            "Epoch(1/5)\t Batch(2600/3375): \tTrain Loss  0.0250\n",
            "Epoch(1/5)\t Batch(2800/3375): \tTrain Loss  0.0347\n",
            "Epoch(1/5)\t Batch(3000/3375): \tTrain Loss  0.0218\n",
            "Epoch(1/5)\t Batch(3200/3375): \tTrain Loss  0.0183\n",
            "End of Epoch: 1 Train Accuracy: 0.8313 Val Loss: 0.0240 Val Accuracy: 0.8372\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/3375): \tTrain Loss  0.0472\n",
            "Epoch(2/5)\t Batch(200/3375): \tTrain Loss  0.0094\n",
            "Epoch(2/5)\t Batch(400/3375): \tTrain Loss  0.0180\n",
            "Epoch(2/5)\t Batch(600/3375): \tTrain Loss  0.0178\n",
            "Epoch(2/5)\t Batch(800/3375): \tTrain Loss  0.0245\n",
            "Epoch(2/5)\t Batch(1000/3375): \tTrain Loss  0.0150\n",
            "Epoch(2/5)\t Batch(1200/3375): \tTrain Loss  0.0483\n",
            "Epoch(2/5)\t Batch(1400/3375): \tTrain Loss  0.0334\n",
            "Epoch(2/5)\t Batch(1600/3375): \tTrain Loss  0.0368\n",
            "Epoch(2/5)\t Batch(1800/3375): \tTrain Loss  0.0049\n",
            "Epoch(2/5)\t Batch(2000/3375): \tTrain Loss  0.0143\n",
            "Epoch(2/5)\t Batch(2200/3375): \tTrain Loss  0.0217\n",
            "Epoch(2/5)\t Batch(2400/3375): \tTrain Loss  0.0164\n",
            "Epoch(2/5)\t Batch(2600/3375): \tTrain Loss  0.0357\n",
            "Epoch(2/5)\t Batch(2800/3375): \tTrain Loss  0.0449\n",
            "Epoch(2/5)\t Batch(3000/3375): \tTrain Loss  0.0146\n",
            "Epoch(2/5)\t Batch(3200/3375): \tTrain Loss  0.0199\n",
            "End of Epoch: 2 Train Accuracy: 0.8607 Val Loss: 0.0207 Val Accuracy: 0.8547\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/3375): \tTrain Loss  0.0300\n",
            "Epoch(3/5)\t Batch(200/3375): \tTrain Loss  0.0253\n",
            "Epoch(3/5)\t Batch(400/3375): \tTrain Loss  0.0138\n",
            "Epoch(3/5)\t Batch(600/3375): \tTrain Loss  0.0112\n",
            "Epoch(3/5)\t Batch(800/3375): \tTrain Loss  0.0462\n",
            "Epoch(3/5)\t Batch(1000/3375): \tTrain Loss  0.0087\n",
            "Epoch(3/5)\t Batch(1200/3375): \tTrain Loss  0.0078\n",
            "Epoch(3/5)\t Batch(1400/3375): \tTrain Loss  0.0133\n",
            "Epoch(3/5)\t Batch(1600/3375): \tTrain Loss  0.0098\n",
            "Epoch(3/5)\t Batch(1800/3375): \tTrain Loss  0.0027\n",
            "Epoch(3/5)\t Batch(2000/3375): \tTrain Loss  0.0049\n",
            "Epoch(3/5)\t Batch(2200/3375): \tTrain Loss  0.0181\n",
            "Epoch(3/5)\t Batch(2400/3375): \tTrain Loss  0.0113\n",
            "Epoch(3/5)\t Batch(2600/3375): \tTrain Loss  0.0277\n",
            "Epoch(3/5)\t Batch(2800/3375): \tTrain Loss  0.0281\n",
            "Epoch(3/5)\t Batch(3000/3375): \tTrain Loss  0.0224\n",
            "Epoch(3/5)\t Batch(3200/3375): \tTrain Loss  0.0334\n",
            "End of Epoch: 3 Train Accuracy: 0.8392 Val Loss: 0.0245 Val Accuracy: 0.8328\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/3375): \tTrain Loss  0.0204\n",
            "Epoch(4/5)\t Batch(200/3375): \tTrain Loss  0.0210\n",
            "Epoch(4/5)\t Batch(400/3375): \tTrain Loss  0.0182\n",
            "Epoch(4/5)\t Batch(600/3375): \tTrain Loss  0.0203\n",
            "Epoch(4/5)\t Batch(800/3375): \tTrain Loss  0.0109\n",
            "Epoch(4/5)\t Batch(1000/3375): \tTrain Loss  0.0114\n",
            "Epoch(4/5)\t Batch(1200/3375): \tTrain Loss  0.0439\n",
            "Epoch(4/5)\t Batch(1400/3375): \tTrain Loss  0.0376\n",
            "Epoch(4/5)\t Batch(1600/3375): \tTrain Loss  0.0168\n",
            "Epoch(4/5)\t Batch(1800/3375): \tTrain Loss  0.0230\n",
            "Epoch(4/5)\t Batch(2000/3375): \tTrain Loss  0.0449\n",
            "Epoch(4/5)\t Batch(2200/3375): \tTrain Loss  0.0239\n",
            "Epoch(4/5)\t Batch(2400/3375): \tTrain Loss  0.0205\n",
            "Epoch(4/5)\t Batch(2600/3375): \tTrain Loss  0.0227\n",
            "Epoch(4/5)\t Batch(2800/3375): \tTrain Loss  0.0097\n",
            "Epoch(4/5)\t Batch(3000/3375): \tTrain Loss  0.0254\n",
            "Epoch(4/5)\t Batch(3200/3375): \tTrain Loss  0.0007\n",
            "End of Epoch: 4 Train Accuracy: 0.8732 Val Loss: 0.0194 Val Accuracy: 0.8647\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/3375): \tTrain Loss  0.0109\n",
            "Epoch(5/5)\t Batch(200/3375): \tTrain Loss  0.0094\n",
            "Epoch(5/5)\t Batch(400/3375): \tTrain Loss  0.0033\n",
            "Epoch(5/5)\t Batch(600/3375): \tTrain Loss  0.0255\n",
            "Epoch(5/5)\t Batch(800/3375): \tTrain Loss  0.0109\n",
            "Epoch(5/5)\t Batch(1000/3375): \tTrain Loss  0.0091\n",
            "Epoch(5/5)\t Batch(1200/3375): \tTrain Loss  0.0194\n",
            "Epoch(5/5)\t Batch(1400/3375): \tTrain Loss  0.0172\n",
            "Epoch(5/5)\t Batch(1600/3375): \tTrain Loss  0.0290\n",
            "Epoch(5/5)\t Batch(1800/3375): \tTrain Loss  0.0001\n",
            "Epoch(5/5)\t Batch(2000/3375): \tTrain Loss  0.0251\n",
            "Epoch(5/5)\t Batch(2200/3375): \tTrain Loss  0.0175\n",
            "Epoch(5/5)\t Batch(2400/3375): \tTrain Loss  0.0078\n",
            "Epoch(5/5)\t Batch(2600/3375): \tTrain Loss  0.0274\n",
            "Epoch(5/5)\t Batch(2800/3375): \tTrain Loss  0.0157\n",
            "Epoch(5/5)\t Batch(3000/3375): \tTrain Loss  0.0234\n",
            "Epoch(5/5)\t Batch(3200/3375): \tTrain Loss  0.0066\n",
            "End of Epoch: 5 Train Accuracy: 0.8814 Val Loss: 0.0185 Val Accuracy: 0.8717\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁█████</td></tr><tr><td>Train Loss</td><td>█▂▁▆▃</td></tr><tr><td>Val Accuracy</td><td>▁█████</td></tr><tr><td>Val Loss</td><td>█▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.88141</td></tr><tr><td>Train Loss</td><td>0.02083</td></tr><tr><td>Val Accuracy</td><td>0.87167</td></tr><tr><td>Val Loss</td><td>0.01855</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_mse|lr=0.001|batch_16|act_relu|hid_5|neurons_32|nrns_5|init_Xavier679</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/f8wjog2f' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/f8wjog2f</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_173656-f8wjog2f/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8d9d0gdm with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nag\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_173757-8d9d0gdm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8d9d0gdm' target=\"_blank\">fanciful-sweep-48</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8d9d0gdm' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8d9d0gdm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0777 Val Loss: 2.2987 Val Accuracy: 0.0830\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/843): \tTrain Loss  2.2997\n",
            "Epoch(1/5)\t Batch(200/843): \tTrain Loss  2.2443\n",
            "Epoch(1/5)\t Batch(400/843): \tTrain Loss  2.2118\n",
            "Epoch(1/5)\t Batch(600/843): \tTrain Loss  2.1903\n",
            "Epoch(1/5)\t Batch(800/843): \tTrain Loss  2.1639\n",
            "End of Epoch: 1 Train Accuracy: 0.5865 Val Loss: 2.1582 Val Accuracy: 0.5817\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/843): \tTrain Loss  2.1587\n",
            "Epoch(2/5)\t Batch(200/843): \tTrain Loss  2.1455\n",
            "Epoch(2/5)\t Batch(400/843): \tTrain Loss  2.1603\n",
            "Epoch(2/5)\t Batch(600/843): \tTrain Loss  2.1441\n",
            "Epoch(2/5)\t Batch(800/843): \tTrain Loss  2.1110\n",
            "End of Epoch: 2 Train Accuracy: 0.5836 Val Loss: 2.1230 Val Accuracy: 0.5762\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/843): \tTrain Loss  2.1178\n",
            "Epoch(3/5)\t Batch(200/843): \tTrain Loss  2.1056\n",
            "Epoch(3/5)\t Batch(400/843): \tTrain Loss  2.1067\n",
            "Epoch(3/5)\t Batch(600/843): \tTrain Loss  2.1291\n",
            "Epoch(3/5)\t Batch(800/843): \tTrain Loss  2.1352\n",
            "End of Epoch: 3 Train Accuracy: 0.5587 Val Loss: 2.1178 Val Accuracy: 0.5483\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/843): \tTrain Loss  2.1075\n",
            "Epoch(4/5)\t Batch(200/843): \tTrain Loss  2.1122\n",
            "Epoch(4/5)\t Batch(400/843): \tTrain Loss  2.1132\n",
            "Epoch(4/5)\t Batch(600/843): \tTrain Loss  2.1106\n",
            "Epoch(4/5)\t Batch(800/843): \tTrain Loss  2.1392\n",
            "End of Epoch: 4 Train Accuracy: 0.5494 Val Loss: 2.1186 Val Accuracy: 0.5417\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/843): \tTrain Loss  2.0850\n",
            "Epoch(5/5)\t Batch(200/843): \tTrain Loss  2.1205\n",
            "Epoch(5/5)\t Batch(400/843): \tTrain Loss  2.1143\n",
            "Epoch(5/5)\t Batch(600/843): \tTrain Loss  2.0988\n",
            "Epoch(5/5)\t Batch(800/843): \tTrain Loss  2.1080\n",
            "End of Epoch: 5 Train Accuracy: 0.5572 Val Loss: 2.1203 Val Accuracy: 0.5525\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁███▇█</td></tr><tr><td>Train Loss</td><td>██▄▁▆</td></tr><tr><td>Val Accuracy</td><td>▁███▇█</td></tr><tr><td>Val Loss</td><td>█▃▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.55719</td></tr><tr><td>Train Loss</td><td>2.12876</td></tr><tr><td>Val Accuracy</td><td>0.5525</td></tr><tr><td>Val Loss</td><td>2.12034</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nag|loss_cross_entropy|lr=0.001|batch_64|act_relu|hid_3|neurons_32|nrns_5|init_Xavier815</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8d9d0gdm' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8d9d0gdm</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_173757-8d9d0gdm/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2min152v with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nag\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_173817-2min152v</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/2min152v' target=\"_blank\">radiant-sweep-49</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/2min152v' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/2min152v</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.1376 Val Loss: 0.0901 Val Accuracy: 0.1375\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/1687): \tTrain Loss  0.0899\n",
            "Epoch(1/10)\t Batch(200/1687): \tTrain Loss  0.0899\n",
            "Epoch(1/10)\t Batch(400/1687): \tTrain Loss  0.0901\n",
            "Epoch(1/10)\t Batch(600/1687): \tTrain Loss  0.0883\n",
            "Epoch(1/10)\t Batch(800/1687): \tTrain Loss  0.0890\n",
            "Epoch(1/10)\t Batch(1000/1687): \tTrain Loss  0.0890\n",
            "Epoch(1/10)\t Batch(1200/1687): \tTrain Loss  0.0882\n",
            "Epoch(1/10)\t Batch(1400/1687): \tTrain Loss  0.0878\n",
            "Epoch(1/10)\t Batch(1600/1687): \tTrain Loss  0.0885\n",
            "End of Epoch: 1 Train Accuracy: 0.2462 Val Loss: 0.0878 Val Accuracy: 0.2420\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/1687): \tTrain Loss  0.0888\n",
            "Epoch(2/10)\t Batch(200/1687): \tTrain Loss  0.0877\n",
            "Epoch(2/10)\t Batch(400/1687): \tTrain Loss  0.0876\n",
            "Epoch(2/10)\t Batch(600/1687): \tTrain Loss  0.0874\n",
            "Epoch(2/10)\t Batch(800/1687): \tTrain Loss  0.0874\n",
            "Epoch(2/10)\t Batch(1000/1687): \tTrain Loss  0.0878\n",
            "Epoch(2/10)\t Batch(1200/1687): \tTrain Loss  0.0870\n",
            "Epoch(2/10)\t Batch(1400/1687): \tTrain Loss  0.0858\n",
            "Epoch(2/10)\t Batch(1600/1687): \tTrain Loss  0.0867\n",
            "End of Epoch: 2 Train Accuracy: 0.3108 Val Loss: 0.0865 Val Accuracy: 0.3087\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/1687): \tTrain Loss  0.0863\n",
            "Epoch(3/10)\t Batch(200/1687): \tTrain Loss  0.0863\n",
            "Epoch(3/10)\t Batch(400/1687): \tTrain Loss  0.0861\n",
            "Epoch(3/10)\t Batch(600/1687): \tTrain Loss  0.0860\n",
            "Epoch(3/10)\t Batch(800/1687): \tTrain Loss  0.0853\n",
            "Epoch(3/10)\t Batch(1000/1687): \tTrain Loss  0.0852\n",
            "Epoch(3/10)\t Batch(1200/1687): \tTrain Loss  0.0837\n",
            "Epoch(3/10)\t Batch(1400/1687): \tTrain Loss  0.0869\n",
            "Epoch(3/10)\t Batch(1600/1687): \tTrain Loss  0.0858\n",
            "End of Epoch: 3 Train Accuracy: 0.3875 Val Loss: 0.0855 Val Accuracy: 0.3883\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/1687): \tTrain Loss  0.0858\n",
            "Epoch(4/10)\t Batch(200/1687): \tTrain Loss  0.0854\n",
            "Epoch(4/10)\t Batch(400/1687): \tTrain Loss  0.0859\n",
            "Epoch(4/10)\t Batch(600/1687): \tTrain Loss  0.0852\n",
            "Epoch(4/10)\t Batch(800/1687): \tTrain Loss  0.0841\n",
            "Epoch(4/10)\t Batch(1000/1687): \tTrain Loss  0.0857\n",
            "Epoch(4/10)\t Batch(1200/1687): \tTrain Loss  0.0870\n",
            "Epoch(4/10)\t Batch(1400/1687): \tTrain Loss  0.0840\n",
            "Epoch(4/10)\t Batch(1600/1687): \tTrain Loss  0.0840\n",
            "End of Epoch: 4 Train Accuracy: 0.4033 Val Loss: 0.0848 Val Accuracy: 0.4002\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/1687): \tTrain Loss  0.0838\n",
            "Epoch(5/10)\t Batch(200/1687): \tTrain Loss  0.0853\n",
            "Epoch(5/10)\t Batch(400/1687): \tTrain Loss  0.0853\n",
            "Epoch(5/10)\t Batch(600/1687): \tTrain Loss  0.0851\n",
            "Epoch(5/10)\t Batch(800/1687): \tTrain Loss  0.0846\n",
            "Epoch(5/10)\t Batch(1000/1687): \tTrain Loss  0.0829\n",
            "Epoch(5/10)\t Batch(1200/1687): \tTrain Loss  0.0844\n",
            "Epoch(5/10)\t Batch(1400/1687): \tTrain Loss  0.0849\n",
            "Epoch(5/10)\t Batch(1600/1687): \tTrain Loss  0.0833\n",
            "End of Epoch: 5 Train Accuracy: 0.3649 Val Loss: 0.0842 Val Accuracy: 0.3632\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/1687): \tTrain Loss  0.0825\n",
            "Epoch(6/10)\t Batch(200/1687): \tTrain Loss  0.0853\n",
            "Epoch(6/10)\t Batch(400/1687): \tTrain Loss  0.0841\n",
            "Epoch(6/10)\t Batch(600/1687): \tTrain Loss  0.0835\n",
            "Epoch(6/10)\t Batch(800/1687): \tTrain Loss  0.0843\n",
            "Epoch(6/10)\t Batch(1000/1687): \tTrain Loss  0.0846\n",
            "Epoch(6/10)\t Batch(1200/1687): \tTrain Loss  0.0836\n",
            "Epoch(6/10)\t Batch(1400/1687): \tTrain Loss  0.0828\n",
            "Epoch(6/10)\t Batch(1600/1687): \tTrain Loss  0.0834\n",
            "End of Epoch: 6 Train Accuracy: 0.3182 Val Loss: 0.0838 Val Accuracy: 0.3157\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/1687): \tTrain Loss  0.0829\n",
            "Epoch(7/10)\t Batch(200/1687): \tTrain Loss  0.0851\n",
            "Epoch(7/10)\t Batch(400/1687): \tTrain Loss  0.0834\n",
            "Epoch(7/10)\t Batch(600/1687): \tTrain Loss  0.0838\n",
            "Epoch(7/10)\t Batch(800/1687): \tTrain Loss  0.0831\n",
            "Epoch(7/10)\t Batch(1000/1687): \tTrain Loss  0.0836\n",
            "Epoch(7/10)\t Batch(1200/1687): \tTrain Loss  0.0830\n",
            "Epoch(7/10)\t Batch(1400/1687): \tTrain Loss  0.0842\n",
            "Epoch(7/10)\t Batch(1600/1687): \tTrain Loss  0.0835\n",
            "End of Epoch: 7 Train Accuracy: 0.2802 Val Loss: 0.0835 Val Accuracy: 0.2795\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/1687): \tTrain Loss  0.0852\n",
            "Epoch(8/10)\t Batch(200/1687): \tTrain Loss  0.0815\n",
            "Epoch(8/10)\t Batch(400/1687): \tTrain Loss  0.0842\n",
            "Epoch(8/10)\t Batch(600/1687): \tTrain Loss  0.0861\n",
            "Epoch(8/10)\t Batch(800/1687): \tTrain Loss  0.0791\n",
            "Epoch(8/10)\t Batch(1000/1687): \tTrain Loss  0.0839\n",
            "Epoch(8/10)\t Batch(1200/1687): \tTrain Loss  0.0836\n",
            "Epoch(8/10)\t Batch(1400/1687): \tTrain Loss  0.0835\n",
            "Epoch(8/10)\t Batch(1600/1687): \tTrain Loss  0.0857\n",
            "End of Epoch: 8 Train Accuracy: 0.2571 Val Loss: 0.0832 Val Accuracy: 0.2575\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/1687): \tTrain Loss  0.0833\n",
            "Epoch(9/10)\t Batch(200/1687): \tTrain Loss  0.0826\n",
            "Epoch(9/10)\t Batch(400/1687): \tTrain Loss  0.0814\n",
            "Epoch(9/10)\t Batch(600/1687): \tTrain Loss  0.0833\n",
            "Epoch(9/10)\t Batch(800/1687): \tTrain Loss  0.0814\n",
            "Epoch(9/10)\t Batch(1000/1687): \tTrain Loss  0.0830\n",
            "Epoch(9/10)\t Batch(1200/1687): \tTrain Loss  0.0841\n",
            "Epoch(9/10)\t Batch(1400/1687): \tTrain Loss  0.0836\n",
            "Epoch(9/10)\t Batch(1600/1687): \tTrain Loss  0.0830\n",
            "End of Epoch: 9 Train Accuracy: 0.2421 Val Loss: 0.0831 Val Accuracy: 0.2415\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/1687): \tTrain Loss  0.0807\n",
            "Epoch(10/10)\t Batch(200/1687): \tTrain Loss  0.0809\n",
            "Epoch(10/10)\t Batch(400/1687): \tTrain Loss  0.0842\n",
            "Epoch(10/10)\t Batch(600/1687): \tTrain Loss  0.0840\n",
            "Epoch(10/10)\t Batch(800/1687): \tTrain Loss  0.0811\n",
            "Epoch(10/10)\t Batch(1000/1687): \tTrain Loss  0.0809\n",
            "Epoch(10/10)\t Batch(1200/1687): \tTrain Loss  0.0841\n",
            "Epoch(10/10)\t Batch(1400/1687): \tTrain Loss  0.0826\n",
            "Epoch(10/10)\t Batch(1600/1687): \tTrain Loss  0.0857\n",
            "End of Epoch: 10 Train Accuracy: 0.2311 Val Loss: 0.0829 Val Accuracy: 0.2305\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▄▆██▇▆▅▄▄▃</td></tr><tr><td>Train Loss</td><td>▇▇█▆▃▃▄▁▅▄</td></tr><tr><td>Val Accuracy</td><td>▁▄▆██▇▆▅▄▄▃</td></tr><tr><td>Val Loss</td><td>█▆▄▄▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.23106</td></tr><tr><td>Train Loss</td><td>0.08318</td></tr><tr><td>Val Accuracy</td><td>0.2305</td></tr><tr><td>Val Loss</td><td>0.08294</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nag|loss_mse|lr=0.0001|batch_32|act_tanh|hid_3|neurons_64|nrns_10|init_Xavier208</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/2min152v' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/2min152v</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_173817-2min152v/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lrhbvpj0 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_173953-lrhbvpj0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/lrhbvpj0' target=\"_blank\">decent-sweep-50</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/lrhbvpj0' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/lrhbvpj0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.1009 Val Loss: 0.1771 Val Accuracy: 0.0990\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  0.1794\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  0.1280\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  0.1236\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  0.1096\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  0.0978\n",
            "End of Epoch: 1 Train Accuracy: 0.3100 Val Loss: 0.0965 Val Accuracy: 0.3080\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  0.1003\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  0.0830\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  0.0891\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  0.0710\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  0.0601\n",
            "End of Epoch: 2 Train Accuracy: 0.4884 Val Loss: 0.0707 Val Accuracy: 0.4770\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  0.0704\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  0.0639\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  0.0667\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  0.0603\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  0.0712\n",
            "End of Epoch: 3 Train Accuracy: 0.5130 Val Loss: 0.0671 Val Accuracy: 0.5000\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  0.0738\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  0.0684\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  0.0650\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  0.0587\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  0.0660\n",
            "End of Epoch: 4 Train Accuracy: 0.5227 Val Loss: 0.0654 Val Accuracy: 0.5088\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  0.0766\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  0.0558\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  0.0624\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  0.0640\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  0.0497\n",
            "End of Epoch: 5 Train Accuracy: 0.6082 Val Loss: 0.0543 Val Accuracy: 0.6002\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  0.0695\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  0.0513\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  0.0502\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  0.0502\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  0.0523\n",
            "End of Epoch: 6 Train Accuracy: 0.6159 Val Loss: 0.0529 Val Accuracy: 0.6063\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  0.0533\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  0.0487\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  0.0512\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  0.0474\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  0.0494\n",
            "End of Epoch: 7 Train Accuracy: 0.6192 Val Loss: 0.0523 Val Accuracy: 0.6090\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  0.0552\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  0.0510\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  0.0511\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  0.0501\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  0.0464\n",
            "End of Epoch: 8 Train Accuracy: 0.6243 Val Loss: 0.0515 Val Accuracy: 0.6132\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  0.0471\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  0.0530\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  0.0524\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  0.0422\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  0.0407\n",
            "End of Epoch: 9 Train Accuracy: 0.6284 Val Loss: 0.0511 Val Accuracy: 0.6180\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  0.0386\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  0.0601\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  0.0571\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  0.0543\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  0.0456\n",
            "End of Epoch: 10 Train Accuracy: 0.6316 Val Loss: 0.0508 Val Accuracy: 0.6195\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▄▆▆▇██████</td></tr><tr><td>Train Loss</td><td>█▅▄▃▃▂▁▃▂▂</td></tr><tr><td>Val Accuracy</td><td>▁▄▆▆▇██████</td></tr><tr><td>Val Loss</td><td>█▄▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.63159</td></tr><tr><td>Train Loss</td><td>0.04445</td></tr><tr><td>Val Accuracy</td><td>0.6195</td></tr><tr><td>Val Loss</td><td>0.0508</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nadam|loss_mse|lr=0.0001|batch_64|act_sigmoid|hid_3|neurons_128|nrns_10|init_Random622</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/lrhbvpj0' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/lrhbvpj0</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_173953-lrhbvpj0/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: u781zlac with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_174154-u781zlac</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/u781zlac' target=\"_blank\">glorious-sweep-51</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/u781zlac' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/u781zlac</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.1004 Val Loss: 0.0913 Val Accuracy: 0.0967\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  0.0896\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  0.0776\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  0.0538\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  0.0399\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  0.0413\n",
            "End of Epoch: 1 Train Accuracy: 0.6949 Val Loss: 0.0410 Val Accuracy: 0.7040\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  0.0400\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  0.0372\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  0.0359\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  0.0338\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  0.0269\n",
            "End of Epoch: 2 Train Accuracy: 0.7627 Val Loss: 0.0326 Val Accuracy: 0.7692\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  0.0409\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  0.0351\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  0.0328\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  0.0332\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  0.0279\n",
            "End of Epoch: 3 Train Accuracy: 0.7992 Val Loss: 0.0273 Val Accuracy: 0.8083\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  0.0267\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  0.0175\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  0.0205\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  0.0325\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  0.0240\n",
            "End of Epoch: 4 Train Accuracy: 0.8118 Val Loss: 0.0255 Val Accuracy: 0.8207\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  0.0209\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  0.0243\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  0.0178\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  0.0248\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  0.0324\n",
            "End of Epoch: 5 Train Accuracy: 0.8313 Val Loss: 0.0239 Val Accuracy: 0.8368\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  0.0207\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  0.0216\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  0.0255\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  0.0266\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  0.0266\n",
            "End of Epoch: 6 Train Accuracy: 0.8459 Val Loss: 0.0224 Val Accuracy: 0.8473\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  0.0232\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  0.0221\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  0.0257\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  0.0159\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  0.0155\n",
            "End of Epoch: 7 Train Accuracy: 0.8564 Val Loss: 0.0212 Val Accuracy: 0.8583\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  0.0304\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  0.0211\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  0.0170\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  0.0212\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  0.0131\n",
            "End of Epoch: 8 Train Accuracy: 0.8530 Val Loss: 0.0212 Val Accuracy: 0.8568\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  0.0191\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  0.0183\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  0.0292\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  0.0216\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  0.0220\n",
            "End of Epoch: 9 Train Accuracy: 0.8629 Val Loss: 0.0200 Val Accuracy: 0.8655\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  0.0153\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  0.0175\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  0.0244\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  0.0128\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  0.0169\n",
            "End of Epoch: 10 Train Accuracy: 0.8701 Val Loss: 0.0193 Val Accuracy: 0.8698\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▆▇▇▇██████</td></tr><tr><td>Train Loss</td><td>█▆▅▄▃▂▄▂▁▅</td></tr><tr><td>Val Accuracy</td><td>▁▆▇▇███████</td></tr><tr><td>Val Loss</td><td>█▃▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.87015</td></tr><tr><td>Train Loss</td><td>0.02901</td></tr><tr><td>Val Accuracy</td><td>0.86983</td></tr><tr><td>Val Loss</td><td>0.01934</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_adam|loss_mse|lr=0.001|batch_64|act_sigmoid|hid_4|neurons_128|nrns_10|init_Xavier34</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/u781zlac' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/u781zlac</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_174154-u781zlac/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: q5ew3qhf with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_174404-q5ew3qhf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/q5ew3qhf' target=\"_blank\">silvery-sweep-52</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/q5ew3qhf' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/q5ew3qhf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.0774 Val Loss: 2.3268 Val Accuracy: 0.0792\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/1687): \tTrain Loss  2.3033\n",
            "Epoch(1/5)\t Batch(200/1687): \tTrain Loss  2.3356\n",
            "Epoch(1/5)\t Batch(400/1687): \tTrain Loss  2.2918\n",
            "Epoch(1/5)\t Batch(600/1687): \tTrain Loss  2.3043\n",
            "Epoch(1/5)\t Batch(800/1687): \tTrain Loss  2.3111\n",
            "Epoch(1/5)\t Batch(1000/1687): \tTrain Loss  2.3150\n",
            "Epoch(1/5)\t Batch(1200/1687): \tTrain Loss  2.2853\n",
            "Epoch(1/5)\t Batch(1400/1687): \tTrain Loss  2.2648\n",
            "Epoch(1/5)\t Batch(1600/1687): \tTrain Loss  2.2906\n",
            "End of Epoch: 1 Train Accuracy: 0.1873 Val Loss: 2.2797 Val Accuracy: 0.1802\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/1687): \tTrain Loss  2.2762\n",
            "Epoch(2/5)\t Batch(200/1687): \tTrain Loss  2.2660\n",
            "Epoch(2/5)\t Batch(400/1687): \tTrain Loss  2.2902\n",
            "Epoch(2/5)\t Batch(600/1687): \tTrain Loss  2.2661\n",
            "Epoch(2/5)\t Batch(800/1687): \tTrain Loss  2.2500\n",
            "Epoch(2/5)\t Batch(1000/1687): \tTrain Loss  2.2510\n",
            "Epoch(2/5)\t Batch(1200/1687): \tTrain Loss  2.2471\n",
            "Epoch(2/5)\t Batch(1400/1687): \tTrain Loss  2.2524\n",
            "Epoch(2/5)\t Batch(1600/1687): \tTrain Loss  2.2475\n",
            "End of Epoch: 2 Train Accuracy: 0.2902 Val Loss: 2.2466 Val Accuracy: 0.2813\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/1687): \tTrain Loss  2.2532\n",
            "Epoch(3/5)\t Batch(200/1687): \tTrain Loss  2.2419\n",
            "Epoch(3/5)\t Batch(400/1687): \tTrain Loss  2.2338\n",
            "Epoch(3/5)\t Batch(600/1687): \tTrain Loss  2.2350\n",
            "Epoch(3/5)\t Batch(800/1687): \tTrain Loss  2.2200\n",
            "Epoch(3/5)\t Batch(1000/1687): \tTrain Loss  2.2313\n",
            "Epoch(3/5)\t Batch(1200/1687): \tTrain Loss  2.2286\n",
            "Epoch(3/5)\t Batch(1400/1687): \tTrain Loss  2.2269\n",
            "Epoch(3/5)\t Batch(1600/1687): \tTrain Loss  2.2508\n",
            "End of Epoch: 3 Train Accuracy: 0.3971 Val Loss: 2.2178 Val Accuracy: 0.3875\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/1687): \tTrain Loss  2.2196\n",
            "Epoch(4/5)\t Batch(200/1687): \tTrain Loss  2.2234\n",
            "Epoch(4/5)\t Batch(400/1687): \tTrain Loss  2.2057\n",
            "Epoch(4/5)\t Batch(600/1687): \tTrain Loss  2.2340\n",
            "Epoch(4/5)\t Batch(800/1687): \tTrain Loss  2.2205\n",
            "Epoch(4/5)\t Batch(1000/1687): \tTrain Loss  2.1727\n",
            "Epoch(4/5)\t Batch(1200/1687): \tTrain Loss  2.1934\n",
            "Epoch(4/5)\t Batch(1400/1687): \tTrain Loss  2.1772\n",
            "Epoch(4/5)\t Batch(1600/1687): \tTrain Loss  2.1579\n",
            "End of Epoch: 4 Train Accuracy: 0.4758 Val Loss: 2.1924 Val Accuracy: 0.4677\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/1687): \tTrain Loss  2.1600\n",
            "Epoch(5/5)\t Batch(200/1687): \tTrain Loss  2.2087\n",
            "Epoch(5/5)\t Batch(400/1687): \tTrain Loss  2.1723\n",
            "Epoch(5/5)\t Batch(600/1687): \tTrain Loss  2.1619\n",
            "Epoch(5/5)\t Batch(800/1687): \tTrain Loss  2.1731\n",
            "Epoch(5/5)\t Batch(1000/1687): \tTrain Loss  2.1641\n",
            "Epoch(5/5)\t Batch(1200/1687): \tTrain Loss  2.1890\n",
            "Epoch(5/5)\t Batch(1400/1687): \tTrain Loss  2.1845\n",
            "Epoch(5/5)\t Batch(1600/1687): \tTrain Loss  2.1530\n",
            "End of Epoch: 5 Train Accuracy: 0.5306 Val Loss: 2.1697 Val Accuracy: 0.5235\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▃▄▆▇█</td></tr><tr><td>Train Loss</td><td>██▅▃▁</td></tr><tr><td>Val Accuracy</td><td>▁▃▄▆▇█</td></tr><tr><td>Val Loss</td><td>█▆▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.53056</td></tr><tr><td>Train Loss</td><td>2.1228</td></tr><tr><td>Val Accuracy</td><td>0.5235</td></tr><tr><td>Val Loss</td><td>2.16975</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_sgd|loss_cross_entropy|lr=0.0001|batch_32|act_tanh|hid_4|neurons_128|nrns_5|init_Xavier3</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/q5ew3qhf' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/q5ew3qhf</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_174404-q5ew3qhf/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6lnxgjo7 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_174515-6lnxgjo7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/6lnxgjo7' target=\"_blank\">unique-sweep-53</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/6lnxgjo7' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/6lnxgjo7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.1090 Val Loss: 8.3234 Val Accuracy: 0.1128\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  8.4978\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  7.1309\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  6.5805\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  5.4116\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  4.6524\n",
            "End of Epoch: 1 Train Accuracy: 0.0859 Val Loss: 4.2413 Val Accuracy: 0.0795\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  4.8623\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  4.1322\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  3.4758\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  3.4311\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  2.8033\n",
            "End of Epoch: 2 Train Accuracy: 0.1082 Val Loss: 2.9359 Val Accuracy: 0.1103\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  2.4512\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  2.6393\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  2.6562\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  2.7625\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  2.6049\n",
            "End of Epoch: 3 Train Accuracy: 0.1095 Val Loss: 2.5803 Val Accuracy: 0.1148\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  2.4810\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  2.4937\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  2.4497\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  2.5331\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  2.5502\n",
            "End of Epoch: 4 Train Accuracy: 0.1121 Val Loss: 2.4547 Val Accuracy: 0.1128\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  2.6602\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  2.5880\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  2.5711\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  2.2716\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  2.2770\n",
            "End of Epoch: 5 Train Accuracy: 0.1094 Val Loss: 2.3961 Val Accuracy: 0.1125\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  2.4492\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  2.2899\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  2.3886\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  2.3517\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  2.4013\n",
            "End of Epoch: 6 Train Accuracy: 0.1204 Val Loss: 2.3688 Val Accuracy: 0.1192\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  2.3256\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  2.3646\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  2.3102\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  2.3093\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  2.2987\n",
            "End of Epoch: 7 Train Accuracy: 0.1214 Val Loss: 2.3518 Val Accuracy: 0.1218\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  2.3250\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  2.2570\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  2.4274\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  2.4570\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  2.3300\n",
            "End of Epoch: 8 Train Accuracy: 0.1266 Val Loss: 2.3379 Val Accuracy: 0.1245\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  2.4862\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  2.3494\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  2.2818\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  2.3566\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  2.3594\n",
            "End of Epoch: 9 Train Accuracy: 0.1272 Val Loss: 2.3269 Val Accuracy: 0.1255\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  2.3016\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  2.2858\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  2.4131\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  2.3033\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  2.3145\n",
            "End of Epoch: 10 Train Accuracy: 0.1335 Val Loss: 2.3186 Val Accuracy: 0.1307\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▄▁▄▄▅▄▆▆▇▇█</td></tr><tr><td>Train Loss</td><td>█▂▃▂▁▁▁▁▁▁</td></tr><tr><td>Val Accuracy</td><td>▆▁▅▆▆▆▆▇▇▇█</td></tr><tr><td>Val Loss</td><td>█▃▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.1335</td></tr><tr><td>Train Loss</td><td>2.37253</td></tr><tr><td>Val Accuracy</td><td>0.13067</td></tr><tr><td>Val Loss</td><td>2.3186</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_sgd|loss_cross_entropy|lr=0.0001|batch_64|act_tanh|hid_4|neurons_32|nrns_10|init_Random11</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/6lnxgjo7' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/6lnxgjo7</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_174515-6lnxgjo7/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: li5t2zc1 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_174555-li5t2zc1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/li5t2zc1' target=\"_blank\">peach-sweep-54</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/li5t2zc1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/li5t2zc1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0897 Val Loss: 25.1774 Val Accuracy: 0.0887\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  25.4723\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  25.0406\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  23.7454\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  23.7454\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  24.6089\n",
            "End of Epoch: 1 Train Accuracy: 0.1597 Val Loss: 23.3438 Val Accuracy: 0.1548\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  22.0670\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  24.6089\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  22.0185\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  20.2915\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  22.8819\n",
            "End of Epoch: 2 Train Accuracy: 0.1619 Val Loss: 23.3335 Val Accuracy: 0.1552\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  20.2915\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  22.4502\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  22.8819\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  24.1771\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  24.1771\n",
            "End of Epoch: 3 Train Accuracy: 0.1500 Val Loss: 23.6230 Val Accuracy: 0.1442\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  22.8819\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  22.4502\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  22.8819\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  22.0185\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  23.3137\n",
            "End of Epoch: 4 Train Accuracy: 0.1390 Val Loss: 23.8869 Val Accuracy: 0.1342\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  24.6089\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  23.7454\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  24.1771\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  24.3911\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  24.1771\n",
            "End of Epoch: 5 Train Accuracy: 0.1287 Val Loss: 24.0220 Val Accuracy: 0.1290\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  24.9026\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  25.4723\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  23.3137\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  23.3137\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  23.4077\n",
            "End of Epoch: 6 Train Accuracy: 0.1208 Val Loss: 24.1605 Val Accuracy: 0.1245\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  25.0406\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  23.5891\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  23.7454\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  22.4502\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  26.3358\n",
            "End of Epoch: 7 Train Accuracy: 0.1165 Val Loss: 24.2949 Val Accuracy: 0.1187\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  25.9942\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  24.1538\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  23.1141\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  23.3137\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  23.9314\n",
            "End of Epoch: 8 Train Accuracy: 0.1108 Val Loss: 24.4954 Val Accuracy: 0.1112\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  25.0406\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  25.4723\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  23.7454\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  25.4723\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  25.0406\n",
            "End of Epoch: 9 Train Accuracy: 0.1069 Val Loss: 24.5261 Val Accuracy: 0.1083\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  24.3078\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  26.7676\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  25.4723\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  24.9277\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  25.9041\n",
            "End of Epoch: 10 Train Accuracy: 0.1030 Val Loss: 24.6553 Val Accuracy: 0.1027\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁██▇▆▅▄▄▃▃▂</td></tr><tr><td>Train Loss</td><td>▅▁▁▃█▇█▆█▄</td></tr><tr><td>Val Accuracy</td><td>▁██▇▆▅▅▄▃▃▂</td></tr><tr><td>Val Loss</td><td>█▁▁▂▃▄▄▅▅▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.10304</td></tr><tr><td>Train Loss</td><td>23.02588</td></tr><tr><td>Val Accuracy</td><td>0.10267</td></tr><tr><td>Val Loss</td><td>24.65535</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_adam|loss_cross_entropy|lr=0.0001|batch_64|act_relu|hid_5|neurons_64|nrns_10|init_Random780</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/li5t2zc1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/li5t2zc1</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_174555-li5t2zc1/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: eqvgzlnl with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_174655-eqvgzlnl</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/eqvgzlnl' target=\"_blank\">summer-sweep-55</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/eqvgzlnl' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/eqvgzlnl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.1003 Val Loss: 2.3784 Val Accuracy: 0.0975\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/1687): \tTrain Loss  2.4125\n",
            "Epoch(1/5)\t Batch(200/1687): \tTrain Loss  2.3029\n",
            "Epoch(1/5)\t Batch(400/1687): \tTrain Loss  2.2857\n",
            "Epoch(1/5)\t Batch(600/1687): \tTrain Loss  2.2071\n",
            "Epoch(1/5)\t Batch(800/1687): \tTrain Loss  2.1648\n",
            "Epoch(1/5)\t Batch(1000/1687): \tTrain Loss  2.1092\n",
            "Epoch(1/5)\t Batch(1200/1687): \tTrain Loss  1.9989\n",
            "Epoch(1/5)\t Batch(1400/1687): \tTrain Loss  1.9863\n",
            "Epoch(1/5)\t Batch(1600/1687): \tTrain Loss  1.9277\n",
            "End of Epoch: 1 Train Accuracy: 0.6385 Val Loss: 1.9425 Val Accuracy: 0.6458\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/1687): \tTrain Loss  1.9661\n",
            "Epoch(2/5)\t Batch(200/1687): \tTrain Loss  1.9126\n",
            "Epoch(2/5)\t Batch(400/1687): \tTrain Loss  1.8847\n",
            "Epoch(2/5)\t Batch(600/1687): \tTrain Loss  1.8556\n",
            "Epoch(2/5)\t Batch(800/1687): \tTrain Loss  1.7752\n",
            "Epoch(2/5)\t Batch(1000/1687): \tTrain Loss  1.8378\n",
            "Epoch(2/5)\t Batch(1200/1687): \tTrain Loss  1.7977\n",
            "Epoch(2/5)\t Batch(1400/1687): \tTrain Loss  1.7792\n",
            "Epoch(2/5)\t Batch(1600/1687): \tTrain Loss  1.7855\n",
            "End of Epoch: 2 Train Accuracy: 0.7585 Val Loss: 1.8077 Val Accuracy: 0.7548\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/1687): \tTrain Loss  1.7910\n",
            "Epoch(3/5)\t Batch(200/1687): \tTrain Loss  1.8102\n",
            "Epoch(3/5)\t Batch(400/1687): \tTrain Loss  1.6750\n",
            "Epoch(3/5)\t Batch(600/1687): \tTrain Loss  1.7685\n",
            "Epoch(3/5)\t Batch(800/1687): \tTrain Loss  1.6877\n",
            "Epoch(3/5)\t Batch(1000/1687): \tTrain Loss  1.7279\n",
            "Epoch(3/5)\t Batch(1200/1687): \tTrain Loss  1.6936\n",
            "Epoch(3/5)\t Batch(1400/1687): \tTrain Loss  1.5804\n",
            "Epoch(3/5)\t Batch(1600/1687): \tTrain Loss  1.6561\n",
            "End of Epoch: 3 Train Accuracy: 0.7947 Val Loss: 1.7298 Val Accuracy: 0.7878\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/1687): \tTrain Loss  1.7876\n",
            "Epoch(4/5)\t Batch(200/1687): \tTrain Loss  1.6828\n",
            "Epoch(4/5)\t Batch(400/1687): \tTrain Loss  1.6765\n",
            "Epoch(4/5)\t Batch(600/1687): \tTrain Loss  1.7365\n",
            "Epoch(4/5)\t Batch(800/1687): \tTrain Loss  1.6654\n",
            "Epoch(4/5)\t Batch(1000/1687): \tTrain Loss  1.6650\n",
            "Epoch(4/5)\t Batch(1200/1687): \tTrain Loss  1.6570\n",
            "Epoch(4/5)\t Batch(1400/1687): \tTrain Loss  1.6694\n",
            "Epoch(4/5)\t Batch(1600/1687): \tTrain Loss  1.7112\n",
            "End of Epoch: 4 Train Accuracy: 0.8375 Val Loss: 1.6891 Val Accuracy: 0.8322\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/1687): \tTrain Loss  1.6534\n",
            "Epoch(5/5)\t Batch(200/1687): \tTrain Loss  1.6503\n",
            "Epoch(5/5)\t Batch(400/1687): \tTrain Loss  1.7205\n",
            "Epoch(5/5)\t Batch(600/1687): \tTrain Loss  1.6438\n",
            "Epoch(5/5)\t Batch(800/1687): \tTrain Loss  1.6562\n",
            "Epoch(5/5)\t Batch(1000/1687): \tTrain Loss  1.6098\n",
            "Epoch(5/5)\t Batch(1200/1687): \tTrain Loss  1.6323\n",
            "Epoch(5/5)\t Batch(1400/1687): \tTrain Loss  1.6558\n",
            "Epoch(5/5)\t Batch(1600/1687): \tTrain Loss  1.6287\n",
            "End of Epoch: 5 Train Accuracy: 0.8632 Val Loss: 1.6480 Val Accuracy: 0.8538\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▆▇▇██</td></tr><tr><td>Train Loss</td><td>█▇▃▁▃</td></tr><tr><td>Val Accuracy</td><td>▁▆▇▇██</td></tr><tr><td>Val Loss</td><td>█▄▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.86317</td></tr><tr><td>Train Loss</td><td>1.68687</td></tr><tr><td>Val Accuracy</td><td>0.85383</td></tr><tr><td>Val Loss</td><td>1.64795</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_cross_entropy|lr=0.001|batch_32|act_sigmoid|hid_5|neurons_64|nrns_5|init_Xavier463</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/eqvgzlnl' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/eqvgzlnl</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_174655-eqvgzlnl/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vk5g4ao0 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: momentum\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_174736-vk5g4ao0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/vk5g4ao0' target=\"_blank\">azure-sweep-56</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/vk5g4ao0' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/vk5g4ao0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.0992 Val Loss: 2.3440 Val Accuracy: 0.1072\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/1687): \tTrain Loss  2.3769\n",
            "Epoch(1/10)\t Batch(200/1687): \tTrain Loss  2.3018\n",
            "Epoch(1/10)\t Batch(400/1687): \tTrain Loss  2.2992\n",
            "Epoch(1/10)\t Batch(600/1687): \tTrain Loss  2.3024\n",
            "Epoch(1/10)\t Batch(800/1687): \tTrain Loss  2.3030\n",
            "Epoch(1/10)\t Batch(1000/1687): \tTrain Loss  2.3025\n",
            "Epoch(1/10)\t Batch(1200/1687): \tTrain Loss  2.3073\n",
            "Epoch(1/10)\t Batch(1400/1687): \tTrain Loss  2.3043\n",
            "Epoch(1/10)\t Batch(1600/1687): \tTrain Loss  2.3019\n",
            "End of Epoch: 1 Train Accuracy: 0.0999 Val Loss: 2.3027 Val Accuracy: 0.1008\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/1687): \tTrain Loss  2.2998\n",
            "Epoch(2/10)\t Batch(200/1687): \tTrain Loss  2.3031\n",
            "Epoch(2/10)\t Batch(400/1687): \tTrain Loss  2.3073\n",
            "Epoch(2/10)\t Batch(600/1687): \tTrain Loss  2.3033\n",
            "Epoch(2/10)\t Batch(800/1687): \tTrain Loss  2.2997\n",
            "Epoch(2/10)\t Batch(1000/1687): \tTrain Loss  2.2975\n",
            "Epoch(2/10)\t Batch(1200/1687): \tTrain Loss  2.3032\n",
            "Epoch(2/10)\t Batch(1400/1687): \tTrain Loss  2.3079\n",
            "Epoch(2/10)\t Batch(1600/1687): \tTrain Loss  2.3043\n",
            "End of Epoch: 2 Train Accuracy: 0.0997 Val Loss: 2.3030 Val Accuracy: 0.1025\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/1687): \tTrain Loss  2.3064\n",
            "Epoch(3/10)\t Batch(200/1687): \tTrain Loss  2.3070\n",
            "Epoch(3/10)\t Batch(400/1687): \tTrain Loss  2.3033\n",
            "Epoch(3/10)\t Batch(600/1687): \tTrain Loss  2.3013\n",
            "Epoch(3/10)\t Batch(800/1687): \tTrain Loss  2.3045\n",
            "Epoch(3/10)\t Batch(1000/1687): \tTrain Loss  2.3030\n",
            "Epoch(3/10)\t Batch(1200/1687): \tTrain Loss  2.3088\n",
            "Epoch(3/10)\t Batch(1400/1687): \tTrain Loss  2.3026\n",
            "Epoch(3/10)\t Batch(1600/1687): \tTrain Loss  2.2973\n",
            "End of Epoch: 3 Train Accuracy: 0.1005 Val Loss: 2.3029 Val Accuracy: 0.0957\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/1687): \tTrain Loss  2.3029\n",
            "Epoch(4/10)\t Batch(200/1687): \tTrain Loss  2.3010\n",
            "Epoch(4/10)\t Batch(400/1687): \tTrain Loss  2.3017\n",
            "Epoch(4/10)\t Batch(600/1687): \tTrain Loss  2.3044\n",
            "Epoch(4/10)\t Batch(800/1687): \tTrain Loss  2.3053\n",
            "Epoch(4/10)\t Batch(1000/1687): \tTrain Loss  2.3027\n",
            "Epoch(4/10)\t Batch(1200/1687): \tTrain Loss  2.3034\n",
            "Epoch(4/10)\t Batch(1400/1687): \tTrain Loss  2.3001\n",
            "Epoch(4/10)\t Batch(1600/1687): \tTrain Loss  2.2992\n",
            "End of Epoch: 4 Train Accuracy: 0.1006 Val Loss: 2.3028 Val Accuracy: 0.0948\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/1687): \tTrain Loss  2.3029\n",
            "Epoch(5/10)\t Batch(200/1687): \tTrain Loss  2.3021\n",
            "Epoch(5/10)\t Batch(400/1687): \tTrain Loss  2.3018\n",
            "Epoch(5/10)\t Batch(600/1687): \tTrain Loss  2.3077\n",
            "Epoch(5/10)\t Batch(800/1687): \tTrain Loss  2.3046\n",
            "Epoch(5/10)\t Batch(1000/1687): \tTrain Loss  2.3007\n",
            "Epoch(5/10)\t Batch(1200/1687): \tTrain Loss  2.3014\n",
            "Epoch(5/10)\t Batch(1400/1687): \tTrain Loss  2.3029\n",
            "Epoch(5/10)\t Batch(1600/1687): \tTrain Loss  2.3023\n",
            "End of Epoch: 5 Train Accuracy: 0.0992 Val Loss: 2.3026 Val Accuracy: 0.1072\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/1687): \tTrain Loss  2.3024\n",
            "Epoch(6/10)\t Batch(200/1687): \tTrain Loss  2.3035\n",
            "Epoch(6/10)\t Batch(400/1687): \tTrain Loss  2.2999\n",
            "Epoch(6/10)\t Batch(600/1687): \tTrain Loss  2.3028\n",
            "Epoch(6/10)\t Batch(800/1687): \tTrain Loss  2.3010\n",
            "Epoch(6/10)\t Batch(1000/1687): \tTrain Loss  2.2981\n",
            "Epoch(6/10)\t Batch(1200/1687): \tTrain Loss  2.3017\n",
            "Epoch(6/10)\t Batch(1400/1687): \tTrain Loss  2.3040\n",
            "Epoch(6/10)\t Batch(1600/1687): \tTrain Loss  2.3026\n",
            "End of Epoch: 6 Train Accuracy: 0.0992 Val Loss: 2.3024 Val Accuracy: 0.1072\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/1687): \tTrain Loss  2.2963\n",
            "Epoch(7/10)\t Batch(200/1687): \tTrain Loss  2.3026\n",
            "Epoch(7/10)\t Batch(400/1687): \tTrain Loss  2.3038\n",
            "Epoch(7/10)\t Batch(600/1687): \tTrain Loss  2.3037\n",
            "Epoch(7/10)\t Batch(800/1687): \tTrain Loss  2.3009\n",
            "Epoch(7/10)\t Batch(1000/1687): \tTrain Loss  2.3023\n",
            "Epoch(7/10)\t Batch(1200/1687): \tTrain Loss  2.3035\n",
            "Epoch(7/10)\t Batch(1400/1687): \tTrain Loss  2.3010\n",
            "Epoch(7/10)\t Batch(1600/1687): \tTrain Loss  2.3007\n",
            "End of Epoch: 7 Train Accuracy: 0.1009 Val Loss: 2.3029 Val Accuracy: 0.0915\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/1687): \tTrain Loss  2.3024\n",
            "Epoch(8/10)\t Batch(200/1687): \tTrain Loss  2.3028\n",
            "Epoch(8/10)\t Batch(400/1687): \tTrain Loss  2.3054\n",
            "Epoch(8/10)\t Batch(600/1687): \tTrain Loss  2.3006\n",
            "Epoch(8/10)\t Batch(800/1687): \tTrain Loss  2.3051\n",
            "Epoch(8/10)\t Batch(1000/1687): \tTrain Loss  2.3033\n",
            "Epoch(8/10)\t Batch(1200/1687): \tTrain Loss  2.3052\n",
            "Epoch(8/10)\t Batch(1400/1687): \tTrain Loss  2.3020\n",
            "Epoch(8/10)\t Batch(1600/1687): \tTrain Loss  2.2984\n",
            "End of Epoch: 8 Train Accuracy: 0.0995 Val Loss: 2.3027 Val Accuracy: 0.1042\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/1687): \tTrain Loss  2.3027\n",
            "Epoch(9/10)\t Batch(200/1687): \tTrain Loss  2.3052\n",
            "Epoch(9/10)\t Batch(400/1687): \tTrain Loss  2.3020\n",
            "Epoch(9/10)\t Batch(600/1687): \tTrain Loss  2.2997\n",
            "Epoch(9/10)\t Batch(800/1687): \tTrain Loss  2.3000\n",
            "Epoch(9/10)\t Batch(1000/1687): \tTrain Loss  2.3055\n",
            "Epoch(9/10)\t Batch(1200/1687): \tTrain Loss  2.3030\n",
            "Epoch(9/10)\t Batch(1400/1687): \tTrain Loss  2.3038\n",
            "Epoch(9/10)\t Batch(1600/1687): \tTrain Loss  2.3013\n",
            "End of Epoch: 9 Train Accuracy: 0.1006 Val Loss: 2.3029 Val Accuracy: 0.0948\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/1687): \tTrain Loss  2.3044\n",
            "Epoch(10/10)\t Batch(200/1687): \tTrain Loss  2.3035\n",
            "Epoch(10/10)\t Batch(400/1687): \tTrain Loss  2.3018\n",
            "Epoch(10/10)\t Batch(600/1687): \tTrain Loss  2.3049\n",
            "Epoch(10/10)\t Batch(800/1687): \tTrain Loss  2.2992\n",
            "Epoch(10/10)\t Batch(1000/1687): \tTrain Loss  2.3015\n",
            "Epoch(10/10)\t Batch(1200/1687): \tTrain Loss  2.3041\n",
            "Epoch(10/10)\t Batch(1400/1687): \tTrain Loss  2.2996\n",
            "Epoch(10/10)\t Batch(1600/1687): \tTrain Loss  2.3041\n",
            "End of Epoch: 10 Train Accuracy: 0.0995 Val Loss: 2.3026 Val Accuracy: 0.1042\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▄▃▆▇▁▁█▂▇▂</td></tr><tr><td>Train Loss</td><td>▁▆▄▆▆▂▅█▄█</td></tr><tr><td>Val Accuracy</td><td>█▅▆▃▂██▁▇▂▇</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.09954</td></tr><tr><td>Train Loss</td><td>2.30663</td></tr><tr><td>Val Accuracy</td><td>0.10417</td></tr><tr><td>Val Loss</td><td>2.30265</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_momentum|loss_cross_entropy|lr=0.001|batch_32|act_sigmoid|hid_5|neurons_64|nrns_10|init_Xavier248</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/vk5g4ao0' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/vk5g4ao0</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_174736-vk5g4ao0/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c0kudi4r with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_174847-c0kudi4r</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/c0kudi4r' target=\"_blank\">comic-sweep-57</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/c0kudi4r' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/c0kudi4r</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.0401 Val Loss: 0.0906 Val Accuracy: 0.0363\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/1687): \tTrain Loss  0.0906\n",
            "Epoch(1/10)\t Batch(200/1687): \tTrain Loss  0.0612\n",
            "Epoch(1/10)\t Batch(400/1687): \tTrain Loss  0.0539\n",
            "Epoch(1/10)\t Batch(600/1687): \tTrain Loss  0.0535\n",
            "Epoch(1/10)\t Batch(800/1687): \tTrain Loss  0.0491\n",
            "Epoch(1/10)\t Batch(1000/1687): \tTrain Loss  0.0431\n",
            "Epoch(1/10)\t Batch(1200/1687): \tTrain Loss  0.0370\n",
            "Epoch(1/10)\t Batch(1400/1687): \tTrain Loss  0.0462\n",
            "Epoch(1/10)\t Batch(1600/1687): \tTrain Loss  0.0326\n",
            "End of Epoch: 1 Train Accuracy: 0.7660 Val Loss: 0.0367 Val Accuracy: 0.7577\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/1687): \tTrain Loss  0.0413\n",
            "Epoch(2/10)\t Batch(200/1687): \tTrain Loss  0.0239\n",
            "Epoch(2/10)\t Batch(400/1687): \tTrain Loss  0.0340\n",
            "Epoch(2/10)\t Batch(600/1687): \tTrain Loss  0.0282\n",
            "Epoch(2/10)\t Batch(800/1687): \tTrain Loss  0.0381\n",
            "Epoch(2/10)\t Batch(1000/1687): \tTrain Loss  0.0343\n",
            "Epoch(2/10)\t Batch(1200/1687): \tTrain Loss  0.0376\n",
            "Epoch(2/10)\t Batch(1400/1687): \tTrain Loss  0.0287\n",
            "Epoch(2/10)\t Batch(1600/1687): \tTrain Loss  0.0356\n",
            "End of Epoch: 2 Train Accuracy: 0.8079 Val Loss: 0.0299 Val Accuracy: 0.8015\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/1687): \tTrain Loss  0.0370\n",
            "Epoch(3/10)\t Batch(200/1687): \tTrain Loss  0.0140\n",
            "Epoch(3/10)\t Batch(400/1687): \tTrain Loss  0.0199\n",
            "Epoch(3/10)\t Batch(600/1687): \tTrain Loss  0.0304\n",
            "Epoch(3/10)\t Batch(800/1687): \tTrain Loss  0.0240\n",
            "Epoch(3/10)\t Batch(1000/1687): \tTrain Loss  0.0192\n",
            "Epoch(3/10)\t Batch(1200/1687): \tTrain Loss  0.0337\n",
            "Epoch(3/10)\t Batch(1400/1687): \tTrain Loss  0.0395\n",
            "Epoch(3/10)\t Batch(1600/1687): \tTrain Loss  0.0378\n",
            "End of Epoch: 3 Train Accuracy: 0.8245 Val Loss: 0.0268 Val Accuracy: 0.8225\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/1687): \tTrain Loss  0.0258\n",
            "Epoch(4/10)\t Batch(200/1687): \tTrain Loss  0.0238\n",
            "Epoch(4/10)\t Batch(400/1687): \tTrain Loss  0.0372\n",
            "Epoch(4/10)\t Batch(600/1687): \tTrain Loss  0.0318\n",
            "Epoch(4/10)\t Batch(800/1687): \tTrain Loss  0.0220\n",
            "Epoch(4/10)\t Batch(1000/1687): \tTrain Loss  0.0232\n",
            "Epoch(4/10)\t Batch(1200/1687): \tTrain Loss  0.0229\n",
            "Epoch(4/10)\t Batch(1400/1687): \tTrain Loss  0.0258\n",
            "Epoch(4/10)\t Batch(1600/1687): \tTrain Loss  0.0372\n",
            "End of Epoch: 4 Train Accuracy: 0.8332 Val Loss: 0.0252 Val Accuracy: 0.8323\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/1687): \tTrain Loss  0.0193\n",
            "Epoch(5/10)\t Batch(200/1687): \tTrain Loss  0.0114\n",
            "Epoch(5/10)\t Batch(400/1687): \tTrain Loss  0.0251\n",
            "Epoch(5/10)\t Batch(600/1687): \tTrain Loss  0.0172\n",
            "Epoch(5/10)\t Batch(800/1687): \tTrain Loss  0.0139\n",
            "Epoch(5/10)\t Batch(1000/1687): \tTrain Loss  0.0199\n",
            "Epoch(5/10)\t Batch(1200/1687): \tTrain Loss  0.0277\n",
            "Epoch(5/10)\t Batch(1400/1687): \tTrain Loss  0.0164\n",
            "Epoch(5/10)\t Batch(1600/1687): \tTrain Loss  0.0238\n",
            "End of Epoch: 5 Train Accuracy: 0.8397 Val Loss: 0.0242 Val Accuracy: 0.8347\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/1687): \tTrain Loss  0.0235\n",
            "Epoch(6/10)\t Batch(200/1687): \tTrain Loss  0.0112\n",
            "Epoch(6/10)\t Batch(400/1687): \tTrain Loss  0.0222\n",
            "Epoch(6/10)\t Batch(600/1687): \tTrain Loss  0.0435\n",
            "Epoch(6/10)\t Batch(800/1687): \tTrain Loss  0.0273\n",
            "Epoch(6/10)\t Batch(1000/1687): \tTrain Loss  0.0289\n",
            "Epoch(6/10)\t Batch(1200/1687): \tTrain Loss  0.0196\n",
            "Epoch(6/10)\t Batch(1400/1687): \tTrain Loss  0.0194\n",
            "Epoch(6/10)\t Batch(1600/1687): \tTrain Loss  0.0210\n",
            "End of Epoch: 6 Train Accuracy: 0.8472 Val Loss: 0.0234 Val Accuracy: 0.8420\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/1687): \tTrain Loss  0.0200\n",
            "Epoch(7/10)\t Batch(200/1687): \tTrain Loss  0.0237\n",
            "Epoch(7/10)\t Batch(400/1687): \tTrain Loss  0.0244\n",
            "Epoch(7/10)\t Batch(600/1687): \tTrain Loss  0.0116\n",
            "Epoch(7/10)\t Batch(800/1687): \tTrain Loss  0.0318\n",
            "Epoch(7/10)\t Batch(1000/1687): \tTrain Loss  0.0266\n",
            "Epoch(7/10)\t Batch(1200/1687): \tTrain Loss  0.0195\n",
            "Epoch(7/10)\t Batch(1400/1687): \tTrain Loss  0.0340\n",
            "Epoch(7/10)\t Batch(1600/1687): \tTrain Loss  0.0168\n",
            "End of Epoch: 7 Train Accuracy: 0.8508 Val Loss: 0.0227 Val Accuracy: 0.8435\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/1687): \tTrain Loss  0.0272\n",
            "Epoch(8/10)\t Batch(200/1687): \tTrain Loss  0.0221\n",
            "Epoch(8/10)\t Batch(400/1687): \tTrain Loss  0.0170\n",
            "Epoch(8/10)\t Batch(600/1687): \tTrain Loss  0.0261\n",
            "Epoch(8/10)\t Batch(800/1687): \tTrain Loss  0.0104\n",
            "Epoch(8/10)\t Batch(1000/1687): \tTrain Loss  0.0226\n",
            "Epoch(8/10)\t Batch(1200/1687): \tTrain Loss  0.0167\n",
            "Epoch(8/10)\t Batch(1400/1687): \tTrain Loss  0.0349\n",
            "Epoch(8/10)\t Batch(1600/1687): \tTrain Loss  0.0108\n",
            "End of Epoch: 8 Train Accuracy: 0.8557 Val Loss: 0.0222 Val Accuracy: 0.8510\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/1687): \tTrain Loss  0.0210\n",
            "Epoch(9/10)\t Batch(200/1687): \tTrain Loss  0.0287\n",
            "Epoch(9/10)\t Batch(400/1687): \tTrain Loss  0.0099\n",
            "Epoch(9/10)\t Batch(600/1687): \tTrain Loss  0.0139\n",
            "Epoch(9/10)\t Batch(800/1687): \tTrain Loss  0.0118\n",
            "Epoch(9/10)\t Batch(1000/1687): \tTrain Loss  0.0271\n",
            "Epoch(9/10)\t Batch(1200/1687): \tTrain Loss  0.0136\n",
            "Epoch(9/10)\t Batch(1400/1687): \tTrain Loss  0.0354\n",
            "Epoch(9/10)\t Batch(1600/1687): \tTrain Loss  0.0247\n",
            "End of Epoch: 9 Train Accuracy: 0.8588 Val Loss: 0.0218 Val Accuracy: 0.8522\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/1687): \tTrain Loss  0.0267\n",
            "Epoch(10/10)\t Batch(200/1687): \tTrain Loss  0.0184\n",
            "Epoch(10/10)\t Batch(400/1687): \tTrain Loss  0.0235\n",
            "Epoch(10/10)\t Batch(600/1687): \tTrain Loss  0.0261\n",
            "Epoch(10/10)\t Batch(800/1687): \tTrain Loss  0.0192\n",
            "Epoch(10/10)\t Batch(1000/1687): \tTrain Loss  0.0220\n",
            "Epoch(10/10)\t Batch(1200/1687): \tTrain Loss  0.0195\n",
            "Epoch(10/10)\t Batch(1400/1687): \tTrain Loss  0.0177\n",
            "Epoch(10/10)\t Batch(1600/1687): \tTrain Loss  0.0166\n",
            "End of Epoch: 10 Train Accuracy: 0.8619 Val Loss: 0.0214 Val Accuracy: 0.8530\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▇█████████</td></tr><tr><td>Train Loss</td><td>█▇▅▄▇▁▆▁▄▄</td></tr><tr><td>Val Accuracy</td><td>▁▇█████████</td></tr><tr><td>Val Loss</td><td>█▃▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.86191</td></tr><tr><td>Train Loss</td><td>0.02467</td></tr><tr><td>Val Accuracy</td><td>0.853</td></tr><tr><td>Val Loss</td><td>0.02142</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_adam|loss_mse|lr=0.0001|batch_32|act_tanh|hid_4|neurons_64|nrns_10|init_Xavier831</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/c0kudi4r' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/c0kudi4r</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_174847-c0kudi4r/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xwy2vfho with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_175052-xwy2vfho</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/xwy2vfho' target=\"_blank\">wise-sweep-58</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/xwy2vfho' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/xwy2vfho</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.1207 Val Loss: 0.0903 Val Accuracy: 0.1152\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/3375): \tTrain Loss  0.0899\n",
            "Epoch(1/5)\t Batch(200/3375): \tTrain Loss  0.0332\n",
            "Epoch(1/5)\t Batch(400/3375): \tTrain Loss  0.0241\n",
            "Epoch(1/5)\t Batch(600/3375): \tTrain Loss  0.0414\n",
            "Epoch(1/5)\t Batch(800/3375): \tTrain Loss  0.0280\n",
            "Epoch(1/5)\t Batch(1000/3375): \tTrain Loss  0.0153\n",
            "Epoch(1/5)\t Batch(1200/3375): \tTrain Loss  0.0168\n",
            "Epoch(1/5)\t Batch(1400/3375): \tTrain Loss  0.0245\n",
            "Epoch(1/5)\t Batch(1600/3375): \tTrain Loss  0.0270\n",
            "Epoch(1/5)\t Batch(1800/3375): \tTrain Loss  0.0323\n",
            "Epoch(1/5)\t Batch(2000/3375): \tTrain Loss  0.0310\n",
            "Epoch(1/5)\t Batch(2200/3375): \tTrain Loss  0.0319\n",
            "Epoch(1/5)\t Batch(2400/3375): \tTrain Loss  0.0191\n",
            "Epoch(1/5)\t Batch(2600/3375): \tTrain Loss  0.0360\n",
            "Epoch(1/5)\t Batch(2800/3375): \tTrain Loss  0.0360\n",
            "Epoch(1/5)\t Batch(3000/3375): \tTrain Loss  0.0211\n",
            "Epoch(1/5)\t Batch(3200/3375): \tTrain Loss  0.0213\n",
            "End of Epoch: 1 Train Accuracy: 0.8600 Val Loss: 0.0211 Val Accuracy: 0.8557\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/3375): \tTrain Loss  0.0145\n",
            "Epoch(2/5)\t Batch(200/3375): \tTrain Loss  0.0035\n",
            "Epoch(2/5)\t Batch(400/3375): \tTrain Loss  0.0177\n",
            "Epoch(2/5)\t Batch(600/3375): \tTrain Loss  0.0416\n",
            "Epoch(2/5)\t Batch(800/3375): \tTrain Loss  0.0143\n",
            "Epoch(2/5)\t Batch(1000/3375): \tTrain Loss  0.0216\n",
            "Epoch(2/5)\t Batch(1200/3375): \tTrain Loss  0.0116\n",
            "Epoch(2/5)\t Batch(1400/3375): \tTrain Loss  0.0344\n",
            "Epoch(2/5)\t Batch(1600/3375): \tTrain Loss  0.0149\n",
            "Epoch(2/5)\t Batch(1800/3375): \tTrain Loss  0.0133\n",
            "Epoch(2/5)\t Batch(2000/3375): \tTrain Loss  0.0254\n",
            "Epoch(2/5)\t Batch(2200/3375): \tTrain Loss  0.0439\n",
            "Epoch(2/5)\t Batch(2400/3375): \tTrain Loss  0.0423\n",
            "Epoch(2/5)\t Batch(2600/3375): \tTrain Loss  0.0266\n",
            "Epoch(2/5)\t Batch(2800/3375): \tTrain Loss  0.0291\n",
            "Epoch(2/5)\t Batch(3000/3375): \tTrain Loss  0.0164\n",
            "Epoch(2/5)\t Batch(3200/3375): \tTrain Loss  0.0142\n",
            "End of Epoch: 2 Train Accuracy: 0.8671 Val Loss: 0.0202 Val Accuracy: 0.8675\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/3375): \tTrain Loss  0.0145\n",
            "Epoch(3/5)\t Batch(200/3375): \tTrain Loss  0.0077\n",
            "Epoch(3/5)\t Batch(400/3375): \tTrain Loss  0.0213\n",
            "Epoch(3/5)\t Batch(600/3375): \tTrain Loss  0.0213\n",
            "Epoch(3/5)\t Batch(800/3375): \tTrain Loss  0.0018\n",
            "Epoch(3/5)\t Batch(1000/3375): \tTrain Loss  0.0043\n",
            "Epoch(3/5)\t Batch(1200/3375): \tTrain Loss  0.0149\n",
            "Epoch(3/5)\t Batch(1400/3375): \tTrain Loss  0.0368\n",
            "Epoch(3/5)\t Batch(1600/3375): \tTrain Loss  0.0178\n",
            "Epoch(3/5)\t Batch(1800/3375): \tTrain Loss  0.0109\n",
            "Epoch(3/5)\t Batch(2000/3375): \tTrain Loss  0.0239\n",
            "Epoch(3/5)\t Batch(2200/3375): \tTrain Loss  0.0105\n",
            "Epoch(3/5)\t Batch(2400/3375): \tTrain Loss  0.0120\n",
            "Epoch(3/5)\t Batch(2600/3375): \tTrain Loss  0.0332\n",
            "Epoch(3/5)\t Batch(2800/3375): \tTrain Loss  0.0291\n",
            "Epoch(3/5)\t Batch(3000/3375): \tTrain Loss  0.0068\n",
            "Epoch(3/5)\t Batch(3200/3375): \tTrain Loss  0.0112\n",
            "End of Epoch: 3 Train Accuracy: 0.8705 Val Loss: 0.0196 Val Accuracy: 0.8660\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/3375): \tTrain Loss  0.0295\n",
            "Epoch(4/5)\t Batch(200/3375): \tTrain Loss  0.0151\n",
            "Epoch(4/5)\t Batch(400/3375): \tTrain Loss  0.0227\n",
            "Epoch(4/5)\t Batch(600/3375): \tTrain Loss  0.0073\n",
            "Epoch(4/5)\t Batch(800/3375): \tTrain Loss  0.0274\n",
            "Epoch(4/5)\t Batch(1000/3375): \tTrain Loss  0.0068\n",
            "Epoch(4/5)\t Batch(1200/3375): \tTrain Loss  0.0300\n",
            "Epoch(4/5)\t Batch(1400/3375): \tTrain Loss  0.0120\n",
            "Epoch(4/5)\t Batch(1600/3375): \tTrain Loss  0.0317\n",
            "Epoch(4/5)\t Batch(1800/3375): \tTrain Loss  0.0058\n",
            "Epoch(4/5)\t Batch(2000/3375): \tTrain Loss  0.0130\n",
            "Epoch(4/5)\t Batch(2200/3375): \tTrain Loss  0.0171\n",
            "Epoch(4/5)\t Batch(2400/3375): \tTrain Loss  0.0215\n",
            "Epoch(4/5)\t Batch(2600/3375): \tTrain Loss  0.0100\n",
            "Epoch(4/5)\t Batch(2800/3375): \tTrain Loss  0.0413\n",
            "Epoch(4/5)\t Batch(3000/3375): \tTrain Loss  0.0142\n",
            "Epoch(4/5)\t Batch(3200/3375): \tTrain Loss  0.0130\n",
            "End of Epoch: 4 Train Accuracy: 0.8814 Val Loss: 0.0187 Val Accuracy: 0.8728\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/3375): \tTrain Loss  0.0199\n",
            "Epoch(5/5)\t Batch(200/3375): \tTrain Loss  0.0054\n",
            "Epoch(5/5)\t Batch(400/3375): \tTrain Loss  0.0134\n",
            "Epoch(5/5)\t Batch(600/3375): \tTrain Loss  0.0040\n",
            "Epoch(5/5)\t Batch(800/3375): \tTrain Loss  0.0029\n",
            "Epoch(5/5)\t Batch(1000/3375): \tTrain Loss  0.0268\n",
            "Epoch(5/5)\t Batch(1200/3375): \tTrain Loss  0.0239\n",
            "Epoch(5/5)\t Batch(1400/3375): \tTrain Loss  0.0100\n",
            "Epoch(5/5)\t Batch(1600/3375): \tTrain Loss  0.0133\n",
            "Epoch(5/5)\t Batch(1800/3375): \tTrain Loss  0.0129\n",
            "Epoch(5/5)\t Batch(2000/3375): \tTrain Loss  0.0264\n",
            "Epoch(5/5)\t Batch(2200/3375): \tTrain Loss  0.0074\n",
            "Epoch(5/5)\t Batch(2400/3375): \tTrain Loss  0.0093\n",
            "Epoch(5/5)\t Batch(2600/3375): \tTrain Loss  0.0426\n",
            "Epoch(5/5)\t Batch(2800/3375): \tTrain Loss  0.0098\n",
            "Epoch(5/5)\t Batch(3000/3375): \tTrain Loss  0.0084\n",
            "Epoch(5/5)\t Batch(3200/3375): \tTrain Loss  0.0094\n",
            "End of Epoch: 5 Train Accuracy: 0.8829 Val Loss: 0.0187 Val Accuracy: 0.8677\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁█████</td></tr><tr><td>Train Loss</td><td>▁▂█▇▄</td></tr><tr><td>Val Accuracy</td><td>▁█████</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.88285</td></tr><tr><td>Train Loss</td><td>0.01931</td></tr><tr><td>Val Accuracy</td><td>0.86767</td></tr><tr><td>Val Loss</td><td>0.01871</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_mse|lr=0.001|batch_16|act_tanh|hid_4|neurons_64|nrns_5|init_Xavier157</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/xwy2vfho' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/xwy2vfho</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_175052-xwy2vfho/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8tjgw6eu with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_175213-8tjgw6eu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8tjgw6eu' target=\"_blank\">vibrant-sweep-59</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8tjgw6eu' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8tjgw6eu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.1269 Val Loss: 2.3180 Val Accuracy: 0.1167\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/3375): \tTrain Loss  2.3561\n",
            "Epoch(1/10)\t Batch(200/3375): \tTrain Loss  2.0121\n",
            "Epoch(1/10)\t Batch(400/3375): \tTrain Loss  1.8557\n",
            "Epoch(1/10)\t Batch(600/3375): \tTrain Loss  1.8241\n",
            "Epoch(1/10)\t Batch(800/3375): \tTrain Loss  1.8725\n",
            "Epoch(1/10)\t Batch(1000/3375): \tTrain Loss  1.8286\n",
            "Epoch(1/10)\t Batch(1200/3375): \tTrain Loss  1.7815\n",
            "Epoch(1/10)\t Batch(1400/3375): \tTrain Loss  1.8394\n",
            "Epoch(1/10)\t Batch(1600/3375): \tTrain Loss  1.7937\n",
            "Epoch(1/10)\t Batch(1800/3375): \tTrain Loss  1.6421\n",
            "Epoch(1/10)\t Batch(2000/3375): \tTrain Loss  1.8504\n",
            "Epoch(1/10)\t Batch(2200/3375): \tTrain Loss  1.6856\n",
            "Epoch(1/10)\t Batch(2400/3375): \tTrain Loss  1.7554\n",
            "Epoch(1/10)\t Batch(2600/3375): \tTrain Loss  1.8230\n",
            "Epoch(1/10)\t Batch(2800/3375): \tTrain Loss  1.7885\n",
            "Epoch(1/10)\t Batch(3000/3375): \tTrain Loss  1.7310\n",
            "Epoch(1/10)\t Batch(3200/3375): \tTrain Loss  1.7183\n",
            "End of Epoch: 1 Train Accuracy: 0.8170 Val Loss: 1.7477 Val Accuracy: 0.8173\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/3375): \tTrain Loss  1.7538\n",
            "Epoch(2/10)\t Batch(200/3375): \tTrain Loss  1.7313\n",
            "Epoch(2/10)\t Batch(400/3375): \tTrain Loss  1.7813\n",
            "Epoch(2/10)\t Batch(600/3375): \tTrain Loss  1.7086\n",
            "Epoch(2/10)\t Batch(800/3375): \tTrain Loss  1.8080\n",
            "Epoch(2/10)\t Batch(1000/3375): \tTrain Loss  1.7095\n",
            "Epoch(2/10)\t Batch(1200/3375): \tTrain Loss  1.7116\n",
            "Epoch(2/10)\t Batch(1400/3375): \tTrain Loss  1.7582\n",
            "Epoch(2/10)\t Batch(1600/3375): \tTrain Loss  1.7559\n",
            "Epoch(2/10)\t Batch(1800/3375): \tTrain Loss  1.7214\n",
            "Epoch(2/10)\t Batch(2000/3375): \tTrain Loss  1.7617\n",
            "Epoch(2/10)\t Batch(2200/3375): \tTrain Loss  1.7208\n",
            "Epoch(2/10)\t Batch(2400/3375): \tTrain Loss  1.7398\n",
            "Epoch(2/10)\t Batch(2600/3375): \tTrain Loss  1.7331\n",
            "Epoch(2/10)\t Batch(2800/3375): \tTrain Loss  1.6805\n",
            "Epoch(2/10)\t Batch(3000/3375): \tTrain Loss  1.6970\n",
            "Epoch(2/10)\t Batch(3200/3375): \tTrain Loss  1.6754\n",
            "End of Epoch: 2 Train Accuracy: 0.8411 Val Loss: 1.7058 Val Accuracy: 0.8433\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/3375): \tTrain Loss  1.6909\n",
            "Epoch(3/10)\t Batch(200/3375): \tTrain Loss  1.6388\n",
            "Epoch(3/10)\t Batch(400/3375): \tTrain Loss  1.8152\n",
            "Epoch(3/10)\t Batch(600/3375): \tTrain Loss  1.5865\n",
            "Epoch(3/10)\t Batch(800/3375): \tTrain Loss  1.7278\n",
            "Epoch(3/10)\t Batch(1000/3375): \tTrain Loss  1.5585\n",
            "Epoch(3/10)\t Batch(1200/3375): \tTrain Loss  1.6005\n",
            "Epoch(3/10)\t Batch(1400/3375): \tTrain Loss  1.6113\n",
            "Epoch(3/10)\t Batch(1600/3375): \tTrain Loss  1.6360\n",
            "Epoch(3/10)\t Batch(1800/3375): \tTrain Loss  1.7355\n",
            "Epoch(3/10)\t Batch(2000/3375): \tTrain Loss  1.8547\n",
            "Epoch(3/10)\t Batch(2200/3375): \tTrain Loss  1.6228\n",
            "Epoch(3/10)\t Batch(2400/3375): \tTrain Loss  1.7913\n",
            "Epoch(3/10)\t Batch(2600/3375): \tTrain Loss  1.6215\n",
            "Epoch(3/10)\t Batch(2800/3375): \tTrain Loss  1.7069\n",
            "Epoch(3/10)\t Batch(3000/3375): \tTrain Loss  1.7250\n",
            "Epoch(3/10)\t Batch(3200/3375): \tTrain Loss  1.6359\n",
            "End of Epoch: 3 Train Accuracy: 0.8494 Val Loss: 1.7030 Val Accuracy: 0.8537\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/3375): \tTrain Loss  1.7471\n",
            "Epoch(4/10)\t Batch(200/3375): \tTrain Loss  1.8322\n",
            "Epoch(4/10)\t Batch(400/3375): \tTrain Loss  1.7141\n",
            "Epoch(4/10)\t Batch(600/3375): \tTrain Loss  1.7329\n",
            "Epoch(4/10)\t Batch(800/3375): \tTrain Loss  1.7220\n",
            "Epoch(4/10)\t Batch(1000/3375): \tTrain Loss  1.7089\n",
            "Epoch(4/10)\t Batch(1200/3375): \tTrain Loss  1.7397\n",
            "Epoch(4/10)\t Batch(1400/3375): \tTrain Loss  1.7984\n",
            "Epoch(4/10)\t Batch(1600/3375): \tTrain Loss  1.6842\n",
            "Epoch(4/10)\t Batch(1800/3375): \tTrain Loss  1.6046\n",
            "Epoch(4/10)\t Batch(2000/3375): \tTrain Loss  1.6625\n",
            "Epoch(4/10)\t Batch(2200/3375): \tTrain Loss  1.7825\n",
            "Epoch(4/10)\t Batch(2400/3375): \tTrain Loss  1.6386\n",
            "Epoch(4/10)\t Batch(2600/3375): \tTrain Loss  1.7780\n",
            "Epoch(4/10)\t Batch(2800/3375): \tTrain Loss  1.6720\n",
            "Epoch(4/10)\t Batch(3000/3375): \tTrain Loss  1.6479\n",
            "Epoch(4/10)\t Batch(3200/3375): \tTrain Loss  1.6942\n",
            "End of Epoch: 4 Train Accuracy: 0.8553 Val Loss: 1.7016 Val Accuracy: 0.8573\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/3375): \tTrain Loss  1.7357\n",
            "Epoch(5/10)\t Batch(200/3375): \tTrain Loss  1.6766\n",
            "Epoch(5/10)\t Batch(400/3375): \tTrain Loss  1.6480\n",
            "Epoch(5/10)\t Batch(600/3375): \tTrain Loss  1.5471\n",
            "Epoch(5/10)\t Batch(800/3375): \tTrain Loss  1.6665\n",
            "Epoch(5/10)\t Batch(1000/3375): \tTrain Loss  1.6968\n",
            "Epoch(5/10)\t Batch(1200/3375): \tTrain Loss  1.5733\n",
            "Epoch(5/10)\t Batch(1400/3375): \tTrain Loss  1.6314\n",
            "Epoch(5/10)\t Batch(1600/3375): \tTrain Loss  1.7043\n",
            "Epoch(5/10)\t Batch(1800/3375): \tTrain Loss  1.7761\n",
            "Epoch(5/10)\t Batch(2000/3375): \tTrain Loss  1.6111\n",
            "Epoch(5/10)\t Batch(2200/3375): \tTrain Loss  1.6250\n",
            "Epoch(5/10)\t Batch(2400/3375): \tTrain Loss  1.7245\n",
            "Epoch(5/10)\t Batch(2600/3375): \tTrain Loss  1.6234\n",
            "Epoch(5/10)\t Batch(2800/3375): \tTrain Loss  1.6124\n",
            "Epoch(5/10)\t Batch(3000/3375): \tTrain Loss  1.7746\n",
            "Epoch(5/10)\t Batch(3200/3375): \tTrain Loss  1.5798\n",
            "End of Epoch: 5 Train Accuracy: 0.8581 Val Loss: 1.6930 Val Accuracy: 0.8615\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/3375): \tTrain Loss  1.5966\n",
            "Epoch(6/10)\t Batch(200/3375): \tTrain Loss  1.5642\n",
            "Epoch(6/10)\t Batch(400/3375): \tTrain Loss  1.7624\n",
            "Epoch(6/10)\t Batch(600/3375): \tTrain Loss  1.6480\n",
            "Epoch(6/10)\t Batch(800/3375): \tTrain Loss  1.7118\n",
            "Epoch(6/10)\t Batch(1000/3375): \tTrain Loss  1.5792\n",
            "Epoch(6/10)\t Batch(1200/3375): \tTrain Loss  1.6462\n",
            "Epoch(6/10)\t Batch(1400/3375): \tTrain Loss  1.6434\n",
            "Epoch(6/10)\t Batch(1600/3375): \tTrain Loss  1.6188\n",
            "Epoch(6/10)\t Batch(1800/3375): \tTrain Loss  1.7125\n",
            "Epoch(6/10)\t Batch(2000/3375): \tTrain Loss  1.7519\n",
            "Epoch(6/10)\t Batch(2200/3375): \tTrain Loss  1.6521\n",
            "Epoch(6/10)\t Batch(2400/3375): \tTrain Loss  1.7088\n",
            "Epoch(6/10)\t Batch(2600/3375): \tTrain Loss  1.6531\n",
            "Epoch(6/10)\t Batch(2800/3375): \tTrain Loss  1.7010\n",
            "Epoch(6/10)\t Batch(3000/3375): \tTrain Loss  1.6531\n",
            "Epoch(6/10)\t Batch(3200/3375): \tTrain Loss  1.6419\n",
            "End of Epoch: 6 Train Accuracy: 0.8582 Val Loss: 1.6972 Val Accuracy: 0.8600\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/3375): \tTrain Loss  1.6162\n",
            "Epoch(7/10)\t Batch(200/3375): \tTrain Loss  1.7827\n",
            "Epoch(7/10)\t Batch(400/3375): \tTrain Loss  1.6185\n",
            "Epoch(7/10)\t Batch(600/3375): \tTrain Loss  1.5896\n",
            "Epoch(7/10)\t Batch(800/3375): \tTrain Loss  1.6325\n",
            "Epoch(7/10)\t Batch(1000/3375): \tTrain Loss  1.6984\n",
            "Epoch(7/10)\t Batch(1200/3375): \tTrain Loss  1.6577\n",
            "Epoch(7/10)\t Batch(1400/3375): \tTrain Loss  1.6709\n",
            "Epoch(7/10)\t Batch(1600/3375): \tTrain Loss  1.6157\n",
            "Epoch(7/10)\t Batch(1800/3375): \tTrain Loss  1.6997\n",
            "Epoch(7/10)\t Batch(2000/3375): \tTrain Loss  1.6527\n",
            "Epoch(7/10)\t Batch(2200/3375): \tTrain Loss  1.6383\n",
            "Epoch(7/10)\t Batch(2400/3375): \tTrain Loss  1.6977\n",
            "Epoch(7/10)\t Batch(2600/3375): \tTrain Loss  1.6326\n",
            "Epoch(7/10)\t Batch(2800/3375): \tTrain Loss  1.6919\n",
            "Epoch(7/10)\t Batch(3000/3375): \tTrain Loss  1.5369\n",
            "Epoch(7/10)\t Batch(3200/3375): \tTrain Loss  1.6167\n",
            "End of Epoch: 7 Train Accuracy: 0.8587 Val Loss: 1.6854 Val Accuracy: 0.8588\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/3375): \tTrain Loss  1.6523\n",
            "Epoch(8/10)\t Batch(200/3375): \tTrain Loss  1.6494\n",
            "Epoch(8/10)\t Batch(400/3375): \tTrain Loss  1.5926\n",
            "Epoch(8/10)\t Batch(600/3375): \tTrain Loss  1.7673\n",
            "Epoch(8/10)\t Batch(800/3375): \tTrain Loss  1.7077\n",
            "Epoch(8/10)\t Batch(1000/3375): \tTrain Loss  1.7008\n",
            "Epoch(8/10)\t Batch(1200/3375): \tTrain Loss  1.8446\n",
            "Epoch(8/10)\t Batch(1400/3375): \tTrain Loss  1.6956\n",
            "Epoch(8/10)\t Batch(1600/3375): \tTrain Loss  1.6549\n",
            "Epoch(8/10)\t Batch(1800/3375): \tTrain Loss  1.6067\n",
            "Epoch(8/10)\t Batch(2000/3375): \tTrain Loss  1.7378\n",
            "Epoch(8/10)\t Batch(2200/3375): \tTrain Loss  1.7193\n",
            "Epoch(8/10)\t Batch(2400/3375): \tTrain Loss  1.6699\n",
            "Epoch(8/10)\t Batch(2600/3375): \tTrain Loss  1.5852\n",
            "Epoch(8/10)\t Batch(2800/3375): \tTrain Loss  1.5710\n",
            "Epoch(8/10)\t Batch(3000/3375): \tTrain Loss  1.6105\n",
            "Epoch(8/10)\t Batch(3200/3375): \tTrain Loss  1.7116\n",
            "End of Epoch: 8 Train Accuracy: 0.8568 Val Loss: 1.6836 Val Accuracy: 0.8547\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/3375): \tTrain Loss  1.6523\n",
            "Epoch(9/10)\t Batch(200/3375): \tTrain Loss  1.6960\n",
            "Epoch(9/10)\t Batch(400/3375): \tTrain Loss  1.7437\n",
            "Epoch(9/10)\t Batch(600/3375): \tTrain Loss  1.6439\n",
            "Epoch(9/10)\t Batch(800/3375): \tTrain Loss  1.6749\n",
            "Epoch(9/10)\t Batch(1000/3375): \tTrain Loss  1.5140\n",
            "Epoch(9/10)\t Batch(1200/3375): \tTrain Loss  1.7081\n",
            "Epoch(9/10)\t Batch(1400/3375): \tTrain Loss  1.6821\n",
            "Epoch(9/10)\t Batch(1600/3375): \tTrain Loss  1.8157\n",
            "Epoch(9/10)\t Batch(1800/3375): \tTrain Loss  1.5714\n",
            "Epoch(9/10)\t Batch(2000/3375): \tTrain Loss  1.6530\n",
            "Epoch(9/10)\t Batch(2200/3375): \tTrain Loss  1.6571\n",
            "Epoch(9/10)\t Batch(2400/3375): \tTrain Loss  1.6259\n",
            "Epoch(9/10)\t Batch(2600/3375): \tTrain Loss  1.7052\n",
            "Epoch(9/10)\t Batch(2800/3375): \tTrain Loss  1.8300\n",
            "Epoch(9/10)\t Batch(3000/3375): \tTrain Loss  1.6442\n",
            "Epoch(9/10)\t Batch(3200/3375): \tTrain Loss  1.7022\n",
            "End of Epoch: 9 Train Accuracy: 0.8628 Val Loss: 1.6786 Val Accuracy: 0.8595\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/3375): \tTrain Loss  1.6688\n",
            "Epoch(10/10)\t Batch(200/3375): \tTrain Loss  1.7041\n",
            "Epoch(10/10)\t Batch(400/3375): \tTrain Loss  1.7737\n",
            "Epoch(10/10)\t Batch(600/3375): \tTrain Loss  1.5752\n",
            "Epoch(10/10)\t Batch(800/3375): \tTrain Loss  1.6588\n",
            "Epoch(10/10)\t Batch(1000/3375): \tTrain Loss  1.6818\n",
            "Epoch(10/10)\t Batch(1200/3375): \tTrain Loss  1.6912\n",
            "Epoch(10/10)\t Batch(1400/3375): \tTrain Loss  1.7592\n",
            "Epoch(10/10)\t Batch(1600/3375): \tTrain Loss  1.6274\n",
            "Epoch(10/10)\t Batch(1800/3375): \tTrain Loss  1.7186\n",
            "Epoch(10/10)\t Batch(2000/3375): \tTrain Loss  1.6840\n",
            "Epoch(10/10)\t Batch(2200/3375): \tTrain Loss  1.6802\n",
            "Epoch(10/10)\t Batch(2400/3375): \tTrain Loss  1.7059\n",
            "Epoch(10/10)\t Batch(2600/3375): \tTrain Loss  1.6643\n",
            "Epoch(10/10)\t Batch(2800/3375): \tTrain Loss  1.7793\n",
            "Epoch(10/10)\t Batch(3000/3375): \tTrain Loss  1.7253\n",
            "Epoch(10/10)\t Batch(3200/3375): \tTrain Loss  1.7570\n",
            "End of Epoch: 10 Train Accuracy: 0.8671 Val Loss: 1.6726 Val Accuracy: 0.8652\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁██████████</td></tr><tr><td>Train Loss</td><td>█▁▅▅▄▅▆▇▁▅</td></tr><tr><td>Val Accuracy</td><td>▁██████████</td></tr><tr><td>Val Loss</td><td>█▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.86707</td></tr><tr><td>Train Loss</td><td>1.67977</td></tr><tr><td>Val Accuracy</td><td>0.86517</td></tr><tr><td>Val Loss</td><td>1.67261</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_cross_entropy|lr=0.0001|batch_16|act_tanh|hid_5|neurons_128|nrns_10|init_Xavier529</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8tjgw6eu' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8tjgw6eu</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_175213-8tjgw6eu/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: eqhfyugz with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nag\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_175634-eqhfyugz</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/eqhfyugz' target=\"_blank\">fallen-sweep-60</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/eqhfyugz' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/eqhfyugz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.1180 Val Loss: 9.8070 Val Accuracy: 0.1162\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/3375): \tTrain Loss  9.9425\n",
            "Epoch(1/5)\t Batch(200/3375): \tTrain Loss  4.2235\n",
            "Epoch(1/5)\t Batch(400/3375): \tTrain Loss  2.8195\n",
            "Epoch(1/5)\t Batch(600/3375): \tTrain Loss  2.5822\n",
            "Epoch(1/5)\t Batch(800/3375): \tTrain Loss  2.1648\n",
            "Epoch(1/5)\t Batch(1000/3375): \tTrain Loss  2.8325\n",
            "Epoch(1/5)\t Batch(1200/3375): \tTrain Loss  2.2870\n",
            "Epoch(1/5)\t Batch(1400/3375): \tTrain Loss  2.2638\n",
            "Epoch(1/5)\t Batch(1600/3375): \tTrain Loss  2.7145\n",
            "Epoch(1/5)\t Batch(1800/3375): \tTrain Loss  2.4496\n",
            "Epoch(1/5)\t Batch(2000/3375): \tTrain Loss  2.2875\n",
            "Epoch(1/5)\t Batch(2200/3375): \tTrain Loss  2.2768\n",
            "Epoch(1/5)\t Batch(2400/3375): \tTrain Loss  2.2994\n",
            "Epoch(1/5)\t Batch(2600/3375): \tTrain Loss  2.3052\n",
            "Epoch(1/5)\t Batch(2800/3375): \tTrain Loss  2.3261\n",
            "Epoch(1/5)\t Batch(3000/3375): \tTrain Loss  2.3569\n",
            "Epoch(1/5)\t Batch(3200/3375): \tTrain Loss  2.2802\n",
            "End of Epoch: 1 Train Accuracy: 0.1076 Val Loss: 2.3380 Val Accuracy: 0.1097\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/3375): \tTrain Loss  2.5124\n",
            "Epoch(2/5)\t Batch(200/3375): \tTrain Loss  2.3306\n",
            "Epoch(2/5)\t Batch(400/3375): \tTrain Loss  2.4182\n",
            "Epoch(2/5)\t Batch(600/3375): \tTrain Loss  2.3782\n",
            "Epoch(2/5)\t Batch(800/3375): \tTrain Loss  2.4246\n",
            "Epoch(2/5)\t Batch(1000/3375): \tTrain Loss  2.2969\n",
            "Epoch(2/5)\t Batch(1200/3375): \tTrain Loss  2.3130\n",
            "Epoch(2/5)\t Batch(1400/3375): \tTrain Loss  2.3016\n",
            "Epoch(2/5)\t Batch(1600/3375): \tTrain Loss  2.3184\n",
            "Epoch(2/5)\t Batch(1800/3375): \tTrain Loss  2.2954\n",
            "Epoch(2/5)\t Batch(2000/3375): \tTrain Loss  2.3027\n",
            "Epoch(2/5)\t Batch(2200/3375): \tTrain Loss  2.3315\n",
            "Epoch(2/5)\t Batch(2400/3375): \tTrain Loss  2.4434\n",
            "Epoch(2/5)\t Batch(2600/3375): \tTrain Loss  2.3077\n",
            "Epoch(2/5)\t Batch(2800/3375): \tTrain Loss  2.2987\n",
            "Epoch(2/5)\t Batch(3000/3375): \tTrain Loss  2.2218\n",
            "Epoch(2/5)\t Batch(3200/3375): \tTrain Loss  2.2981\n",
            "End of Epoch: 2 Train Accuracy: 0.1113 Val Loss: 2.3107 Val Accuracy: 0.1090\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/3375): \tTrain Loss  2.2792\n",
            "Epoch(3/5)\t Batch(200/3375): \tTrain Loss  2.2960\n",
            "Epoch(3/5)\t Batch(400/3375): \tTrain Loss  2.3017\n",
            "Epoch(3/5)\t Batch(600/3375): \tTrain Loss  2.3066\n",
            "Epoch(3/5)\t Batch(800/3375): \tTrain Loss  2.3031\n",
            "Epoch(3/5)\t Batch(1000/3375): \tTrain Loss  2.2956\n",
            "Epoch(3/5)\t Batch(1200/3375): \tTrain Loss  2.3350\n",
            "Epoch(3/5)\t Batch(1400/3375): \tTrain Loss  2.3047\n",
            "Epoch(3/5)\t Batch(1600/3375): \tTrain Loss  2.2979\n",
            "Epoch(3/5)\t Batch(1800/3375): \tTrain Loss  2.2961\n",
            "Epoch(3/5)\t Batch(2000/3375): \tTrain Loss  2.3031\n",
            "Epoch(3/5)\t Batch(2200/3375): \tTrain Loss  2.3038\n",
            "Epoch(3/5)\t Batch(2400/3375): \tTrain Loss  2.3045\n",
            "Epoch(3/5)\t Batch(2600/3375): \tTrain Loss  2.3047\n",
            "Epoch(3/5)\t Batch(2800/3375): \tTrain Loss  2.3065\n",
            "Epoch(3/5)\t Batch(3000/3375): \tTrain Loss  2.3029\n",
            "Epoch(3/5)\t Batch(3200/3375): \tTrain Loss  2.3011\n",
            "End of Epoch: 3 Train Accuracy: 0.1340 Val Loss: 2.3056 Val Accuracy: 0.1372\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/3375): \tTrain Loss  2.2970\n",
            "Epoch(4/5)\t Batch(200/3375): \tTrain Loss  2.4010\n",
            "Epoch(4/5)\t Batch(400/3375): \tTrain Loss  2.3000\n",
            "Epoch(4/5)\t Batch(600/3375): \tTrain Loss  2.3497\n",
            "Epoch(4/5)\t Batch(800/3375): \tTrain Loss  2.3014\n",
            "Epoch(4/5)\t Batch(1000/3375): \tTrain Loss  2.2933\n",
            "Epoch(4/5)\t Batch(1200/3375): \tTrain Loss  2.3289\n",
            "Epoch(4/5)\t Batch(1400/3375): \tTrain Loss  2.3056\n",
            "Epoch(4/5)\t Batch(1600/3375): \tTrain Loss  2.3073\n",
            "Epoch(4/5)\t Batch(1800/3375): \tTrain Loss  2.3232\n",
            "Epoch(4/5)\t Batch(2000/3375): \tTrain Loss  2.2979\n",
            "Epoch(4/5)\t Batch(2200/3375): \tTrain Loss  2.3417\n",
            "Epoch(4/5)\t Batch(2400/3375): \tTrain Loss  2.3037\n",
            "Epoch(4/5)\t Batch(2600/3375): \tTrain Loss  2.3028\n",
            "Epoch(4/5)\t Batch(2800/3375): \tTrain Loss  2.3007\n",
            "Epoch(4/5)\t Batch(3000/3375): \tTrain Loss  2.3098\n",
            "Epoch(4/5)\t Batch(3200/3375): \tTrain Loss  2.3029\n",
            "End of Epoch: 4 Train Accuracy: 0.1245 Val Loss: 2.3032 Val Accuracy: 0.1248\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/3375): \tTrain Loss  2.2996\n",
            "Epoch(5/5)\t Batch(200/3375): \tTrain Loss  2.2973\n",
            "Epoch(5/5)\t Batch(400/3375): \tTrain Loss  2.3122\n",
            "Epoch(5/5)\t Batch(600/3375): \tTrain Loss  2.3019\n",
            "Epoch(5/5)\t Batch(800/3375): \tTrain Loss  2.3014\n",
            "Epoch(5/5)\t Batch(1000/3375): \tTrain Loss  2.3009\n",
            "Epoch(5/5)\t Batch(1200/3375): \tTrain Loss  2.3333\n",
            "Epoch(5/5)\t Batch(1400/3375): \tTrain Loss  2.2975\n",
            "Epoch(5/5)\t Batch(1600/3375): \tTrain Loss  2.3003\n",
            "Epoch(5/5)\t Batch(1800/3375): \tTrain Loss  2.2829\n",
            "Epoch(5/5)\t Batch(2000/3375): \tTrain Loss  2.2995\n",
            "Epoch(5/5)\t Batch(2200/3375): \tTrain Loss  2.2728\n",
            "Epoch(5/5)\t Batch(2400/3375): \tTrain Loss  2.3034\n",
            "Epoch(5/5)\t Batch(2600/3375): \tTrain Loss  2.3026\n",
            "Epoch(5/5)\t Batch(2800/3375): \tTrain Loss  2.2948\n",
            "Epoch(5/5)\t Batch(3000/3375): \tTrain Loss  2.2908\n",
            "Epoch(5/5)\t Batch(3200/3375): \tTrain Loss  2.3009\n",
            "End of Epoch: 5 Train Accuracy: 0.1345 Val Loss: 2.3012 Val Accuracy: 0.1348\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▄▁▂█▅█</td></tr><tr><td>Train Loss</td><td>▃█▂▁▁</td></tr><tr><td>Val Accuracy</td><td>▃▁▁█▅▇</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.13446</td></tr><tr><td>Train Loss</td><td>2.29285</td></tr><tr><td>Val Accuracy</td><td>0.13483</td></tr><tr><td>Val Loss</td><td>2.30124</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nag|loss_cross_entropy|lr=0.0001|batch_16|act_tanh|hid_3|neurons_64|nrns_5|init_Random740</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/eqhfyugz' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/eqhfyugz</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_175634-eqhfyugz/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cuf0945n with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nag\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_175730-cuf0945n</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/cuf0945n' target=\"_blank\">elated-sweep-61</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/cuf0945n' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/cuf0945n</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.1291 Val Loss: 7.7410 Val Accuracy: 0.1283\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/843): \tTrain Loss  7.9148\n",
            "Epoch(1/5)\t Batch(200/843): \tTrain Loss  2.3535\n",
            "Epoch(1/5)\t Batch(400/843): \tTrain Loss  2.2936\n",
            "Epoch(1/5)\t Batch(600/843): \tTrain Loss  2.3006\n",
            "Epoch(1/5)\t Batch(800/843): \tTrain Loss  2.2935\n",
            "End of Epoch: 1 Train Accuracy: 0.1425 Val Loss: 2.2926 Val Accuracy: 0.1367\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/843): \tTrain Loss  2.2979\n",
            "Epoch(2/5)\t Batch(200/843): \tTrain Loss  2.2894\n",
            "Epoch(2/5)\t Batch(400/843): \tTrain Loss  2.2872\n",
            "Epoch(2/5)\t Batch(600/843): \tTrain Loss  2.2869\n",
            "Epoch(2/5)\t Batch(800/843): \tTrain Loss  2.2663\n",
            "End of Epoch: 2 Train Accuracy: 0.2240 Val Loss: 2.2682 Val Accuracy: 0.2305\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/843): \tTrain Loss  2.2726\n",
            "Epoch(3/5)\t Batch(200/843): \tTrain Loss  2.2658\n",
            "Epoch(3/5)\t Batch(400/843): \tTrain Loss  2.2484\n",
            "Epoch(3/5)\t Batch(600/843): \tTrain Loss  2.2593\n",
            "Epoch(3/5)\t Batch(800/843): \tTrain Loss  2.1981\n",
            "End of Epoch: 3 Train Accuracy: 0.3356 Val Loss: 2.2287 Val Accuracy: 0.3322\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/843): \tTrain Loss  2.2222\n",
            "Epoch(4/5)\t Batch(200/843): \tTrain Loss  2.2127\n",
            "Epoch(4/5)\t Batch(400/843): \tTrain Loss  2.2012\n",
            "Epoch(4/5)\t Batch(600/843): \tTrain Loss  2.1959\n",
            "Epoch(4/5)\t Batch(800/843): \tTrain Loss  2.1449\n",
            "End of Epoch: 4 Train Accuracy: 0.4404 Val Loss: 2.1733 Val Accuracy: 0.4423\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/843): \tTrain Loss  2.1493\n",
            "Epoch(5/5)\t Batch(200/843): \tTrain Loss  2.1411\n",
            "Epoch(5/5)\t Batch(400/843): \tTrain Loss  2.1685\n",
            "Epoch(5/5)\t Batch(600/843): \tTrain Loss  2.1330\n",
            "Epoch(5/5)\t Batch(800/843): \tTrain Loss  2.1354\n",
            "End of Epoch: 5 Train Accuracy: 0.5159 Val Loss: 2.1259 Val Accuracy: 0.5150\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▁▃▅▇█</td></tr><tr><td>Train Loss</td><td>█▇▅▃▁</td></tr><tr><td>Val Accuracy</td><td>▁▁▃▅▇█</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.51591</td></tr><tr><td>Train Loss</td><td>2.12783</td></tr><tr><td>Val Accuracy</td><td>0.515</td></tr><tr><td>Val Loss</td><td>2.12586</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nag|loss_cross_entropy|lr=0.001|batch_64|act_tanh|hid_3|neurons_32|nrns_5|init_Random346</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/cuf0945n' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/cuf0945n</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_175730-cuf0945n/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kcoxrd39 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nag\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_175751-kcoxrd39</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/kcoxrd39' target=\"_blank\">divine-sweep-62</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/kcoxrd39' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/kcoxrd39</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.0712 Val Loss: 0.1749 Val Accuracy: 0.0722\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/3375): \tTrain Loss  0.1665\n",
            "Epoch(1/10)\t Batch(200/3375): \tTrain Loss  0.1463\n",
            "Epoch(1/10)\t Batch(400/3375): \tTrain Loss  0.1220\n",
            "Epoch(1/10)\t Batch(600/3375): \tTrain Loss  0.1679\n",
            "Epoch(1/10)\t Batch(800/3375): \tTrain Loss  0.1678\n",
            "Epoch(1/10)\t Batch(1000/3375): \tTrain Loss  0.1494\n",
            "Epoch(1/10)\t Batch(1200/3375): \tTrain Loss  0.1688\n",
            "Epoch(1/10)\t Batch(1400/3375): \tTrain Loss  0.0927\n",
            "Epoch(1/10)\t Batch(1600/3375): \tTrain Loss  0.1576\n",
            "Epoch(1/10)\t Batch(1800/3375): \tTrain Loss  0.1207\n",
            "Epoch(1/10)\t Batch(2000/3375): \tTrain Loss  0.1327\n",
            "Epoch(1/10)\t Batch(2200/3375): \tTrain Loss  0.1344\n",
            "Epoch(1/10)\t Batch(2400/3375): \tTrain Loss  0.1423\n",
            "Epoch(1/10)\t Batch(2600/3375): \tTrain Loss  0.1106\n",
            "Epoch(1/10)\t Batch(2800/3375): \tTrain Loss  0.1226\n",
            "Epoch(1/10)\t Batch(3000/3375): \tTrain Loss  0.1273\n",
            "Epoch(1/10)\t Batch(3200/3375): \tTrain Loss  0.1167\n",
            "End of Epoch: 1 Train Accuracy: 0.3332 Val Loss: 0.1233 Val Accuracy: 0.3292\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/3375): \tTrain Loss  0.1158\n",
            "Epoch(2/10)\t Batch(200/3375): \tTrain Loss  0.0757\n",
            "Epoch(2/10)\t Batch(400/3375): \tTrain Loss  0.1172\n",
            "Epoch(2/10)\t Batch(600/3375): \tTrain Loss  0.1004\n",
            "Epoch(2/10)\t Batch(800/3375): \tTrain Loss  0.1145\n",
            "Epoch(2/10)\t Batch(1000/3375): \tTrain Loss  0.1372\n",
            "Epoch(2/10)\t Batch(1200/3375): \tTrain Loss  0.1117\n",
            "Epoch(2/10)\t Batch(1400/3375): \tTrain Loss  0.1078\n",
            "Epoch(2/10)\t Batch(1600/3375): \tTrain Loss  0.1405\n",
            "Epoch(2/10)\t Batch(1800/3375): \tTrain Loss  0.0772\n",
            "Epoch(2/10)\t Batch(2000/3375): \tTrain Loss  0.1251\n",
            "Epoch(2/10)\t Batch(2200/3375): \tTrain Loss  0.0768\n",
            "Epoch(2/10)\t Batch(2400/3375): \tTrain Loss  0.1337\n",
            "Epoch(2/10)\t Batch(2600/3375): \tTrain Loss  0.0982\n",
            "Epoch(2/10)\t Batch(2800/3375): \tTrain Loss  0.0516\n",
            "Epoch(2/10)\t Batch(3000/3375): \tTrain Loss  0.1181\n",
            "Epoch(2/10)\t Batch(3200/3375): \tTrain Loss  0.1025\n",
            "End of Epoch: 2 Train Accuracy: 0.4194 Val Loss: 0.1092 Val Accuracy: 0.4057\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/3375): \tTrain Loss  0.1291\n",
            "Epoch(3/10)\t Batch(200/3375): \tTrain Loss  0.1367\n",
            "Epoch(3/10)\t Batch(400/3375): \tTrain Loss  0.1077\n",
            "Epoch(3/10)\t Batch(600/3375): \tTrain Loss  0.1070\n",
            "Epoch(3/10)\t Batch(800/3375): \tTrain Loss  0.1289\n",
            "Epoch(3/10)\t Batch(1000/3375): \tTrain Loss  0.1081\n",
            "Epoch(3/10)\t Batch(1200/3375): \tTrain Loss  0.0856\n",
            "Epoch(3/10)\t Batch(1400/3375): \tTrain Loss  0.1338\n",
            "Epoch(3/10)\t Batch(1600/3375): \tTrain Loss  0.1360\n",
            "Epoch(3/10)\t Batch(1800/3375): \tTrain Loss  0.1025\n",
            "Epoch(3/10)\t Batch(2000/3375): \tTrain Loss  0.0858\n",
            "Epoch(3/10)\t Batch(2200/3375): \tTrain Loss  0.0815\n",
            "Epoch(3/10)\t Batch(2400/3375): \tTrain Loss  0.0689\n",
            "Epoch(3/10)\t Batch(2600/3375): \tTrain Loss  0.0982\n",
            "Epoch(3/10)\t Batch(2800/3375): \tTrain Loss  0.1339\n",
            "Epoch(3/10)\t Batch(3000/3375): \tTrain Loss  0.1220\n",
            "Epoch(3/10)\t Batch(3200/3375): \tTrain Loss  0.1031\n",
            "End of Epoch: 3 Train Accuracy: 0.4554 Val Loss: 0.1023 Val Accuracy: 0.4458\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/3375): \tTrain Loss  0.0940\n",
            "Epoch(4/10)\t Batch(200/3375): \tTrain Loss  0.1109\n",
            "Epoch(4/10)\t Batch(400/3375): \tTrain Loss  0.1047\n",
            "Epoch(4/10)\t Batch(600/3375): \tTrain Loss  0.1092\n",
            "Epoch(4/10)\t Batch(800/3375): \tTrain Loss  0.1082\n",
            "Epoch(4/10)\t Batch(1000/3375): \tTrain Loss  0.0839\n",
            "Epoch(4/10)\t Batch(1200/3375): \tTrain Loss  0.1068\n",
            "Epoch(4/10)\t Batch(1400/3375): \tTrain Loss  0.0575\n",
            "Epoch(4/10)\t Batch(1600/3375): \tTrain Loss  0.0965\n",
            "Epoch(4/10)\t Batch(1800/3375): \tTrain Loss  0.1066\n",
            "Epoch(4/10)\t Batch(2000/3375): \tTrain Loss  0.0795\n",
            "Epoch(4/10)\t Batch(2200/3375): \tTrain Loss  0.1100\n",
            "Epoch(4/10)\t Batch(2400/3375): \tTrain Loss  0.0786\n",
            "Epoch(4/10)\t Batch(2600/3375): \tTrain Loss  0.1362\n",
            "Epoch(4/10)\t Batch(2800/3375): \tTrain Loss  0.1197\n",
            "Epoch(4/10)\t Batch(3000/3375): \tTrain Loss  0.1064\n",
            "Epoch(4/10)\t Batch(3200/3375): \tTrain Loss  0.0358\n",
            "End of Epoch: 4 Train Accuracy: 0.4704 Val Loss: 0.0985 Val Accuracy: 0.4683\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/3375): \tTrain Loss  0.1044\n",
            "Epoch(5/10)\t Batch(200/3375): \tTrain Loss  0.0670\n",
            "Epoch(5/10)\t Batch(400/3375): \tTrain Loss  0.0805\n",
            "Epoch(5/10)\t Batch(600/3375): \tTrain Loss  0.0642\n",
            "Epoch(5/10)\t Batch(800/3375): \tTrain Loss  0.0899\n",
            "Epoch(5/10)\t Batch(1000/3375): \tTrain Loss  0.0817\n",
            "Epoch(5/10)\t Batch(1200/3375): \tTrain Loss  0.1116\n",
            "Epoch(5/10)\t Batch(1400/3375): \tTrain Loss  0.0957\n",
            "Epoch(5/10)\t Batch(1600/3375): \tTrain Loss  0.1543\n",
            "Epoch(5/10)\t Batch(1800/3375): \tTrain Loss  0.1000\n",
            "Epoch(5/10)\t Batch(2000/3375): \tTrain Loss  0.0890\n",
            "Epoch(5/10)\t Batch(2200/3375): \tTrain Loss  0.0687\n",
            "Epoch(5/10)\t Batch(2400/3375): \tTrain Loss  0.1261\n",
            "Epoch(5/10)\t Batch(2600/3375): \tTrain Loss  0.0920\n",
            "Epoch(5/10)\t Batch(2800/3375): \tTrain Loss  0.0893\n",
            "Epoch(5/10)\t Batch(3000/3375): \tTrain Loss  0.1198\n",
            "Epoch(5/10)\t Batch(3200/3375): \tTrain Loss  0.1099\n",
            "End of Epoch: 5 Train Accuracy: 0.5366 Val Loss: 0.0849 Val Accuracy: 0.5335\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/3375): \tTrain Loss  0.0698\n",
            "Epoch(6/10)\t Batch(200/3375): \tTrain Loss  0.0652\n",
            "Epoch(6/10)\t Batch(400/3375): \tTrain Loss  0.0651\n",
            "Epoch(6/10)\t Batch(600/3375): \tTrain Loss  0.0347\n",
            "Epoch(6/10)\t Batch(800/3375): \tTrain Loss  0.0636\n",
            "Epoch(6/10)\t Batch(1000/3375): \tTrain Loss  0.0707\n",
            "Epoch(6/10)\t Batch(1200/3375): \tTrain Loss  0.0991\n",
            "Epoch(6/10)\t Batch(1400/3375): \tTrain Loss  0.0915\n",
            "Epoch(6/10)\t Batch(1600/3375): \tTrain Loss  0.0847\n",
            "Epoch(6/10)\t Batch(1800/3375): \tTrain Loss  0.0917\n",
            "Epoch(6/10)\t Batch(2000/3375): \tTrain Loss  0.0486\n",
            "Epoch(6/10)\t Batch(2200/3375): \tTrain Loss  0.0834\n",
            "Epoch(6/10)\t Batch(2400/3375): \tTrain Loss  0.1155\n",
            "Epoch(6/10)\t Batch(2600/3375): \tTrain Loss  0.0989\n",
            "Epoch(6/10)\t Batch(2800/3375): \tTrain Loss  0.0730\n",
            "Epoch(6/10)\t Batch(3000/3375): \tTrain Loss  0.0971\n",
            "Epoch(6/10)\t Batch(3200/3375): \tTrain Loss  0.0600\n",
            "End of Epoch: 6 Train Accuracy: 0.5708 Val Loss: 0.0800 Val Accuracy: 0.5592\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/3375): \tTrain Loss  0.0928\n",
            "Epoch(7/10)\t Batch(200/3375): \tTrain Loss  0.0698\n",
            "Epoch(7/10)\t Batch(400/3375): \tTrain Loss  0.0949\n",
            "Epoch(7/10)\t Batch(600/3375): \tTrain Loss  0.0992\n",
            "Epoch(7/10)\t Batch(800/3375): \tTrain Loss  0.0696\n",
            "Epoch(7/10)\t Batch(1000/3375): \tTrain Loss  0.0858\n",
            "Epoch(7/10)\t Batch(1200/3375): \tTrain Loss  0.0792\n",
            "Epoch(7/10)\t Batch(1400/3375): \tTrain Loss  0.0566\n",
            "Epoch(7/10)\t Batch(1600/3375): \tTrain Loss  0.0883\n",
            "Epoch(7/10)\t Batch(1800/3375): \tTrain Loss  0.0992\n",
            "Epoch(7/10)\t Batch(2000/3375): \tTrain Loss  0.0300\n",
            "Epoch(7/10)\t Batch(2200/3375): \tTrain Loss  0.0441\n",
            "Epoch(7/10)\t Batch(2400/3375): \tTrain Loss  0.0820\n",
            "Epoch(7/10)\t Batch(2600/3375): \tTrain Loss  0.0722\n",
            "Epoch(7/10)\t Batch(2800/3375): \tTrain Loss  0.0615\n",
            "Epoch(7/10)\t Batch(3000/3375): \tTrain Loss  0.0501\n",
            "Epoch(7/10)\t Batch(3200/3375): \tTrain Loss  0.0856\n",
            "End of Epoch: 7 Train Accuracy: 0.6001 Val Loss: 0.0744 Val Accuracy: 0.5833\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/3375): \tTrain Loss  0.0683\n",
            "Epoch(8/10)\t Batch(200/3375): \tTrain Loss  0.1049\n",
            "Epoch(8/10)\t Batch(400/3375): \tTrain Loss  0.0759\n",
            "Epoch(8/10)\t Batch(600/3375): \tTrain Loss  0.0962\n",
            "Epoch(8/10)\t Batch(800/3375): \tTrain Loss  0.0665\n",
            "Epoch(8/10)\t Batch(1000/3375): \tTrain Loss  0.0721\n",
            "Epoch(8/10)\t Batch(1200/3375): \tTrain Loss  0.0707\n",
            "Epoch(8/10)\t Batch(1400/3375): \tTrain Loss  0.0698\n",
            "Epoch(8/10)\t Batch(1600/3375): \tTrain Loss  0.0662\n",
            "Epoch(8/10)\t Batch(1800/3375): \tTrain Loss  0.0602\n",
            "Epoch(8/10)\t Batch(2000/3375): \tTrain Loss  0.0573\n",
            "Epoch(8/10)\t Batch(2200/3375): \tTrain Loss  0.0596\n",
            "Epoch(8/10)\t Batch(2400/3375): \tTrain Loss  0.0763\n",
            "Epoch(8/10)\t Batch(2600/3375): \tTrain Loss  0.0983\n",
            "Epoch(8/10)\t Batch(2800/3375): \tTrain Loss  0.1027\n",
            "Epoch(8/10)\t Batch(3000/3375): \tTrain Loss  0.0687\n",
            "Epoch(8/10)\t Batch(3200/3375): \tTrain Loss  0.0332\n",
            "End of Epoch: 8 Train Accuracy: 0.6265 Val Loss: 0.0700 Val Accuracy: 0.6057\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/3375): \tTrain Loss  0.0999\n",
            "Epoch(9/10)\t Batch(200/3375): \tTrain Loss  0.0898\n",
            "Epoch(9/10)\t Batch(400/3375): \tTrain Loss  0.1060\n",
            "Epoch(9/10)\t Batch(600/3375): \tTrain Loss  0.0676\n",
            "Epoch(9/10)\t Batch(800/3375): \tTrain Loss  0.0452\n",
            "Epoch(9/10)\t Batch(1000/3375): \tTrain Loss  0.0745\n",
            "Epoch(9/10)\t Batch(1200/3375): \tTrain Loss  0.0466\n",
            "Epoch(9/10)\t Batch(1400/3375): \tTrain Loss  0.0575\n",
            "Epoch(9/10)\t Batch(1600/3375): \tTrain Loss  0.0542\n",
            "Epoch(9/10)\t Batch(1800/3375): \tTrain Loss  0.0274\n",
            "Epoch(9/10)\t Batch(2000/3375): \tTrain Loss  0.0965\n",
            "Epoch(9/10)\t Batch(2200/3375): \tTrain Loss  0.0491\n",
            "Epoch(9/10)\t Batch(2400/3375): \tTrain Loss  0.0494\n",
            "Epoch(9/10)\t Batch(2600/3375): \tTrain Loss  0.0794\n",
            "Epoch(9/10)\t Batch(2800/3375): \tTrain Loss  0.0598\n",
            "Epoch(9/10)\t Batch(3000/3375): \tTrain Loss  0.0360\n",
            "Epoch(9/10)\t Batch(3200/3375): \tTrain Loss  0.0557\n",
            "End of Epoch: 9 Train Accuracy: 0.6400 Val Loss: 0.0653 Val Accuracy: 0.6277\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/3375): \tTrain Loss  0.0650\n",
            "Epoch(10/10)\t Batch(200/3375): \tTrain Loss  0.0605\n",
            "Epoch(10/10)\t Batch(400/3375): \tTrain Loss  0.0739\n",
            "Epoch(10/10)\t Batch(600/3375): \tTrain Loss  0.0810\n",
            "Epoch(10/10)\t Batch(800/3375): \tTrain Loss  0.0924\n",
            "Epoch(10/10)\t Batch(1000/3375): \tTrain Loss  0.0601\n",
            "Epoch(10/10)\t Batch(1200/3375): \tTrain Loss  0.0946\n",
            "Epoch(10/10)\t Batch(1400/3375): \tTrain Loss  0.0362\n",
            "Epoch(10/10)\t Batch(1600/3375): \tTrain Loss  0.0559\n",
            "Epoch(10/10)\t Batch(1800/3375): \tTrain Loss  0.0864\n",
            "Epoch(10/10)\t Batch(2000/3375): \tTrain Loss  0.0864\n",
            "Epoch(10/10)\t Batch(2200/3375): \tTrain Loss  0.0652\n",
            "Epoch(10/10)\t Batch(2400/3375): \tTrain Loss  0.0685\n",
            "Epoch(10/10)\t Batch(2600/3375): \tTrain Loss  0.0638\n",
            "Epoch(10/10)\t Batch(2800/3375): \tTrain Loss  0.0513\n",
            "Epoch(10/10)\t Batch(3000/3375): \tTrain Loss  0.0794\n",
            "Epoch(10/10)\t Batch(3200/3375): \tTrain Loss  0.0225\n",
            "End of Epoch: 10 Train Accuracy: 0.6434 Val Loss: 0.0609 Val Accuracy: 0.6263\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▄▅▆▆▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▅▄▅▄▂▄▄▃▁</td></tr><tr><td>Val Accuracy</td><td>▁▄▅▆▆▇▇▇███</td></tr><tr><td>Val Loss</td><td>█▅▄▄▃▂▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.64337</td></tr><tr><td>Train Loss</td><td>0.04361</td></tr><tr><td>Val Accuracy</td><td>0.62633</td></tr><tr><td>Val Loss</td><td>0.06089</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nag|loss_mse|lr=0.001|batch_16|act_tanh|hid_3|neurons_128|nrns_10|init_Random290</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/kcoxrd39' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/kcoxrd39</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_175751-kcoxrd39/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vdgb96yi with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_180117-vdgb96yi</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/vdgb96yi' target=\"_blank\">wobbly-sweep-63</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/vdgb96yi' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/vdgb96yi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.1398 Val Loss: 2.2786 Val Accuracy: 0.1365\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  2.2667\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  1.7888\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  1.6925\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  1.7509\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  1.6972\n",
            "End of Epoch: 1 Train Accuracy: 0.8320 Val Loss: 1.7119 Val Accuracy: 0.8222\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  1.7428\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  1.6818\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  1.6828\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  1.6885\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  1.6133\n",
            "End of Epoch: 2 Train Accuracy: 0.8485 Val Loss: 1.6695 Val Accuracy: 0.8367\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  1.7305\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  1.5918\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  1.6149\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  1.6367\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  1.6885\n",
            "End of Epoch: 3 Train Accuracy: 0.8620 Val Loss: 1.6578 Val Accuracy: 0.8503\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  1.6026\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  1.6142\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  1.6799\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  1.5967\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  1.6458\n",
            "End of Epoch: 4 Train Accuracy: 0.8635 Val Loss: 1.6529 Val Accuracy: 0.8523\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  1.6580\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  1.6427\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  1.6423\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  1.6073\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  1.6465\n",
            "End of Epoch: 5 Train Accuracy: 0.8763 Val Loss: 1.6352 Val Accuracy: 0.8655\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  1.6090\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  1.6614\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  1.6620\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  1.6041\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  1.6378\n",
            "End of Epoch: 6 Train Accuracy: 0.8794 Val Loss: 1.6390 Val Accuracy: 0.8652\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  1.6283\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  1.6161\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  1.6575\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  1.5900\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  1.5800\n",
            "End of Epoch: 7 Train Accuracy: 0.8830 Val Loss: 1.6327 Val Accuracy: 0.8698\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  1.5881\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  1.6006\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  1.6850\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  1.6797\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  1.6390\n",
            "End of Epoch: 8 Train Accuracy: 0.8849 Val Loss: 1.6387 Val Accuracy: 0.8695\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  1.6495\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  1.6790\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  1.6654\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  1.6570\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  1.6329\n",
            "End of Epoch: 9 Train Accuracy: 0.8903 Val Loss: 1.6257 Val Accuracy: 0.8762\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  1.6280\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  1.5798\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  1.6373\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  1.6476\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  1.6115\n",
            "End of Epoch: 10 Train Accuracy: 0.8834 Val Loss: 1.6259 Val Accuracy: 0.8713\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▇█████████</td></tr><tr><td>Train Loss</td><td>█▃▁▂▁▄▁▁▃▄</td></tr><tr><td>Val Accuracy</td><td>▁▇█████████</td></tr><tr><td>Val Loss</td><td>█▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.88341</td></tr><tr><td>Train Loss</td><td>1.6656</td></tr><tr><td>Val Accuracy</td><td>0.87133</td></tr><tr><td>Val Loss</td><td>1.62586</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_cross_entropy|lr=0.001|batch_64|act_tanh|hid_4|neurons_32|nrns_10|init_Xavier674</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/vdgb96yi' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/vdgb96yi</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_180117-vdgb96yi/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fx7d4c55 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_180202-fx7d4c55</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/fx7d4c55' target=\"_blank\">dry-sweep-64</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/fx7d4c55' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/fx7d4c55</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0851 Val Loss: 25.3066 Val Accuracy: 0.0818\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/843): \tTrain Loss  26.3358\n",
            "Epoch(1/5)\t Batch(200/843): \tTrain Loss  4.8800\n",
            "Epoch(1/5)\t Batch(400/843): \tTrain Loss  2.3121\n",
            "Epoch(1/5)\t Batch(600/843): \tTrain Loss  2.2993\n",
            "Epoch(1/5)\t Batch(800/843): \tTrain Loss  2.3019\n",
            "End of Epoch: 1 Train Accuracy: 0.1005 Val Loss: 2.3090 Val Accuracy: 0.0953\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/843): \tTrain Loss  2.3021\n",
            "Epoch(2/5)\t Batch(200/843): \tTrain Loss  2.3032\n",
            "Epoch(2/5)\t Batch(400/843): \tTrain Loss  2.3031\n",
            "Epoch(2/5)\t Batch(600/843): \tTrain Loss  2.2987\n",
            "Epoch(2/5)\t Batch(800/843): \tTrain Loss  2.3028\n",
            "End of Epoch: 2 Train Accuracy: 0.0999 Val Loss: 2.3046 Val Accuracy: 0.1005\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/843): \tTrain Loss  2.3022\n",
            "Epoch(3/5)\t Batch(200/843): \tTrain Loss  2.3038\n",
            "Epoch(3/5)\t Batch(400/843): \tTrain Loss  2.3029\n",
            "Epoch(3/5)\t Batch(600/843): \tTrain Loss  2.3012\n",
            "Epoch(3/5)\t Batch(800/843): \tTrain Loss  2.3037\n",
            "End of Epoch: 3 Train Accuracy: 0.0991 Val Loss: 2.3045 Val Accuracy: 0.1082\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/843): \tTrain Loss  2.2997\n",
            "Epoch(4/5)\t Batch(200/843): \tTrain Loss  2.3015\n",
            "Epoch(4/5)\t Batch(400/843): \tTrain Loss  2.3016\n",
            "Epoch(4/5)\t Batch(600/843): \tTrain Loss  2.3017\n",
            "Epoch(4/5)\t Batch(800/843): \tTrain Loss  2.3015\n",
            "End of Epoch: 4 Train Accuracy: 0.0991 Val Loss: 2.3044 Val Accuracy: 0.1082\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/843): \tTrain Loss  2.3008\n",
            "Epoch(5/5)\t Batch(200/843): \tTrain Loss  2.3039\n",
            "Epoch(5/5)\t Batch(400/843): \tTrain Loss  2.3048\n",
            "Epoch(5/5)\t Batch(600/843): \tTrain Loss  2.3037\n",
            "Epoch(5/5)\t Batch(800/843): \tTrain Loss  2.3042\n",
            "End of Epoch: 5 Train Accuracy: 0.1005 Val Loss: 2.3046 Val Accuracy: 0.0953\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁██▇▇█</td></tr><tr><td>Train Loss</td><td>▁▆▆█▆</td></tr><tr><td>Val Accuracy</td><td>▁▅▆██▅</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.10052</td></tr><tr><td>Train Loss</td><td>2.30361</td></tr><tr><td>Val Accuracy</td><td>0.09533</td></tr><tr><td>Val Loss</td><td>2.30458</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nadam|loss_cross_entropy|lr=0.001|batch_64|act_relu|hid_4|neurons_32|nrns_5|init_Random751</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/fx7d4c55' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/fx7d4c55</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_180202-fx7d4c55/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f9s8f9ok with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nag\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_180228-f9s8f9ok</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/f9s8f9ok' target=\"_blank\">comic-sweep-65</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/f9s8f9ok' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/f9s8f9ok</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.1001 Val Loss: 0.0917 Val Accuracy: 0.0993\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/3375): \tTrain Loss  0.0916\n",
            "Epoch(1/5)\t Batch(200/3375): \tTrain Loss  0.0893\n",
            "Epoch(1/5)\t Batch(400/3375): \tTrain Loss  0.0906\n",
            "Epoch(1/5)\t Batch(600/3375): \tTrain Loss  0.0897\n",
            "Epoch(1/5)\t Batch(800/3375): \tTrain Loss  0.0904\n",
            "Epoch(1/5)\t Batch(1000/3375): \tTrain Loss  0.0895\n",
            "Epoch(1/5)\t Batch(1200/3375): \tTrain Loss  0.0898\n",
            "Epoch(1/5)\t Batch(1400/3375): \tTrain Loss  0.0898\n",
            "Epoch(1/5)\t Batch(1600/3375): \tTrain Loss  0.0903\n",
            "Epoch(1/5)\t Batch(1800/3375): \tTrain Loss  0.0901\n",
            "Epoch(1/5)\t Batch(2000/3375): \tTrain Loss  0.0900\n",
            "Epoch(1/5)\t Batch(2200/3375): \tTrain Loss  0.0902\n",
            "Epoch(1/5)\t Batch(2400/3375): \tTrain Loss  0.0898\n",
            "Epoch(1/5)\t Batch(2600/3375): \tTrain Loss  0.0900\n",
            "Epoch(1/5)\t Batch(2800/3375): \tTrain Loss  0.0899\n",
            "Epoch(1/5)\t Batch(3000/3375): \tTrain Loss  0.0900\n",
            "Epoch(1/5)\t Batch(3200/3375): \tTrain Loss  0.0899\n",
            "End of Epoch: 1 Train Accuracy: 0.1003 Val Loss: 0.0900 Val Accuracy: 0.0977\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/3375): \tTrain Loss  0.0902\n",
            "Epoch(2/5)\t Batch(200/3375): \tTrain Loss  0.0901\n",
            "Epoch(2/5)\t Batch(400/3375): \tTrain Loss  0.0899\n",
            "Epoch(2/5)\t Batch(600/3375): \tTrain Loss  0.0900\n",
            "Epoch(2/5)\t Batch(800/3375): \tTrain Loss  0.0900\n",
            "Epoch(2/5)\t Batch(1000/3375): \tTrain Loss  0.0900\n",
            "Epoch(2/5)\t Batch(1200/3375): \tTrain Loss  0.0899\n",
            "Epoch(2/5)\t Batch(1400/3375): \tTrain Loss  0.0901\n",
            "Epoch(2/5)\t Batch(1600/3375): \tTrain Loss  0.0902\n",
            "Epoch(2/5)\t Batch(1800/3375): \tTrain Loss  0.0902\n",
            "Epoch(2/5)\t Batch(2000/3375): \tTrain Loss  0.0900\n",
            "Epoch(2/5)\t Batch(2200/3375): \tTrain Loss  0.0900\n",
            "Epoch(2/5)\t Batch(2400/3375): \tTrain Loss  0.0899\n",
            "Epoch(2/5)\t Batch(2600/3375): \tTrain Loss  0.0899\n",
            "Epoch(2/5)\t Batch(2800/3375): \tTrain Loss  0.0898\n",
            "Epoch(2/5)\t Batch(3000/3375): \tTrain Loss  0.0899\n",
            "Epoch(2/5)\t Batch(3200/3375): \tTrain Loss  0.0903\n",
            "End of Epoch: 2 Train Accuracy: 0.1001 Val Loss: 0.0900 Val Accuracy: 0.0995\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/3375): \tTrain Loss  0.0899\n",
            "Epoch(3/5)\t Batch(200/3375): \tTrain Loss  0.0901\n",
            "Epoch(3/5)\t Batch(400/3375): \tTrain Loss  0.0900\n",
            "Epoch(3/5)\t Batch(600/3375): \tTrain Loss  0.0900\n",
            "Epoch(3/5)\t Batch(800/3375): \tTrain Loss  0.0900\n",
            "Epoch(3/5)\t Batch(1000/3375): \tTrain Loss  0.0899\n",
            "Epoch(3/5)\t Batch(1200/3375): \tTrain Loss  0.0899\n",
            "Epoch(3/5)\t Batch(1400/3375): \tTrain Loss  0.0901\n",
            "Epoch(3/5)\t Batch(1600/3375): \tTrain Loss  0.0901\n",
            "Epoch(3/5)\t Batch(1800/3375): \tTrain Loss  0.0899\n",
            "Epoch(3/5)\t Batch(2000/3375): \tTrain Loss  0.0899\n",
            "Epoch(3/5)\t Batch(2200/3375): \tTrain Loss  0.0902\n",
            "Epoch(3/5)\t Batch(2400/3375): \tTrain Loss  0.0901\n",
            "Epoch(3/5)\t Batch(2600/3375): \tTrain Loss  0.0903\n",
            "Epoch(3/5)\t Batch(2800/3375): \tTrain Loss  0.0900\n",
            "Epoch(3/5)\t Batch(3000/3375): \tTrain Loss  0.0899\n",
            "Epoch(3/5)\t Batch(3200/3375): \tTrain Loss  0.0900\n",
            "End of Epoch: 3 Train Accuracy: 0.1003 Val Loss: 0.0900 Val Accuracy: 0.0977\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/3375): \tTrain Loss  0.0899\n",
            "Epoch(4/5)\t Batch(200/3375): \tTrain Loss  0.0900\n",
            "Epoch(4/5)\t Batch(400/3375): \tTrain Loss  0.0900\n",
            "Epoch(4/5)\t Batch(600/3375): \tTrain Loss  0.0901\n",
            "Epoch(4/5)\t Batch(800/3375): \tTrain Loss  0.0899\n",
            "Epoch(4/5)\t Batch(1000/3375): \tTrain Loss  0.0898\n",
            "Epoch(4/5)\t Batch(1200/3375): \tTrain Loss  0.0899\n",
            "Epoch(4/5)\t Batch(1400/3375): \tTrain Loss  0.0899\n",
            "Epoch(4/5)\t Batch(1600/3375): \tTrain Loss  0.0902\n",
            "Epoch(4/5)\t Batch(1800/3375): \tTrain Loss  0.0902\n",
            "Epoch(4/5)\t Batch(2000/3375): \tTrain Loss  0.0901\n",
            "Epoch(4/5)\t Batch(2200/3375): \tTrain Loss  0.0901\n",
            "Epoch(4/5)\t Batch(2400/3375): \tTrain Loss  0.0900\n",
            "Epoch(4/5)\t Batch(2600/3375): \tTrain Loss  0.0900\n",
            "Epoch(4/5)\t Batch(2800/3375): \tTrain Loss  0.0900\n",
            "Epoch(4/5)\t Batch(3000/3375): \tTrain Loss  0.0899\n",
            "Epoch(4/5)\t Batch(3200/3375): \tTrain Loss  0.0899\n",
            "End of Epoch: 4 Train Accuracy: 0.1003 Val Loss: 0.0900 Val Accuracy: 0.0977\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/3375): \tTrain Loss  0.0901\n",
            "Epoch(5/5)\t Batch(200/3375): \tTrain Loss  0.0899\n",
            "Epoch(5/5)\t Batch(400/3375): \tTrain Loss  0.0899\n",
            "Epoch(5/5)\t Batch(600/3375): \tTrain Loss  0.0899\n",
            "Epoch(5/5)\t Batch(800/3375): \tTrain Loss  0.0900\n",
            "Epoch(5/5)\t Batch(1000/3375): \tTrain Loss  0.0902\n",
            "Epoch(5/5)\t Batch(1200/3375): \tTrain Loss  0.0899\n",
            "Epoch(5/5)\t Batch(1400/3375): \tTrain Loss  0.0899\n",
            "Epoch(5/5)\t Batch(1600/3375): \tTrain Loss  0.0901\n",
            "Epoch(5/5)\t Batch(1800/3375): \tTrain Loss  0.0899\n",
            "Epoch(5/5)\t Batch(2000/3375): \tTrain Loss  0.0900\n",
            "Epoch(5/5)\t Batch(2200/3375): \tTrain Loss  0.0902\n",
            "Epoch(5/5)\t Batch(2400/3375): \tTrain Loss  0.0901\n",
            "Epoch(5/5)\t Batch(2600/3375): \tTrain Loss  0.0901\n",
            "Epoch(5/5)\t Batch(2800/3375): \tTrain Loss  0.0900\n",
            "Epoch(5/5)\t Batch(3000/3375): \tTrain Loss  0.0901\n",
            "Epoch(5/5)\t Batch(3200/3375): \tTrain Loss  0.0900\n",
            "End of Epoch: 5 Train Accuracy: 0.1002 Val Loss: 0.0900 Val Accuracy: 0.0980\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▂█▁██▇</td></tr><tr><td>Train Loss</td><td>█▅▃▁▅</td></tr><tr><td>Val Accuracy</td><td>▇▁█▁▁▂</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.10022</td></tr><tr><td>Train Loss</td><td>0.08999</td></tr><tr><td>Val Accuracy</td><td>0.098</td></tr><tr><td>Val Loss</td><td>0.09</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nag|loss_mse|lr=0.001|batch_16|act_sigmoid|hid_4|neurons_32|nrns_5|init_Xavier704</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/f9s8f9ok' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/f9s8f9ok</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_180228-f9s8f9ok/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: immizied with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_180319-immizied</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/immizied' target=\"_blank\">sparkling-sweep-66</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/immizied' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/immizied</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.0906 Val Loss: 0.1167 Val Accuracy: 0.0900\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/1687): \tTrain Loss  0.1275\n",
            "Epoch(1/5)\t Batch(200/1687): \tTrain Loss  0.1134\n",
            "Epoch(1/5)\t Batch(400/1687): \tTrain Loss  0.1125\n",
            "Epoch(1/5)\t Batch(600/1687): \tTrain Loss  0.1151\n",
            "Epoch(1/5)\t Batch(800/1687): \tTrain Loss  0.1168\n",
            "Epoch(1/5)\t Batch(1000/1687): \tTrain Loss  0.0998\n",
            "Epoch(1/5)\t Batch(1200/1687): \tTrain Loss  0.1052\n",
            "Epoch(1/5)\t Batch(1400/1687): \tTrain Loss  0.1181\n",
            "Epoch(1/5)\t Batch(1600/1687): \tTrain Loss  0.1090\n",
            "End of Epoch: 1 Train Accuracy: 0.1283 Val Loss: 0.1074 Val Accuracy: 0.1277\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/1687): \tTrain Loss  0.1030\n",
            "Epoch(2/5)\t Batch(200/1687): \tTrain Loss  0.1021\n",
            "Epoch(2/5)\t Batch(400/1687): \tTrain Loss  0.1117\n",
            "Epoch(2/5)\t Batch(600/1687): \tTrain Loss  0.1163\n",
            "Epoch(2/5)\t Batch(800/1687): \tTrain Loss  0.1029\n",
            "Epoch(2/5)\t Batch(1000/1687): \tTrain Loss  0.0997\n",
            "Epoch(2/5)\t Batch(1200/1687): \tTrain Loss  0.1070\n",
            "Epoch(2/5)\t Batch(1400/1687): \tTrain Loss  0.1099\n",
            "Epoch(2/5)\t Batch(1600/1687): \tTrain Loss  0.1021\n",
            "End of Epoch: 2 Train Accuracy: 0.1430 Val Loss: 0.1050 Val Accuracy: 0.1425\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/1687): \tTrain Loss  0.0988\n",
            "Epoch(3/5)\t Batch(200/1687): \tTrain Loss  0.0988\n",
            "Epoch(3/5)\t Batch(400/1687): \tTrain Loss  0.1082\n",
            "Epoch(3/5)\t Batch(600/1687): \tTrain Loss  0.0974\n",
            "Epoch(3/5)\t Batch(800/1687): \tTrain Loss  0.1020\n",
            "Epoch(3/5)\t Batch(1000/1687): \tTrain Loss  0.1029\n",
            "Epoch(3/5)\t Batch(1200/1687): \tTrain Loss  0.1109\n",
            "Epoch(3/5)\t Batch(1400/1687): \tTrain Loss  0.0974\n",
            "Epoch(3/5)\t Batch(1600/1687): \tTrain Loss  0.0997\n",
            "End of Epoch: 3 Train Accuracy: 0.1588 Val Loss: 0.1032 Val Accuracy: 0.1577\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/1687): \tTrain Loss  0.0958\n",
            "Epoch(4/5)\t Batch(200/1687): \tTrain Loss  0.1071\n",
            "Epoch(4/5)\t Batch(400/1687): \tTrain Loss  0.1076\n",
            "Epoch(4/5)\t Batch(600/1687): \tTrain Loss  0.1064\n",
            "Epoch(4/5)\t Batch(800/1687): \tTrain Loss  0.0986\n",
            "Epoch(4/5)\t Batch(1000/1687): \tTrain Loss  0.1105\n",
            "Epoch(4/5)\t Batch(1200/1687): \tTrain Loss  0.0924\n",
            "Epoch(4/5)\t Batch(1400/1687): \tTrain Loss  0.0985\n",
            "Epoch(4/5)\t Batch(1600/1687): \tTrain Loss  0.1013\n",
            "End of Epoch: 4 Train Accuracy: 0.1786 Val Loss: 0.1016 Val Accuracy: 0.1777\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/1687): \tTrain Loss  0.1046\n",
            "Epoch(5/5)\t Batch(200/1687): \tTrain Loss  0.1046\n",
            "Epoch(5/5)\t Batch(400/1687): \tTrain Loss  0.0953\n",
            "Epoch(5/5)\t Batch(600/1687): \tTrain Loss  0.0962\n",
            "Epoch(5/5)\t Batch(800/1687): \tTrain Loss  0.0973\n",
            "Epoch(5/5)\t Batch(1000/1687): \tTrain Loss  0.1013\n",
            "Epoch(5/5)\t Batch(1200/1687): \tTrain Loss  0.0906\n",
            "Epoch(5/5)\t Batch(1400/1687): \tTrain Loss  0.0952\n",
            "Epoch(5/5)\t Batch(1600/1687): \tTrain Loss  0.0994\n",
            "End of Epoch: 5 Train Accuracy: 0.1966 Val Loss: 0.1002 Val Accuracy: 0.1983\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▃▄▆▇█</td></tr><tr><td>Train Loss</td><td>▆█▃▁▄</td></tr><tr><td>Val Accuracy</td><td>▁▃▄▅▇█</td></tr><tr><td>Val Loss</td><td>█▄▃▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.19663</td></tr><tr><td>Train Loss</td><td>0.10652</td></tr><tr><td>Val Accuracy</td><td>0.19833</td></tr><tr><td>Val Loss</td><td>0.1002</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_sgd|loss_mse|lr=0.001|batch_32|act_sigmoid|hid_3|neurons_32|nrns_5|init_Random472</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/immizied' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/immizied</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_180319-immizied/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0lfv6o7q with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_180354-0lfv6o7q</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/0lfv6o7q' target=\"_blank\">jumping-sweep-67</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/0lfv6o7q' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/0lfv6o7q</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.1000 Val Loss: 0.1794 Val Accuracy: 0.1028\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  0.1844\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  0.1781\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  0.1687\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  0.1719\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  0.1651\n",
            "End of Epoch: 1 Train Accuracy: 0.1444 Val Loss: 0.1703 Val Accuracy: 0.1483\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  0.1656\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  0.1625\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  0.1594\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  0.1812\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  0.1625\n",
            "End of Epoch: 2 Train Accuracy: 0.1560 Val Loss: 0.1682 Val Accuracy: 0.1587\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  0.1844\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  0.1687\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  0.1687\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  0.1750\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  0.1719\n",
            "End of Epoch: 3 Train Accuracy: 0.1602 Val Loss: 0.1672 Val Accuracy: 0.1637\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  0.1562\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  0.1687\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  0.1656\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  0.1875\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  0.1719\n",
            "End of Epoch: 4 Train Accuracy: 0.1640 Val Loss: 0.1668 Val Accuracy: 0.1657\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  0.1469\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  0.1625\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  0.1687\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  0.1687\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  0.1719\n",
            "End of Epoch: 5 Train Accuracy: 0.1706 Val Loss: 0.1655 Val Accuracy: 0.1725\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  0.1750\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  0.1625\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  0.1594\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  0.1812\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  0.1812\n",
            "End of Epoch: 6 Train Accuracy: 0.1792 Val Loss: 0.1637 Val Accuracy: 0.1813\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  0.1594\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  0.1531\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  0.1625\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  0.1562\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  0.1594\n",
            "End of Epoch: 7 Train Accuracy: 0.1893 Val Loss: 0.1614 Val Accuracy: 0.1932\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  0.1719\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  0.1469\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  0.1625\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  0.1594\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  0.1562\n",
            "End of Epoch: 8 Train Accuracy: 0.1909 Val Loss: 0.1609 Val Accuracy: 0.1952\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  0.1781\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  0.1625\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  0.1594\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  0.1531\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  0.1469\n",
            "End of Epoch: 9 Train Accuracy: 0.1935 Val Loss: 0.1606 Val Accuracy: 0.1967\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  0.1656\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  0.1531\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  0.1625\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  0.1781\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  0.1656\n",
            "End of Epoch: 10 Train Accuracy: 0.1927 Val Loss: 0.1607 Val Accuracy: 0.1963\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▄▅▆▆▆▇████</td></tr><tr><td>Train Loss</td><td>▅▃▃▅▃▆▅▁█▁</td></tr><tr><td>Val Accuracy</td><td>▁▄▅▆▆▆▇████</td></tr><tr><td>Val Loss</td><td>█▅▄▃▃▃▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.19269</td></tr><tr><td>Train Loss</td><td>0.15833</td></tr><tr><td>Val Accuracy</td><td>0.19633</td></tr><tr><td>Val Loss</td><td>0.16071</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_adam|loss_mse|lr=0.001|batch_64|act_relu|hid_5|neurons_32|nrns_10|init_Random15</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/0lfv6o7q' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/0lfv6o7q</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_180354-0lfv6o7q/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: q5f5598y with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_180459-q5f5598y</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/q5f5598y' target=\"_blank\">misty-sweep-68</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/q5f5598y' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/q5f5598y</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.0999 Val Loss: 2.3300 Val Accuracy: 0.1005\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/3375): \tTrain Loss  2.2680\n",
            "Epoch(1/5)\t Batch(200/3375): \tTrain Loss  2.2349\n",
            "Epoch(1/5)\t Batch(400/3375): \tTrain Loss  2.1505\n",
            "Epoch(1/5)\t Batch(600/3375): \tTrain Loss  2.1155\n",
            "Epoch(1/5)\t Batch(800/3375): \tTrain Loss  2.0561\n",
            "Epoch(1/5)\t Batch(1000/3375): \tTrain Loss  1.9757\n",
            "Epoch(1/5)\t Batch(1200/3375): \tTrain Loss  1.8288\n",
            "Epoch(1/5)\t Batch(1400/3375): \tTrain Loss  1.9100\n",
            "Epoch(1/5)\t Batch(1600/3375): \tTrain Loss  1.8663\n",
            "Epoch(1/5)\t Batch(1800/3375): \tTrain Loss  1.7824\n",
            "Epoch(1/5)\t Batch(2000/3375): \tTrain Loss  1.7411\n",
            "Epoch(1/5)\t Batch(2200/3375): \tTrain Loss  1.6737\n",
            "Epoch(1/5)\t Batch(2400/3375): \tTrain Loss  1.7273\n",
            "Epoch(1/5)\t Batch(2600/3375): \tTrain Loss  1.7310\n",
            "Epoch(1/5)\t Batch(2800/3375): \tTrain Loss  1.7505\n",
            "Epoch(1/5)\t Batch(3000/3375): \tTrain Loss  1.8161\n",
            "Epoch(1/5)\t Batch(3200/3375): \tTrain Loss  1.6980\n",
            "End of Epoch: 1 Train Accuracy: 0.7966 Val Loss: 1.7479 Val Accuracy: 0.7883\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/3375): \tTrain Loss  1.7452\n",
            "Epoch(2/5)\t Batch(200/3375): \tTrain Loss  1.7170\n",
            "Epoch(2/5)\t Batch(400/3375): \tTrain Loss  1.9099\n",
            "Epoch(2/5)\t Batch(600/3375): \tTrain Loss  1.6293\n",
            "Epoch(2/5)\t Batch(800/3375): \tTrain Loss  1.7325\n",
            "Epoch(2/5)\t Batch(1000/3375): \tTrain Loss  1.8291\n",
            "Epoch(2/5)\t Batch(1200/3375): \tTrain Loss  1.6205\n",
            "Epoch(2/5)\t Batch(1400/3375): \tTrain Loss  1.7237\n",
            "Epoch(2/5)\t Batch(1600/3375): \tTrain Loss  1.7783\n",
            "Epoch(2/5)\t Batch(1800/3375): \tTrain Loss  1.8115\n",
            "Epoch(2/5)\t Batch(2000/3375): \tTrain Loss  1.6795\n",
            "Epoch(2/5)\t Batch(2200/3375): \tTrain Loss  1.6484\n",
            "Epoch(2/5)\t Batch(2400/3375): \tTrain Loss  1.5365\n",
            "Epoch(2/5)\t Batch(2600/3375): \tTrain Loss  1.6632\n",
            "Epoch(2/5)\t Batch(2800/3375): \tTrain Loss  1.6858\n",
            "Epoch(2/5)\t Batch(3000/3375): \tTrain Loss  1.7661\n",
            "Epoch(2/5)\t Batch(3200/3375): \tTrain Loss  1.6571\n",
            "End of Epoch: 2 Train Accuracy: 0.8442 Val Loss: 1.6724 Val Accuracy: 0.8283\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/3375): \tTrain Loss  1.6698\n",
            "Epoch(3/5)\t Batch(200/3375): \tTrain Loss  1.6863\n",
            "Epoch(3/5)\t Batch(400/3375): \tTrain Loss  1.7151\n",
            "Epoch(3/5)\t Batch(600/3375): \tTrain Loss  1.7595\n",
            "Epoch(3/5)\t Batch(800/3375): \tTrain Loss  1.6434\n",
            "Epoch(3/5)\t Batch(1000/3375): \tTrain Loss  1.5775\n",
            "Epoch(3/5)\t Batch(1200/3375): \tTrain Loss  1.6072\n",
            "Epoch(3/5)\t Batch(1400/3375): \tTrain Loss  1.5853\n",
            "Epoch(3/5)\t Batch(1600/3375): \tTrain Loss  1.6348\n",
            "Epoch(3/5)\t Batch(1800/3375): \tTrain Loss  1.5769\n",
            "Epoch(3/5)\t Batch(2000/3375): \tTrain Loss  1.6708\n",
            "Epoch(3/5)\t Batch(2200/3375): \tTrain Loss  1.6351\n",
            "Epoch(3/5)\t Batch(2400/3375): \tTrain Loss  1.6587\n",
            "Epoch(3/5)\t Batch(2600/3375): \tTrain Loss  1.6816\n",
            "Epoch(3/5)\t Batch(2800/3375): \tTrain Loss  1.5387\n",
            "Epoch(3/5)\t Batch(3000/3375): \tTrain Loss  1.5493\n",
            "Epoch(3/5)\t Batch(3200/3375): \tTrain Loss  1.7447\n",
            "End of Epoch: 3 Train Accuracy: 0.8703 Val Loss: 1.6504 Val Accuracy: 0.8542\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/3375): \tTrain Loss  1.6817\n",
            "Epoch(4/5)\t Batch(200/3375): \tTrain Loss  1.7691\n",
            "Epoch(4/5)\t Batch(400/3375): \tTrain Loss  1.5857\n",
            "Epoch(4/5)\t Batch(600/3375): \tTrain Loss  1.6046\n",
            "Epoch(4/5)\t Batch(800/3375): \tTrain Loss  1.5847\n",
            "Epoch(4/5)\t Batch(1000/3375): \tTrain Loss  1.5922\n",
            "Epoch(4/5)\t Batch(1200/3375): \tTrain Loss  1.6144\n",
            "Epoch(4/5)\t Batch(1400/3375): \tTrain Loss  1.7388\n",
            "Epoch(4/5)\t Batch(1600/3375): \tTrain Loss  1.6919\n",
            "Epoch(4/5)\t Batch(1800/3375): \tTrain Loss  1.6440\n",
            "Epoch(4/5)\t Batch(2000/3375): \tTrain Loss  1.6064\n",
            "Epoch(4/5)\t Batch(2200/3375): \tTrain Loss  1.6254\n",
            "Epoch(4/5)\t Batch(2400/3375): \tTrain Loss  1.6561\n",
            "Epoch(4/5)\t Batch(2600/3375): \tTrain Loss  1.5558\n",
            "Epoch(4/5)\t Batch(2800/3375): \tTrain Loss  1.6518\n",
            "Epoch(4/5)\t Batch(3000/3375): \tTrain Loss  1.6425\n",
            "Epoch(4/5)\t Batch(3200/3375): \tTrain Loss  1.7537\n",
            "End of Epoch: 4 Train Accuracy: 0.8748 Val Loss: 1.6428 Val Accuracy: 0.8583\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/3375): \tTrain Loss  1.6703\n",
            "Epoch(5/5)\t Batch(200/3375): \tTrain Loss  1.6750\n",
            "Epoch(5/5)\t Batch(400/3375): \tTrain Loss  1.6025\n",
            "Epoch(5/5)\t Batch(600/3375): \tTrain Loss  1.6111\n",
            "Epoch(5/5)\t Batch(800/3375): \tTrain Loss  1.5736\n",
            "Epoch(5/5)\t Batch(1000/3375): \tTrain Loss  1.7052\n",
            "Epoch(5/5)\t Batch(1200/3375): \tTrain Loss  1.6348\n",
            "Epoch(5/5)\t Batch(1400/3375): \tTrain Loss  1.5839\n",
            "Epoch(5/5)\t Batch(1600/3375): \tTrain Loss  1.7162\n",
            "Epoch(5/5)\t Batch(1800/3375): \tTrain Loss  1.6151\n",
            "Epoch(5/5)\t Batch(2000/3375): \tTrain Loss  1.6767\n",
            "Epoch(5/5)\t Batch(2200/3375): \tTrain Loss  1.7328\n",
            "Epoch(5/5)\t Batch(2400/3375): \tTrain Loss  1.6562\n",
            "Epoch(5/5)\t Batch(2600/3375): \tTrain Loss  1.6962\n",
            "Epoch(5/5)\t Batch(2800/3375): \tTrain Loss  1.6195\n",
            "Epoch(5/5)\t Batch(3000/3375): \tTrain Loss  1.5978\n",
            "Epoch(5/5)\t Batch(3200/3375): \tTrain Loss  1.5420\n",
            "End of Epoch: 5 Train Accuracy: 0.8822 Val Loss: 1.6309 Val Accuracy: 0.8658\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▇████</td></tr><tr><td>Train Loss</td><td>█▇█▄▁</td></tr><tr><td>Val Accuracy</td><td>▁▇████</td></tr><tr><td>Val Loss</td><td>█▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.88222</td></tr><tr><td>Train Loss</td><td>1.54788</td></tr><tr><td>Val Accuracy</td><td>0.86583</td></tr><tr><td>Val Loss</td><td>1.63089</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_cross_entropy|lr=0.001|batch_16|act_sigmoid|hid_4|neurons_64|nrns_5|init_Xavier453</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/q5f5598y' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/q5f5598y</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_180459-q5f5598y/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vyks19pd with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_180559-vyks19pd</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/vyks19pd' target=\"_blank\">lucky-sweep-69</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/vyks19pd' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/vyks19pd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.0826 Val Loss: 2.2987 Val Accuracy: 0.0830\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/3375): \tTrain Loss  2.2878\n",
            "Epoch(1/5)\t Batch(200/3375): \tTrain Loss  2.2915\n",
            "Epoch(1/5)\t Batch(400/3375): \tTrain Loss  2.2940\n",
            "Epoch(1/5)\t Batch(600/3375): \tTrain Loss  2.2991\n",
            "Epoch(1/5)\t Batch(800/3375): \tTrain Loss  2.2986\n",
            "Epoch(1/5)\t Batch(1000/3375): \tTrain Loss  2.3003\n",
            "Epoch(1/5)\t Batch(1200/3375): \tTrain Loss  2.3003\n",
            "Epoch(1/5)\t Batch(1400/3375): \tTrain Loss  2.3019\n",
            "Epoch(1/5)\t Batch(1600/3375): \tTrain Loss  2.3018\n",
            "Epoch(1/5)\t Batch(1800/3375): \tTrain Loss  2.3017\n",
            "Epoch(1/5)\t Batch(2000/3375): \tTrain Loss  2.3015\n",
            "Epoch(1/5)\t Batch(2200/3375): \tTrain Loss  2.3029\n",
            "Epoch(1/5)\t Batch(2400/3375): \tTrain Loss  2.3027\n",
            "Epoch(1/5)\t Batch(2600/3375): \tTrain Loss  2.3026\n",
            "Epoch(1/5)\t Batch(2800/3375): \tTrain Loss  2.3033\n",
            "Epoch(1/5)\t Batch(3000/3375): \tTrain Loss  2.3036\n",
            "Epoch(1/5)\t Batch(3200/3375): \tTrain Loss  2.3021\n",
            "End of Epoch: 1 Train Accuracy: 0.1007 Val Loss: 2.3026 Val Accuracy: 0.0933\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/3375): \tTrain Loss  2.3026\n",
            "Epoch(2/5)\t Batch(200/3375): \tTrain Loss  2.3030\n",
            "Epoch(2/5)\t Batch(400/3375): \tTrain Loss  2.3024\n",
            "Epoch(2/5)\t Batch(600/3375): \tTrain Loss  2.3028\n",
            "Epoch(2/5)\t Batch(800/3375): \tTrain Loss  2.3027\n",
            "Epoch(2/5)\t Batch(1000/3375): \tTrain Loss  2.3027\n",
            "Epoch(2/5)\t Batch(1200/3375): \tTrain Loss  2.3030\n",
            "Epoch(2/5)\t Batch(1400/3375): \tTrain Loss  2.3026\n",
            "Epoch(2/5)\t Batch(1600/3375): \tTrain Loss  2.3023\n",
            "Epoch(2/5)\t Batch(1800/3375): \tTrain Loss  2.3026\n",
            "Epoch(2/5)\t Batch(2000/3375): \tTrain Loss  2.3027\n",
            "Epoch(2/5)\t Batch(2200/3375): \tTrain Loss  2.3029\n",
            "Epoch(2/5)\t Batch(2400/3375): \tTrain Loss  2.3026\n",
            "Epoch(2/5)\t Batch(2600/3375): \tTrain Loss  2.3024\n",
            "Epoch(2/5)\t Batch(2800/3375): \tTrain Loss  2.3030\n",
            "Epoch(2/5)\t Batch(3000/3375): \tTrain Loss  2.3028\n",
            "Epoch(2/5)\t Batch(3200/3375): \tTrain Loss  2.3026\n",
            "End of Epoch: 2 Train Accuracy: 0.1001 Val Loss: 2.3026 Val Accuracy: 0.0995\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/3375): \tTrain Loss  2.3025\n",
            "Epoch(3/5)\t Batch(200/3375): \tTrain Loss  2.3025\n",
            "Epoch(3/5)\t Batch(400/3375): \tTrain Loss  2.3030\n",
            "Epoch(3/5)\t Batch(600/3375): \tTrain Loss  2.3025\n",
            "Epoch(3/5)\t Batch(800/3375): \tTrain Loss  2.3024\n",
            "Epoch(3/5)\t Batch(1000/3375): \tTrain Loss  2.3024\n",
            "Epoch(3/5)\t Batch(1200/3375): \tTrain Loss  2.3026\n",
            "Epoch(3/5)\t Batch(1400/3375): \tTrain Loss  2.3025\n",
            "Epoch(3/5)\t Batch(1600/3375): \tTrain Loss  2.3025\n",
            "Epoch(3/5)\t Batch(1800/3375): \tTrain Loss  2.3025\n",
            "Epoch(3/5)\t Batch(2000/3375): \tTrain Loss  2.3020\n",
            "Epoch(3/5)\t Batch(2200/3375): \tTrain Loss  2.3027\n",
            "Epoch(3/5)\t Batch(2400/3375): \tTrain Loss  2.3024\n",
            "Epoch(3/5)\t Batch(2600/3375): \tTrain Loss  2.3025\n",
            "Epoch(3/5)\t Batch(2800/3375): \tTrain Loss  2.3023\n",
            "Epoch(3/5)\t Batch(3000/3375): \tTrain Loss  2.3029\n",
            "Epoch(3/5)\t Batch(3200/3375): \tTrain Loss  2.3022\n",
            "End of Epoch: 3 Train Accuracy: 0.0993 Val Loss: 2.3026 Val Accuracy: 0.1065\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/3375): \tTrain Loss  2.3029\n",
            "Epoch(4/5)\t Batch(200/3375): \tTrain Loss  2.3028\n",
            "Epoch(4/5)\t Batch(400/3375): \tTrain Loss  2.3023\n",
            "Epoch(4/5)\t Batch(600/3375): \tTrain Loss  2.3023\n",
            "Epoch(4/5)\t Batch(800/3375): \tTrain Loss  2.3024\n",
            "Epoch(4/5)\t Batch(1000/3375): \tTrain Loss  2.3031\n",
            "Epoch(4/5)\t Batch(1200/3375): \tTrain Loss  2.3026\n",
            "Epoch(4/5)\t Batch(1400/3375): \tTrain Loss  2.3025\n",
            "Epoch(4/5)\t Batch(1600/3375): \tTrain Loss  2.3023\n",
            "Epoch(4/5)\t Batch(1800/3375): \tTrain Loss  2.3028\n",
            "Epoch(4/5)\t Batch(2000/3375): \tTrain Loss  2.3025\n",
            "Epoch(4/5)\t Batch(2200/3375): \tTrain Loss  2.3032\n",
            "Epoch(4/5)\t Batch(2400/3375): \tTrain Loss  2.3028\n",
            "Epoch(4/5)\t Batch(2600/3375): \tTrain Loss  2.3025\n",
            "Epoch(4/5)\t Batch(2800/3375): \tTrain Loss  2.3028\n",
            "Epoch(4/5)\t Batch(3000/3375): \tTrain Loss  2.3022\n",
            "Epoch(4/5)\t Batch(3200/3375): \tTrain Loss  2.3024\n",
            "End of Epoch: 4 Train Accuracy: 0.0999 Val Loss: 2.3026 Val Accuracy: 0.1010\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/3375): \tTrain Loss  2.3027\n",
            "Epoch(5/5)\t Batch(200/3375): \tTrain Loss  2.3022\n",
            "Epoch(5/5)\t Batch(400/3375): \tTrain Loss  2.3027\n",
            "Epoch(5/5)\t Batch(600/3375): \tTrain Loss  2.3025\n",
            "Epoch(5/5)\t Batch(800/3375): \tTrain Loss  2.3025\n",
            "Epoch(5/5)\t Batch(1000/3375): \tTrain Loss  2.3031\n",
            "Epoch(5/5)\t Batch(1200/3375): \tTrain Loss  2.3025\n",
            "Epoch(5/5)\t Batch(1400/3375): \tTrain Loss  2.3029\n",
            "Epoch(5/5)\t Batch(1600/3375): \tTrain Loss  2.3017\n",
            "Epoch(5/5)\t Batch(1800/3375): \tTrain Loss  2.3018\n",
            "Epoch(5/5)\t Batch(2000/3375): \tTrain Loss  2.3029\n",
            "Epoch(5/5)\t Batch(2200/3375): \tTrain Loss  2.3024\n",
            "Epoch(5/5)\t Batch(2400/3375): \tTrain Loss  2.3027\n",
            "Epoch(5/5)\t Batch(2600/3375): \tTrain Loss  2.3026\n",
            "Epoch(5/5)\t Batch(2800/3375): \tTrain Loss  2.3033\n",
            "Epoch(5/5)\t Batch(3000/3375): \tTrain Loss  2.3027\n",
            "Epoch(5/5)\t Batch(3200/3375): \tTrain Loss  2.3028\n",
            "End of Epoch: 5 Train Accuracy: 0.0999 Val Loss: 2.3026 Val Accuracy: 0.1005\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁██▇██</td></tr><tr><td>Train Loss</td><td>█▄▁▄▄</td></tr><tr><td>Val Accuracy</td><td>▁▄▆█▆▆</td></tr><tr><td>Val Loss</td><td>▁█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.09994</td></tr><tr><td>Train Loss</td><td>2.30258</td></tr><tr><td>Val Accuracy</td><td>0.1005</td></tr><tr><td>Val Loss</td><td>2.30259</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_sgd|loss_cross_entropy|lr=0.001|batch_16|act_tanh|hid_5|neurons_32|nrns_5|init_Xavier279</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/vyks19pd' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/vyks19pd</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_180559-vyks19pd/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1gmgry0h with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_180640-1gmgry0h</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/1gmgry0h' target=\"_blank\">unique-sweep-70</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/1gmgry0h' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/1gmgry0h</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.1070 Val Loss: 0.1664 Val Accuracy: 0.1078\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/3375): \tTrain Loss  0.1843\n",
            "Epoch(1/5)\t Batch(200/3375): \tTrain Loss  0.1688\n",
            "Epoch(1/5)\t Batch(400/3375): \tTrain Loss  0.1450\n",
            "Epoch(1/5)\t Batch(600/3375): \tTrain Loss  0.1557\n",
            "Epoch(1/5)\t Batch(800/3375): \tTrain Loss  0.1528\n",
            "Epoch(1/5)\t Batch(1000/3375): \tTrain Loss  0.1505\n",
            "Epoch(1/5)\t Batch(1200/3375): \tTrain Loss  0.1395\n",
            "Epoch(1/5)\t Batch(1400/3375): \tTrain Loss  0.1425\n",
            "Epoch(1/5)\t Batch(1600/3375): \tTrain Loss  0.1217\n",
            "Epoch(1/5)\t Batch(1800/3375): \tTrain Loss  0.1453\n",
            "Epoch(1/5)\t Batch(2000/3375): \tTrain Loss  0.1132\n",
            "Epoch(1/5)\t Batch(2200/3375): \tTrain Loss  0.1500\n",
            "Epoch(1/5)\t Batch(2400/3375): \tTrain Loss  0.1442\n",
            "Epoch(1/5)\t Batch(2600/3375): \tTrain Loss  0.1640\n",
            "Epoch(1/5)\t Batch(2800/3375): \tTrain Loss  0.1232\n",
            "Epoch(1/5)\t Batch(3000/3375): \tTrain Loss  0.1101\n",
            "Epoch(1/5)\t Batch(3200/3375): \tTrain Loss  0.1450\n",
            "End of Epoch: 1 Train Accuracy: 0.2285 Val Loss: 0.1426 Val Accuracy: 0.2283\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/3375): \tTrain Loss  0.1581\n",
            "Epoch(2/5)\t Batch(200/3375): \tTrain Loss  0.1575\n",
            "Epoch(2/5)\t Batch(400/3375): \tTrain Loss  0.1925\n",
            "Epoch(2/5)\t Batch(600/3375): \tTrain Loss  0.1428\n",
            "Epoch(2/5)\t Batch(800/3375): \tTrain Loss  0.1469\n",
            "Epoch(2/5)\t Batch(1000/3375): \tTrain Loss  0.1553\n",
            "Epoch(2/5)\t Batch(1200/3375): \tTrain Loss  0.1286\n",
            "Epoch(2/5)\t Batch(1400/3375): \tTrain Loss  0.1341\n",
            "Epoch(2/5)\t Batch(1600/3375): \tTrain Loss  0.1695\n",
            "Epoch(2/5)\t Batch(1800/3375): \tTrain Loss  0.1662\n",
            "Epoch(2/5)\t Batch(2000/3375): \tTrain Loss  0.1438\n",
            "Epoch(2/5)\t Batch(2200/3375): \tTrain Loss  0.0972\n",
            "Epoch(2/5)\t Batch(2400/3375): \tTrain Loss  0.0919\n",
            "Epoch(2/5)\t Batch(2600/3375): \tTrain Loss  0.1386\n",
            "Epoch(2/5)\t Batch(2800/3375): \tTrain Loss  0.1439\n",
            "Epoch(2/5)\t Batch(3000/3375): \tTrain Loss  0.1295\n",
            "Epoch(2/5)\t Batch(3200/3375): \tTrain Loss  0.1474\n",
            "End of Epoch: 2 Train Accuracy: 0.3258 Val Loss: 0.1243 Val Accuracy: 0.3235\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/3375): \tTrain Loss  0.1112\n",
            "Epoch(3/5)\t Batch(200/3375): \tTrain Loss  0.1315\n",
            "Epoch(3/5)\t Batch(400/3375): \tTrain Loss  0.0729\n",
            "Epoch(3/5)\t Batch(600/3375): \tTrain Loss  0.1307\n",
            "Epoch(3/5)\t Batch(800/3375): \tTrain Loss  0.1031\n",
            "Epoch(3/5)\t Batch(1000/3375): \tTrain Loss  0.1288\n",
            "Epoch(3/5)\t Batch(1200/3375): \tTrain Loss  0.1088\n",
            "Epoch(3/5)\t Batch(1400/3375): \tTrain Loss  0.1328\n",
            "Epoch(3/5)\t Batch(1600/3375): \tTrain Loss  0.1094\n",
            "Epoch(3/5)\t Batch(1800/3375): \tTrain Loss  0.0970\n",
            "Epoch(3/5)\t Batch(2000/3375): \tTrain Loss  0.1370\n",
            "Epoch(3/5)\t Batch(2200/3375): \tTrain Loss  0.1341\n",
            "Epoch(3/5)\t Batch(2400/3375): \tTrain Loss  0.0976\n",
            "Epoch(3/5)\t Batch(2600/3375): \tTrain Loss  0.0823\n",
            "Epoch(3/5)\t Batch(2800/3375): \tTrain Loss  0.1228\n",
            "Epoch(3/5)\t Batch(3000/3375): \tTrain Loss  0.0590\n",
            "Epoch(3/5)\t Batch(3200/3375): \tTrain Loss  0.1019\n",
            "End of Epoch: 3 Train Accuracy: 0.3885 Val Loss: 0.1136 Val Accuracy: 0.3863\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/3375): \tTrain Loss  0.1194\n",
            "Epoch(4/5)\t Batch(200/3375): \tTrain Loss  0.1228\n",
            "Epoch(4/5)\t Batch(400/3375): \tTrain Loss  0.1195\n",
            "Epoch(4/5)\t Batch(600/3375): \tTrain Loss  0.0734\n",
            "Epoch(4/5)\t Batch(800/3375): \tTrain Loss  0.1349\n",
            "Epoch(4/5)\t Batch(1000/3375): \tTrain Loss  0.1241\n",
            "Epoch(4/5)\t Batch(1200/3375): \tTrain Loss  0.1211\n",
            "Epoch(4/5)\t Batch(1400/3375): \tTrain Loss  0.1007\n",
            "Epoch(4/5)\t Batch(1600/3375): \tTrain Loss  0.1123\n",
            "Epoch(4/5)\t Batch(1800/3375): \tTrain Loss  0.1096\n",
            "Epoch(4/5)\t Batch(2000/3375): \tTrain Loss  0.1377\n",
            "Epoch(4/5)\t Batch(2200/3375): \tTrain Loss  0.1294\n",
            "Epoch(4/5)\t Batch(2400/3375): \tTrain Loss  0.0709\n",
            "Epoch(4/5)\t Batch(2600/3375): \tTrain Loss  0.1309\n",
            "Epoch(4/5)\t Batch(2800/3375): \tTrain Loss  0.0735\n",
            "Epoch(4/5)\t Batch(3000/3375): \tTrain Loss  0.0928\n",
            "Epoch(4/5)\t Batch(3200/3375): \tTrain Loss  0.1257\n",
            "End of Epoch: 4 Train Accuracy: 0.4338 Val Loss: 0.1059 Val Accuracy: 0.4270\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/3375): \tTrain Loss  0.0799\n",
            "Epoch(5/5)\t Batch(200/3375): \tTrain Loss  0.1179\n",
            "Epoch(5/5)\t Batch(400/3375): \tTrain Loss  0.1410\n",
            "Epoch(5/5)\t Batch(600/3375): \tTrain Loss  0.0364\n",
            "Epoch(5/5)\t Batch(800/3375): \tTrain Loss  0.0848\n",
            "Epoch(5/5)\t Batch(1000/3375): \tTrain Loss  0.1215\n",
            "Epoch(5/5)\t Batch(1200/3375): \tTrain Loss  0.1130\n",
            "Epoch(5/5)\t Batch(1400/3375): \tTrain Loss  0.1191\n",
            "Epoch(5/5)\t Batch(1600/3375): \tTrain Loss  0.1048\n",
            "Epoch(5/5)\t Batch(1800/3375): \tTrain Loss  0.0806\n",
            "Epoch(5/5)\t Batch(2000/3375): \tTrain Loss  0.0628\n",
            "Epoch(5/5)\t Batch(2200/3375): \tTrain Loss  0.0632\n",
            "Epoch(5/5)\t Batch(2400/3375): \tTrain Loss  0.0892\n",
            "Epoch(5/5)\t Batch(2600/3375): \tTrain Loss  0.0808\n",
            "Epoch(5/5)\t Batch(2800/3375): \tTrain Loss  0.0560\n",
            "Epoch(5/5)\t Batch(3000/3375): \tTrain Loss  0.0824\n",
            "Epoch(5/5)\t Batch(3200/3375): \tTrain Loss  0.0970\n",
            "End of Epoch: 5 Train Accuracy: 0.4809 Val Loss: 0.0972 Val Accuracy: 0.4755\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▃▅▆▇█</td></tr><tr><td>Train Loss</td><td>▅▆▁▅█</td></tr><tr><td>Val Accuracy</td><td>▁▃▅▆▇█</td></tr><tr><td>Val Loss</td><td>█▆▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.48085</td></tr><tr><td>Train Loss</td><td>0.13808</td></tr><tr><td>Val Accuracy</td><td>0.4755</td></tr><tr><td>Val Loss</td><td>0.09724</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_mse|lr=0.0001|batch_16|act_tanh|hid_3|neurons_128|nrns_5|init_Random59</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/1gmgry0h' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/1gmgry0h</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_180640-1gmgry0h/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p1nosfwo with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: momentum\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_180835-p1nosfwo</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/p1nosfwo' target=\"_blank\">comfy-sweep-71</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/p1nosfwo' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/p1nosfwo</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.1061 Val Loss: 10.7671 Val Accuracy: 0.1017\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/1687): \tTrain Loss  12.8338\n",
            "Epoch(1/10)\t Batch(200/1687): \tTrain Loss  2.3190\n",
            "Epoch(1/10)\t Batch(400/1687): \tTrain Loss  2.2927\n",
            "Epoch(1/10)\t Batch(600/1687): \tTrain Loss  2.3009\n",
            "Epoch(1/10)\t Batch(800/1687): \tTrain Loss  2.2954\n",
            "Epoch(1/10)\t Batch(1000/1687): \tTrain Loss  2.2992\n",
            "Epoch(1/10)\t Batch(1200/1687): \tTrain Loss  2.2805\n",
            "Epoch(1/10)\t Batch(1400/1687): \tTrain Loss  2.2772\n",
            "Epoch(1/10)\t Batch(1600/1687): \tTrain Loss  2.2732\n",
            "End of Epoch: 1 Train Accuracy: 0.1475 Val Loss: 2.2670 Val Accuracy: 0.1468\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/1687): \tTrain Loss  2.2610\n",
            "Epoch(2/10)\t Batch(200/1687): \tTrain Loss  2.2236\n",
            "Epoch(2/10)\t Batch(400/1687): \tTrain Loss  2.2486\n",
            "Epoch(2/10)\t Batch(600/1687): \tTrain Loss  2.2025\n",
            "Epoch(2/10)\t Batch(800/1687): \tTrain Loss  2.2290\n",
            "Epoch(2/10)\t Batch(1000/1687): \tTrain Loss  2.1863\n",
            "Epoch(2/10)\t Batch(1200/1687): \tTrain Loss  2.1692\n",
            "Epoch(2/10)\t Batch(1400/1687): \tTrain Loss  2.1838\n",
            "Epoch(2/10)\t Batch(1600/1687): \tTrain Loss  2.1478\n",
            "End of Epoch: 2 Train Accuracy: 0.4473 Val Loss: 2.1427 Val Accuracy: 0.4557\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/1687): \tTrain Loss  2.1219\n",
            "Epoch(3/10)\t Batch(200/1687): \tTrain Loss  2.0901\n",
            "Epoch(3/10)\t Batch(400/1687): \tTrain Loss  2.1235\n",
            "Epoch(3/10)\t Batch(600/1687): \tTrain Loss  2.1730\n",
            "Epoch(3/10)\t Batch(800/1687): \tTrain Loss  2.1456\n",
            "Epoch(3/10)\t Batch(1000/1687): \tTrain Loss  2.1460\n",
            "Epoch(3/10)\t Batch(1200/1687): \tTrain Loss  2.1034\n",
            "Epoch(3/10)\t Batch(1400/1687): \tTrain Loss  2.1110\n",
            "Epoch(3/10)\t Batch(1600/1687): \tTrain Loss  2.1667\n",
            "End of Epoch: 3 Train Accuracy: 0.4961 Val Loss: 2.1289 Val Accuracy: 0.4853\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/1687): \tTrain Loss  2.1113\n",
            "Epoch(4/10)\t Batch(200/1687): \tTrain Loss  2.1526\n",
            "Epoch(4/10)\t Batch(400/1687): \tTrain Loss  2.1453\n",
            "Epoch(4/10)\t Batch(600/1687): \tTrain Loss  2.1765\n",
            "Epoch(4/10)\t Batch(800/1687): \tTrain Loss  2.1240\n",
            "Epoch(4/10)\t Batch(1000/1687): \tTrain Loss  2.1305\n",
            "Epoch(4/10)\t Batch(1200/1687): \tTrain Loss  2.1476\n",
            "Epoch(4/10)\t Batch(1400/1687): \tTrain Loss  2.1756\n",
            "Epoch(4/10)\t Batch(1600/1687): \tTrain Loss  2.1508\n",
            "End of Epoch: 4 Train Accuracy: 0.4782 Val Loss: 2.1707 Val Accuracy: 0.4788\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/1687): \tTrain Loss  2.1711\n",
            "Epoch(5/10)\t Batch(200/1687): \tTrain Loss  2.1716\n",
            "Epoch(5/10)\t Batch(400/1687): \tTrain Loss  2.2003\n",
            "Epoch(5/10)\t Batch(600/1687): \tTrain Loss  2.1704\n",
            "Epoch(5/10)\t Batch(800/1687): \tTrain Loss  2.2079\n",
            "Epoch(5/10)\t Batch(1000/1687): \tTrain Loss  2.1937\n",
            "Epoch(5/10)\t Batch(1200/1687): \tTrain Loss  2.1897\n",
            "Epoch(5/10)\t Batch(1400/1687): \tTrain Loss  2.1881\n",
            "Epoch(5/10)\t Batch(1600/1687): \tTrain Loss  2.1929\n",
            "End of Epoch: 5 Train Accuracy: 0.4434 Val Loss: 2.1958 Val Accuracy: 0.4417\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/1687): \tTrain Loss  2.2076\n",
            "Epoch(6/10)\t Batch(200/1687): \tTrain Loss  2.1992\n",
            "Epoch(6/10)\t Batch(400/1687): \tTrain Loss  2.1797\n",
            "Epoch(6/10)\t Batch(600/1687): \tTrain Loss  2.2107\n",
            "Epoch(6/10)\t Batch(800/1687): \tTrain Loss  2.2134\n",
            "Epoch(6/10)\t Batch(1000/1687): \tTrain Loss  2.2127\n",
            "Epoch(6/10)\t Batch(1200/1687): \tTrain Loss  2.2038\n",
            "Epoch(6/10)\t Batch(1400/1687): \tTrain Loss  2.2064\n",
            "Epoch(6/10)\t Batch(1600/1687): \tTrain Loss  2.2145\n",
            "End of Epoch: 6 Train Accuracy: 0.4234 Val Loss: 2.2030 Val Accuracy: 0.4258\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/1687): \tTrain Loss  2.1980\n",
            "Epoch(7/10)\t Batch(200/1687): \tTrain Loss  2.1820\n",
            "Epoch(7/10)\t Batch(400/1687): \tTrain Loss  2.2038\n",
            "Epoch(7/10)\t Batch(600/1687): \tTrain Loss  2.2013\n",
            "Epoch(7/10)\t Batch(800/1687): \tTrain Loss  2.2050\n",
            "Epoch(7/10)\t Batch(1000/1687): \tTrain Loss  2.1875\n",
            "Epoch(7/10)\t Batch(1200/1687): \tTrain Loss  2.2038\n",
            "Epoch(7/10)\t Batch(1400/1687): \tTrain Loss  2.1839\n",
            "Epoch(7/10)\t Batch(1600/1687): \tTrain Loss  2.2062\n",
            "End of Epoch: 7 Train Accuracy: 0.4409 Val Loss: 2.2036 Val Accuracy: 0.4408\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/1687): \tTrain Loss  2.1971\n",
            "Epoch(8/10)\t Batch(200/1687): \tTrain Loss  2.2013\n",
            "Epoch(8/10)\t Batch(400/1687): \tTrain Loss  2.1988\n",
            "Epoch(8/10)\t Batch(600/1687): \tTrain Loss  2.2185\n",
            "Epoch(8/10)\t Batch(800/1687): \tTrain Loss  2.2108\n",
            "Epoch(8/10)\t Batch(1000/1687): \tTrain Loss  2.2135\n",
            "Epoch(8/10)\t Batch(1200/1687): \tTrain Loss  2.2097\n",
            "Epoch(8/10)\t Batch(1400/1687): \tTrain Loss  2.2180\n",
            "Epoch(8/10)\t Batch(1600/1687): \tTrain Loss  2.1987\n",
            "End of Epoch: 8 Train Accuracy: 0.4158 Val Loss: 2.2023 Val Accuracy: 0.4090\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/1687): \tTrain Loss  2.2049\n",
            "Epoch(9/10)\t Batch(200/1687): \tTrain Loss  2.2006\n",
            "Epoch(9/10)\t Batch(400/1687): \tTrain Loss  2.1985\n",
            "Epoch(9/10)\t Batch(600/1687): \tTrain Loss  2.2004\n",
            "Epoch(9/10)\t Batch(800/1687): \tTrain Loss  2.2106\n",
            "Epoch(9/10)\t Batch(1000/1687): \tTrain Loss  2.2071\n",
            "Epoch(9/10)\t Batch(1200/1687): \tTrain Loss  2.2080\n",
            "Epoch(9/10)\t Batch(1400/1687): \tTrain Loss  2.1987\n",
            "Epoch(9/10)\t Batch(1600/1687): \tTrain Loss  2.1929\n",
            "End of Epoch: 9 Train Accuracy: 0.4071 Val Loss: 2.2024 Val Accuracy: 0.4055\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/1687): \tTrain Loss  2.2016\n",
            "Epoch(10/10)\t Batch(200/1687): \tTrain Loss  2.1992\n",
            "Epoch(10/10)\t Batch(400/1687): \tTrain Loss  2.2283\n",
            "Epoch(10/10)\t Batch(600/1687): \tTrain Loss  2.1966\n",
            "Epoch(10/10)\t Batch(800/1687): \tTrain Loss  2.1931\n",
            "Epoch(10/10)\t Batch(1000/1687): \tTrain Loss  2.1992\n",
            "Epoch(10/10)\t Batch(1200/1687): \tTrain Loss  2.1985\n",
            "Epoch(10/10)\t Batch(1400/1687): \tTrain Loss  2.2190\n",
            "Epoch(10/10)\t Batch(1600/1687): \tTrain Loss  2.2146\n",
            "End of Epoch: 10 Train Accuracy: 0.3571 Val Loss: 2.2021 Val Accuracy: 0.3543\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▂▇██▇▇▇▇▆▆</td></tr><tr><td>Train Loss</td><td>█▁▂▅▄▅▆▅▅▅</td></tr><tr><td>Val Accuracy</td><td>▁▂▇██▇▇▇▇▇▆</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.35707</td></tr><tr><td>Train Loss</td><td>2.19762</td></tr><tr><td>Val Accuracy</td><td>0.35433</td></tr><tr><td>Val Loss</td><td>2.20208</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_momentum|loss_cross_entropy|lr=0.001|batch_32|act_tanh|hid_4|neurons_64|nrns_10|init_Random48</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/p1nosfwo' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/p1nosfwo</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_180835-p1nosfwo/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ava62fkc with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_180956-ava62fkc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ava62fkc' target=\"_blank\">smart-sweep-72</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ava62fkc' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ava62fkc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.0992 Val Loss: 0.0923 Val Accuracy: 0.1068\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/843): \tTrain Loss  0.0935\n",
            "Epoch(1/5)\t Batch(200/843): \tTrain Loss  0.0900\n",
            "Epoch(1/5)\t Batch(400/843): \tTrain Loss  0.0899\n",
            "Epoch(1/5)\t Batch(600/843): \tTrain Loss  0.0898\n",
            "Epoch(1/5)\t Batch(800/843): \tTrain Loss  0.0896\n",
            "End of Epoch: 1 Train Accuracy: 0.2178 Val Loss: 0.0895 Val Accuracy: 0.2158\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/843): \tTrain Loss  0.0894\n",
            "Epoch(2/5)\t Batch(200/843): \tTrain Loss  0.0890\n",
            "Epoch(2/5)\t Batch(400/843): \tTrain Loss  0.0876\n",
            "Epoch(2/5)\t Batch(600/843): \tTrain Loss  0.0864\n",
            "Epoch(2/5)\t Batch(800/843): \tTrain Loss  0.0845\n",
            "End of Epoch: 2 Train Accuracy: 0.2006 Val Loss: 0.0849 Val Accuracy: 0.1907\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/843): \tTrain Loss  0.0855\n",
            "Epoch(3/5)\t Batch(200/843): \tTrain Loss  0.0841\n",
            "Epoch(3/5)\t Batch(400/843): \tTrain Loss  0.0822\n",
            "Epoch(3/5)\t Batch(600/843): \tTrain Loss  0.0812\n",
            "Epoch(3/5)\t Batch(800/843): \tTrain Loss  0.0823\n",
            "End of Epoch: 3 Train Accuracy: 0.2171 Val Loss: 0.0817 Val Accuracy: 0.2093\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/843): \tTrain Loss  0.0820\n",
            "Epoch(4/5)\t Batch(200/843): \tTrain Loss  0.0812\n",
            "Epoch(4/5)\t Batch(400/843): \tTrain Loss  0.0805\n",
            "Epoch(4/5)\t Batch(600/843): \tTrain Loss  0.0807\n",
            "Epoch(4/5)\t Batch(800/843): \tTrain Loss  0.0800\n",
            "End of Epoch: 4 Train Accuracy: 0.3540 Val Loss: 0.0793 Val Accuracy: 0.3487\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/843): \tTrain Loss  0.0786\n",
            "Epoch(5/5)\t Batch(200/843): \tTrain Loss  0.0793\n",
            "Epoch(5/5)\t Batch(400/843): \tTrain Loss  0.0771\n",
            "Epoch(5/5)\t Batch(600/843): \tTrain Loss  0.0771\n",
            "Epoch(5/5)\t Batch(800/843): \tTrain Loss  0.0757\n",
            "End of Epoch: 5 Train Accuracy: 0.3819 Val Loss: 0.0763 Val Accuracy: 0.3763\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▄▄▄▇█</td></tr><tr><td>Train Loss</td><td>█▆▄▃▁</td></tr><tr><td>Val Accuracy</td><td>▁▄▃▄▇█</td></tr><tr><td>Val Loss</td><td>█▇▅▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.38185</td></tr><tr><td>Train Loss</td><td>0.07639</td></tr><tr><td>Val Accuracy</td><td>0.37633</td></tr><tr><td>Val Loss</td><td>0.07631</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_adam|loss_mse|lr=0.0001|batch_64|act_sigmoid|hid_4|neurons_128|nrns_5|init_Xavier695</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ava62fkc' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ava62fkc</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_180956-ava62fkc/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gcgf4va8 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_181106-gcgf4va8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/gcgf4va8' target=\"_blank\">chocolate-sweep-73</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/gcgf4va8' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/gcgf4va8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0957 Val Loss: 0.0900 Val Accuracy: 0.1043\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  0.0899\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  0.0486\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  0.0349\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  0.0253\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  0.0260\n",
            "End of Epoch: 1 Train Accuracy: 0.7931 Val Loss: 0.0300 Val Accuracy: 0.7908\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  0.0287\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  0.0230\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  0.0212\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  0.0279\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  0.0341\n",
            "End of Epoch: 2 Train Accuracy: 0.8371 Val Loss: 0.0250 Val Accuracy: 0.8287\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  0.0173\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  0.0171\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  0.0195\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  0.0213\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  0.0403\n",
            "End of Epoch: 3 Train Accuracy: 0.8475 Val Loss: 0.0231 Val Accuracy: 0.8442\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  0.0185\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  0.0254\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  0.0202\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  0.0238\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  0.0234\n",
            "End of Epoch: 4 Train Accuracy: 0.8508 Val Loss: 0.0234 Val Accuracy: 0.8402\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  0.0224\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  0.0211\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  0.0199\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  0.0223\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  0.0232\n",
            "End of Epoch: 5 Train Accuracy: 0.8515 Val Loss: 0.0224 Val Accuracy: 0.8497\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  0.0213\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  0.0193\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  0.0141\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  0.0150\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  0.0263\n",
            "End of Epoch: 6 Train Accuracy: 0.8639 Val Loss: 0.0218 Val Accuracy: 0.8482\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  0.0320\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  0.0250\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  0.0202\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  0.0101\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  0.0253\n",
            "End of Epoch: 7 Train Accuracy: 0.8693 Val Loss: 0.0209 Val Accuracy: 0.8605\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  0.0214\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  0.0243\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  0.0132\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  0.0140\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  0.0206\n",
            "End of Epoch: 8 Train Accuracy: 0.8730 Val Loss: 0.0204 Val Accuracy: 0.8633\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  0.0198\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  0.0254\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  0.0152\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  0.0341\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  0.0208\n",
            "End of Epoch: 9 Train Accuracy: 0.8738 Val Loss: 0.0202 Val Accuracy: 0.8662\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  0.0104\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  0.0179\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  0.0205\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  0.0227\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  0.0237\n",
            "End of Epoch: 10 Train Accuracy: 0.8720 Val Loss: 0.0202 Val Accuracy: 0.8647\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▇█████████</td></tr><tr><td>Train Loss</td><td>▇▅▂▆▁▆▃▁▅█</td></tr><tr><td>Val Accuracy</td><td>▁▇█████████</td></tr><tr><td>Val Loss</td><td>█▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.87196</td></tr><tr><td>Train Loss</td><td>0.03229</td></tr><tr><td>Val Accuracy</td><td>0.86467</td></tr><tr><td>Val Loss</td><td>0.0202</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nadam|loss_mse|lr=0.0001|batch_64|act_relu|hid_4|neurons_64|nrns_10|init_Xavier102</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/gcgf4va8' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/gcgf4va8</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_181106-gcgf4va8/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bjq68m41 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_181236-bjq68m41</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bjq68m41' target=\"_blank\">genial-sweep-74</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bjq68m41' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bjq68m41</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.1067 Val Loss: 24.6334 Val Accuracy: 0.1083\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/843): \tTrain Loss  25.9041\n",
            "Epoch(1/5)\t Batch(200/843): \tTrain Loss  25.0406\n",
            "Epoch(1/5)\t Batch(400/843): \tTrain Loss  25.0406\n",
            "Epoch(1/5)\t Batch(600/843): \tTrain Loss  25.0406\n",
            "Epoch(1/5)\t Batch(800/843): \tTrain Loss  26.3358\n",
            "End of Epoch: 1 Train Accuracy: 0.0712 Val Loss: 25.4922 Val Accuracy: 0.0762\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/843): \tTrain Loss  25.4723\n",
            "Epoch(2/5)\t Batch(200/843): \tTrain Loss  25.4723\n",
            "Epoch(2/5)\t Batch(400/843): \tTrain Loss  24.6809\n",
            "Epoch(2/5)\t Batch(600/843): \tTrain Loss  26.7676\n",
            "Epoch(2/5)\t Batch(800/843): \tTrain Loss  24.6089\n",
            "End of Epoch: 2 Train Accuracy: 0.0591 Val Loss: 25.9702 Val Accuracy: 0.0587\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/843): \tTrain Loss  25.9041\n",
            "Epoch(3/5)\t Batch(200/843): \tTrain Loss  26.3358\n",
            "Epoch(3/5)\t Batch(400/843): \tTrain Loss  27.1993\n",
            "Epoch(3/5)\t Batch(600/843): \tTrain Loss  27.3682\n",
            "Epoch(3/5)\t Batch(800/843): \tTrain Loss  25.9054\n",
            "End of Epoch: 3 Train Accuracy: 0.0613 Val Loss: 25.8818 Val Accuracy: 0.0613\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/843): \tTrain Loss  27.1993\n",
            "Epoch(4/5)\t Batch(200/843): \tTrain Loss  25.1274\n",
            "Epoch(4/5)\t Batch(400/843): \tTrain Loss  25.0952\n",
            "Epoch(4/5)\t Batch(600/843): \tTrain Loss  24.1771\n",
            "Epoch(4/5)\t Batch(800/843): \tTrain Loss  25.7638\n",
            "End of Epoch: 4 Train Accuracy: 0.0642 Val Loss: 25.7679 Val Accuracy: 0.0648\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/843): \tTrain Loss  25.9041\n",
            "Epoch(5/5)\t Batch(200/843): \tTrain Loss  26.7676\n",
            "Epoch(5/5)\t Batch(400/843): \tTrain Loss  25.9041\n",
            "Epoch(5/5)\t Batch(600/843): \tTrain Loss  27.1039\n",
            "Epoch(5/5)\t Batch(800/843): \tTrain Loss  25.4723\n",
            "End of Epoch: 5 Train Accuracy: 0.0656 Val Loss: 25.7037 Val Accuracy: 0.0675\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>█▃▁▁▂▂</td></tr><tr><td>Train Loss</td><td>▁▄▄█▆</td></tr><tr><td>Val Accuracy</td><td>█▃▁▁▂▂</td></tr><tr><td>Val Loss</td><td>▁▅██▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.06563</td></tr><tr><td>Train Loss</td><td>26.08551</td></tr><tr><td>Val Accuracy</td><td>0.0675</td></tr><tr><td>Val Loss</td><td>25.70371</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_adam|loss_cross_entropy|lr=0.0001|batch_64|act_relu|hid_5|neurons_32|nrns_5|init_Random679</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bjq68m41' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bjq68m41</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_181236-bjq68m41/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: swixxerr with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_181312-swixxerr</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/swixxerr' target=\"_blank\">brisk-sweep-75</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/swixxerr' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/swixxerr</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.1302 Val Loss: 0.1557 Val Accuracy: 0.1267\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/1687): \tTrain Loss  0.1565\n",
            "Epoch(1/10)\t Batch(200/1687): \tTrain Loss  0.0909\n",
            "Epoch(1/10)\t Batch(400/1687): \tTrain Loss  0.1114\n",
            "Epoch(1/10)\t Batch(600/1687): \tTrain Loss  0.1077\n",
            "Epoch(1/10)\t Batch(800/1687): \tTrain Loss  0.0968\n",
            "Epoch(1/10)\t Batch(1000/1687): \tTrain Loss  0.0866\n",
            "Epoch(1/10)\t Batch(1200/1687): \tTrain Loss  0.0711\n",
            "Epoch(1/10)\t Batch(1400/1687): \tTrain Loss  0.0643\n",
            "Epoch(1/10)\t Batch(1600/1687): \tTrain Loss  0.0491\n",
            "End of Epoch: 1 Train Accuracy: 0.5988 Val Loss: 0.0562 Val Accuracy: 0.5967\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/1687): \tTrain Loss  0.0424\n",
            "Epoch(2/10)\t Batch(200/1687): \tTrain Loss  0.0736\n",
            "Epoch(2/10)\t Batch(400/1687): \tTrain Loss  0.0391\n",
            "Epoch(2/10)\t Batch(600/1687): \tTrain Loss  0.0509\n",
            "Epoch(2/10)\t Batch(800/1687): \tTrain Loss  0.0473\n",
            "Epoch(2/10)\t Batch(1000/1687): \tTrain Loss  0.0649\n",
            "Epoch(2/10)\t Batch(1200/1687): \tTrain Loss  0.0525\n",
            "Epoch(2/10)\t Batch(1400/1687): \tTrain Loss  0.0459\n",
            "Epoch(2/10)\t Batch(1600/1687): \tTrain Loss  0.0619\n",
            "End of Epoch: 2 Train Accuracy: 0.6868 Val Loss: 0.0453 Val Accuracy: 0.6892\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/1687): \tTrain Loss  0.0432\n",
            "Epoch(3/10)\t Batch(200/1687): \tTrain Loss  0.0447\n",
            "Epoch(3/10)\t Batch(400/1687): \tTrain Loss  0.0423\n",
            "Epoch(3/10)\t Batch(600/1687): \tTrain Loss  0.0244\n",
            "Epoch(3/10)\t Batch(800/1687): \tTrain Loss  0.0519\n",
            "Epoch(3/10)\t Batch(1000/1687): \tTrain Loss  0.0411\n",
            "Epoch(3/10)\t Batch(1200/1687): \tTrain Loss  0.0302\n",
            "Epoch(3/10)\t Batch(1400/1687): \tTrain Loss  0.0311\n",
            "Epoch(3/10)\t Batch(1600/1687): \tTrain Loss  0.0286\n",
            "End of Epoch: 3 Train Accuracy: 0.7204 Val Loss: 0.0396 Val Accuracy: 0.7117\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/1687): \tTrain Loss  0.0400\n",
            "Epoch(4/10)\t Batch(200/1687): \tTrain Loss  0.0291\n",
            "Epoch(4/10)\t Batch(400/1687): \tTrain Loss  0.0262\n",
            "Epoch(4/10)\t Batch(600/1687): \tTrain Loss  0.0253\n",
            "Epoch(4/10)\t Batch(800/1687): \tTrain Loss  0.0544\n",
            "Epoch(4/10)\t Batch(1000/1687): \tTrain Loss  0.0326\n",
            "Epoch(4/10)\t Batch(1200/1687): \tTrain Loss  0.0445\n",
            "Epoch(4/10)\t Batch(1400/1687): \tTrain Loss  0.0548\n",
            "Epoch(4/10)\t Batch(1600/1687): \tTrain Loss  0.0267\n",
            "End of Epoch: 4 Train Accuracy: 0.7578 Val Loss: 0.0368 Val Accuracy: 0.7523\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/1687): \tTrain Loss  0.0382\n",
            "Epoch(5/10)\t Batch(200/1687): \tTrain Loss  0.0378\n",
            "Epoch(5/10)\t Batch(400/1687): \tTrain Loss  0.0401\n",
            "Epoch(5/10)\t Batch(600/1687): \tTrain Loss  0.0277\n",
            "Epoch(5/10)\t Batch(800/1687): \tTrain Loss  0.0172\n",
            "Epoch(5/10)\t Batch(1000/1687): \tTrain Loss  0.0455\n",
            "Epoch(5/10)\t Batch(1200/1687): \tTrain Loss  0.0307\n",
            "Epoch(5/10)\t Batch(1400/1687): \tTrain Loss  0.0268\n",
            "Epoch(5/10)\t Batch(1600/1687): \tTrain Loss  0.0263\n",
            "End of Epoch: 5 Train Accuracy: 0.7814 Val Loss: 0.0338 Val Accuracy: 0.7718\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/1687): \tTrain Loss  0.0363\n",
            "Epoch(6/10)\t Batch(200/1687): \tTrain Loss  0.0241\n",
            "Epoch(6/10)\t Batch(400/1687): \tTrain Loss  0.0293\n",
            "Epoch(6/10)\t Batch(600/1687): \tTrain Loss  0.0130\n",
            "Epoch(6/10)\t Batch(800/1687): \tTrain Loss  0.0306\n",
            "Epoch(6/10)\t Batch(1000/1687): \tTrain Loss  0.0421\n",
            "Epoch(6/10)\t Batch(1200/1687): \tTrain Loss  0.0404\n",
            "Epoch(6/10)\t Batch(1400/1687): \tTrain Loss  0.0251\n",
            "Epoch(6/10)\t Batch(1600/1687): \tTrain Loss  0.0309\n",
            "End of Epoch: 6 Train Accuracy: 0.7935 Val Loss: 0.0324 Val Accuracy: 0.7858\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/1687): \tTrain Loss  0.0293\n",
            "Epoch(7/10)\t Batch(200/1687): \tTrain Loss  0.0140\n",
            "Epoch(7/10)\t Batch(400/1687): \tTrain Loss  0.0323\n",
            "Epoch(7/10)\t Batch(600/1687): \tTrain Loss  0.0420\n",
            "Epoch(7/10)\t Batch(800/1687): \tTrain Loss  0.0295\n",
            "Epoch(7/10)\t Batch(1000/1687): \tTrain Loss  0.0127\n",
            "Epoch(7/10)\t Batch(1200/1687): \tTrain Loss  0.0544\n",
            "Epoch(7/10)\t Batch(1400/1687): \tTrain Loss  0.0288\n",
            "Epoch(7/10)\t Batch(1600/1687): \tTrain Loss  0.0364\n",
            "End of Epoch: 7 Train Accuracy: 0.7919 Val Loss: 0.0315 Val Accuracy: 0.7847\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/1687): \tTrain Loss  0.0523\n",
            "Epoch(8/10)\t Batch(200/1687): \tTrain Loss  0.0379\n",
            "Epoch(8/10)\t Batch(400/1687): \tTrain Loss  0.0392\n",
            "Epoch(8/10)\t Batch(600/1687): \tTrain Loss  0.0477\n",
            "Epoch(8/10)\t Batch(800/1687): \tTrain Loss  0.0277\n",
            "Epoch(8/10)\t Batch(1000/1687): \tTrain Loss  0.0275\n",
            "Epoch(8/10)\t Batch(1200/1687): \tTrain Loss  0.0319\n",
            "Epoch(8/10)\t Batch(1400/1687): \tTrain Loss  0.0278\n",
            "Epoch(8/10)\t Batch(1600/1687): \tTrain Loss  0.0195\n",
            "End of Epoch: 8 Train Accuracy: 0.8036 Val Loss: 0.0310 Val Accuracy: 0.7928\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/1687): \tTrain Loss  0.0385\n",
            "Epoch(9/10)\t Batch(200/1687): \tTrain Loss  0.0313\n",
            "Epoch(9/10)\t Batch(400/1687): \tTrain Loss  0.0152\n",
            "Epoch(9/10)\t Batch(600/1687): \tTrain Loss  0.0318\n",
            "Epoch(9/10)\t Batch(800/1687): \tTrain Loss  0.0248\n",
            "Epoch(9/10)\t Batch(1000/1687): \tTrain Loss  0.0310\n",
            "Epoch(9/10)\t Batch(1200/1687): \tTrain Loss  0.0260\n",
            "Epoch(9/10)\t Batch(1400/1687): \tTrain Loss  0.0328\n",
            "Epoch(9/10)\t Batch(1600/1687): \tTrain Loss  0.0389\n",
            "End of Epoch: 9 Train Accuracy: 0.8094 Val Loss: 0.0297 Val Accuracy: 0.8012\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/1687): \tTrain Loss  0.0179\n",
            "Epoch(10/10)\t Batch(200/1687): \tTrain Loss  0.0265\n",
            "Epoch(10/10)\t Batch(400/1687): \tTrain Loss  0.0208\n",
            "Epoch(10/10)\t Batch(600/1687): \tTrain Loss  0.0167\n",
            "Epoch(10/10)\t Batch(800/1687): \tTrain Loss  0.0391\n",
            "Epoch(10/10)\t Batch(1000/1687): \tTrain Loss  0.0301\n",
            "Epoch(10/10)\t Batch(1200/1687): \tTrain Loss  0.0463\n",
            "Epoch(10/10)\t Batch(1400/1687): \tTrain Loss  0.0197\n",
            "Epoch(10/10)\t Batch(1600/1687): \tTrain Loss  0.0198\n",
            "End of Epoch: 10 Train Accuracy: 0.8205 Val Loss: 0.0284 Val Accuracy: 0.8090\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▆▇▇▇██████</td></tr><tr><td>Train Loss</td><td>▆███▆▇▂▄▅▁</td></tr><tr><td>Val Accuracy</td><td>▁▆▇▇▇██████</td></tr><tr><td>Val Loss</td><td>█▃▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.8205</td></tr><tr><td>Train Loss</td><td>0.01542</td></tr><tr><td>Val Accuracy</td><td>0.809</td></tr><tr><td>Val Loss</td><td>0.0284</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nadam|loss_mse|lr=0.001|batch_32|act_tanh|hid_4|neurons_64|nrns_10|init_Random191</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/swixxerr' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/swixxerr</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_181312-swixxerr/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 36ut075e with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: momentum\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_181533-36ut075e</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/36ut075e' target=\"_blank\">iconic-sweep-76</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/36ut075e' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/36ut075e</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.1302 Val Loss: 12.2260 Val Accuracy: 0.1373\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/3375): \tTrain Loss  10.7692\n",
            "Epoch(1/10)\t Batch(200/3375): \tTrain Loss  5.7885\n",
            "Epoch(1/10)\t Batch(400/3375): \tTrain Loss  3.1182\n",
            "Epoch(1/10)\t Batch(600/3375): \tTrain Loss  2.8777\n",
            "Epoch(1/10)\t Batch(800/3375): \tTrain Loss  2.8749\n",
            "Epoch(1/10)\t Batch(1000/3375): \tTrain Loss  2.6304\n",
            "Epoch(1/10)\t Batch(1200/3375): \tTrain Loss  2.3313\n",
            "Epoch(1/10)\t Batch(1400/3375): \tTrain Loss  2.7405\n",
            "Epoch(1/10)\t Batch(1600/3375): \tTrain Loss  2.3934\n",
            "Epoch(1/10)\t Batch(1800/3375): \tTrain Loss  2.6332\n",
            "Epoch(1/10)\t Batch(2000/3375): \tTrain Loss  2.4714\n",
            "Epoch(1/10)\t Batch(2200/3375): \tTrain Loss  2.2677\n",
            "Epoch(1/10)\t Batch(2400/3375): \tTrain Loss  2.2225\n",
            "Epoch(1/10)\t Batch(2600/3375): \tTrain Loss  2.6146\n",
            "Epoch(1/10)\t Batch(2800/3375): \tTrain Loss  2.2903\n",
            "Epoch(1/10)\t Batch(3000/3375): \tTrain Loss  2.1536\n",
            "Epoch(1/10)\t Batch(3200/3375): \tTrain Loss  2.4656\n",
            "End of Epoch: 1 Train Accuracy: 0.1705 Val Loss: 2.3116 Val Accuracy: 0.1632\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/3375): \tTrain Loss  2.2317\n",
            "Epoch(2/10)\t Batch(200/3375): \tTrain Loss  2.2891\n",
            "Epoch(2/10)\t Batch(400/3375): \tTrain Loss  2.3725\n",
            "Epoch(2/10)\t Batch(600/3375): \tTrain Loss  2.2713\n",
            "Epoch(2/10)\t Batch(800/3375): \tTrain Loss  2.2085\n",
            "Epoch(2/10)\t Batch(1000/3375): \tTrain Loss  2.3017\n",
            "Epoch(2/10)\t Batch(1200/3375): \tTrain Loss  2.2621\n",
            "Epoch(2/10)\t Batch(1400/3375): \tTrain Loss  2.2255\n",
            "Epoch(2/10)\t Batch(1600/3375): \tTrain Loss  2.2608\n",
            "Epoch(2/10)\t Batch(1800/3375): \tTrain Loss  2.5953\n",
            "Epoch(2/10)\t Batch(2000/3375): \tTrain Loss  2.2505\n",
            "Epoch(2/10)\t Batch(2200/3375): \tTrain Loss  2.2463\n",
            "Epoch(2/10)\t Batch(2400/3375): \tTrain Loss  2.2238\n",
            "Epoch(2/10)\t Batch(2600/3375): \tTrain Loss  2.2127\n",
            "Epoch(2/10)\t Batch(2800/3375): \tTrain Loss  2.1941\n",
            "Epoch(2/10)\t Batch(3000/3375): \tTrain Loss  2.2214\n",
            "Epoch(2/10)\t Batch(3200/3375): \tTrain Loss  2.2550\n",
            "End of Epoch: 2 Train Accuracy: 0.2412 Val Loss: 2.2366 Val Accuracy: 0.2388\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/3375): \tTrain Loss  2.4130\n",
            "Epoch(3/10)\t Batch(200/3375): \tTrain Loss  2.2515\n",
            "Epoch(3/10)\t Batch(400/3375): \tTrain Loss  2.2113\n",
            "Epoch(3/10)\t Batch(600/3375): \tTrain Loss  2.4458\n",
            "Epoch(3/10)\t Batch(800/3375): \tTrain Loss  2.3088\n",
            "Epoch(3/10)\t Batch(1000/3375): \tTrain Loss  2.5559\n",
            "Epoch(3/10)\t Batch(1200/3375): \tTrain Loss  2.2082\n",
            "Epoch(3/10)\t Batch(1400/3375): \tTrain Loss  2.3036\n",
            "Epoch(3/10)\t Batch(1600/3375): \tTrain Loss  2.1963\n",
            "Epoch(3/10)\t Batch(1800/3375): \tTrain Loss  2.1506\n",
            "Epoch(3/10)\t Batch(2000/3375): \tTrain Loss  2.2209\n",
            "Epoch(3/10)\t Batch(2200/3375): \tTrain Loss  2.3163\n",
            "Epoch(3/10)\t Batch(2400/3375): \tTrain Loss  2.1990\n",
            "Epoch(3/10)\t Batch(2600/3375): \tTrain Loss  2.2211\n",
            "Epoch(3/10)\t Batch(2800/3375): \tTrain Loss  2.2119\n",
            "Epoch(3/10)\t Batch(3000/3375): \tTrain Loss  2.1834\n",
            "Epoch(3/10)\t Batch(3200/3375): \tTrain Loss  2.2155\n",
            "End of Epoch: 3 Train Accuracy: 0.2357 Val Loss: 2.2250 Val Accuracy: 0.2302\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/3375): \tTrain Loss  2.1628\n",
            "Epoch(4/10)\t Batch(200/3375): \tTrain Loss  2.2816\n",
            "Epoch(4/10)\t Batch(400/3375): \tTrain Loss  2.2681\n",
            "Epoch(4/10)\t Batch(600/3375): \tTrain Loss  2.1883\n",
            "Epoch(4/10)\t Batch(800/3375): \tTrain Loss  2.1810\n",
            "Epoch(4/10)\t Batch(1000/3375): \tTrain Loss  2.2370\n",
            "Epoch(4/10)\t Batch(1200/3375): \tTrain Loss  2.1223\n",
            "Epoch(4/10)\t Batch(1400/3375): \tTrain Loss  2.1669\n",
            "Epoch(4/10)\t Batch(1600/3375): \tTrain Loss  2.1837\n",
            "Epoch(4/10)\t Batch(1800/3375): \tTrain Loss  2.2016\n",
            "Epoch(4/10)\t Batch(2000/3375): \tTrain Loss  2.1778\n",
            "Epoch(4/10)\t Batch(2200/3375): \tTrain Loss  2.2306\n",
            "Epoch(4/10)\t Batch(2400/3375): \tTrain Loss  2.2037\n",
            "Epoch(4/10)\t Batch(2600/3375): \tTrain Loss  2.2019\n",
            "Epoch(4/10)\t Batch(2800/3375): \tTrain Loss  2.2061\n",
            "Epoch(4/10)\t Batch(3000/3375): \tTrain Loss  2.2335\n",
            "Epoch(4/10)\t Batch(3200/3375): \tTrain Loss  2.2387\n",
            "End of Epoch: 4 Train Accuracy: 0.2575 Val Loss: 2.2215 Val Accuracy: 0.2613\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/3375): \tTrain Loss  2.2055\n",
            "Epoch(5/10)\t Batch(200/3375): \tTrain Loss  2.2508\n",
            "Epoch(5/10)\t Batch(400/3375): \tTrain Loss  2.2216\n",
            "Epoch(5/10)\t Batch(600/3375): \tTrain Loss  2.2141\n",
            "Epoch(5/10)\t Batch(800/3375): \tTrain Loss  2.2189\n",
            "Epoch(5/10)\t Batch(1000/3375): \tTrain Loss  2.2152\n",
            "Epoch(5/10)\t Batch(1200/3375): \tTrain Loss  2.2057\n",
            "Epoch(5/10)\t Batch(1400/3375): \tTrain Loss  2.2414\n",
            "Epoch(5/10)\t Batch(1600/3375): \tTrain Loss  2.2122\n",
            "Epoch(5/10)\t Batch(1800/3375): \tTrain Loss  2.2112\n",
            "Epoch(5/10)\t Batch(2000/3375): \tTrain Loss  2.2225\n",
            "Epoch(5/10)\t Batch(2200/3375): \tTrain Loss  2.2255\n",
            "Epoch(5/10)\t Batch(2400/3375): \tTrain Loss  2.2162\n",
            "Epoch(5/10)\t Batch(2600/3375): \tTrain Loss  2.2098\n",
            "Epoch(5/10)\t Batch(2800/3375): \tTrain Loss  2.1947\n",
            "Epoch(5/10)\t Batch(3000/3375): \tTrain Loss  2.2353\n",
            "Epoch(5/10)\t Batch(3200/3375): \tTrain Loss  2.1726\n",
            "End of Epoch: 5 Train Accuracy: 0.2629 Val Loss: 2.2166 Val Accuracy: 0.2623\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/3375): \tTrain Loss  2.1912\n",
            "Epoch(6/10)\t Batch(200/3375): \tTrain Loss  2.1829\n",
            "Epoch(6/10)\t Batch(400/3375): \tTrain Loss  2.2064\n",
            "Epoch(6/10)\t Batch(600/3375): \tTrain Loss  2.2086\n",
            "Epoch(6/10)\t Batch(800/3375): \tTrain Loss  2.2064\n",
            "Epoch(6/10)\t Batch(1000/3375): \tTrain Loss  2.1955\n",
            "Epoch(6/10)\t Batch(1200/3375): \tTrain Loss  2.2208\n",
            "Epoch(6/10)\t Batch(1400/3375): \tTrain Loss  2.1783\n",
            "Epoch(6/10)\t Batch(1600/3375): \tTrain Loss  2.2357\n",
            "Epoch(6/10)\t Batch(1800/3375): \tTrain Loss  2.2333\n",
            "Epoch(6/10)\t Batch(2000/3375): \tTrain Loss  2.2601\n",
            "Epoch(6/10)\t Batch(2200/3375): \tTrain Loss  2.2146\n",
            "Epoch(6/10)\t Batch(2400/3375): \tTrain Loss  2.1764\n",
            "Epoch(6/10)\t Batch(2600/3375): \tTrain Loss  2.2012\n",
            "Epoch(6/10)\t Batch(2800/3375): \tTrain Loss  2.1926\n",
            "Epoch(6/10)\t Batch(3000/3375): \tTrain Loss  2.2242\n",
            "Epoch(6/10)\t Batch(3200/3375): \tTrain Loss  2.2081\n",
            "End of Epoch: 6 Train Accuracy: 0.2774 Val Loss: 2.2106 Val Accuracy: 0.2807\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/3375): \tTrain Loss  2.2231\n",
            "Epoch(7/10)\t Batch(200/3375): \tTrain Loss  2.2548\n",
            "Epoch(7/10)\t Batch(400/3375): \tTrain Loss  2.2394\n",
            "Epoch(7/10)\t Batch(600/3375): \tTrain Loss  2.2059\n",
            "Epoch(7/10)\t Batch(800/3375): \tTrain Loss  2.1716\n",
            "Epoch(7/10)\t Batch(1000/3375): \tTrain Loss  2.1663\n",
            "Epoch(7/10)\t Batch(1200/3375): \tTrain Loss  2.2352\n",
            "Epoch(7/10)\t Batch(1400/3375): \tTrain Loss  2.1365\n",
            "Epoch(7/10)\t Batch(1600/3375): \tTrain Loss  2.1743\n",
            "Epoch(7/10)\t Batch(1800/3375): \tTrain Loss  2.1645\n",
            "Epoch(7/10)\t Batch(2000/3375): \tTrain Loss  2.2009\n",
            "Epoch(7/10)\t Batch(2200/3375): \tTrain Loss  2.2206\n",
            "Epoch(7/10)\t Batch(2400/3375): \tTrain Loss  2.2000\n",
            "Epoch(7/10)\t Batch(2600/3375): \tTrain Loss  2.1794\n",
            "Epoch(7/10)\t Batch(2800/3375): \tTrain Loss  2.1988\n",
            "Epoch(7/10)\t Batch(3000/3375): \tTrain Loss  2.2063\n",
            "Epoch(7/10)\t Batch(3200/3375): \tTrain Loss  2.1860\n",
            "End of Epoch: 7 Train Accuracy: 0.2853 Val Loss: 2.2047 Val Accuracy: 0.2788\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/3375): \tTrain Loss  2.2007\n",
            "Epoch(8/10)\t Batch(200/3375): \tTrain Loss  2.2140\n",
            "Epoch(8/10)\t Batch(400/3375): \tTrain Loss  2.1729\n",
            "Epoch(8/10)\t Batch(600/3375): \tTrain Loss  2.2111\n",
            "Epoch(8/10)\t Batch(800/3375): \tTrain Loss  2.1768\n",
            "Epoch(8/10)\t Batch(1000/3375): \tTrain Loss  2.2210\n",
            "Epoch(8/10)\t Batch(1200/3375): \tTrain Loss  2.2335\n",
            "Epoch(8/10)\t Batch(1400/3375): \tTrain Loss  2.2092\n",
            "Epoch(8/10)\t Batch(1600/3375): \tTrain Loss  2.1812\n",
            "Epoch(8/10)\t Batch(1800/3375): \tTrain Loss  2.1206\n",
            "Epoch(8/10)\t Batch(2000/3375): \tTrain Loss  2.1699\n",
            "Epoch(8/10)\t Batch(2200/3375): \tTrain Loss  2.1820\n",
            "Epoch(8/10)\t Batch(2400/3375): \tTrain Loss  2.1818\n",
            "Epoch(8/10)\t Batch(2600/3375): \tTrain Loss  2.1857\n",
            "Epoch(8/10)\t Batch(2800/3375): \tTrain Loss  2.2209\n",
            "Epoch(8/10)\t Batch(3000/3375): \tTrain Loss  2.2732\n",
            "Epoch(8/10)\t Batch(3200/3375): \tTrain Loss  2.1696\n",
            "End of Epoch: 8 Train Accuracy: 0.2850 Val Loss: 2.1971 Val Accuracy: 0.2848\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/3375): \tTrain Loss  2.2039\n",
            "Epoch(9/10)\t Batch(200/3375): \tTrain Loss  2.2099\n",
            "Epoch(9/10)\t Batch(400/3375): \tTrain Loss  2.1999\n",
            "Epoch(9/10)\t Batch(600/3375): \tTrain Loss  2.1989\n",
            "Epoch(9/10)\t Batch(800/3375): \tTrain Loss  2.3604\n",
            "Epoch(9/10)\t Batch(1000/3375): \tTrain Loss  2.1772\n",
            "Epoch(9/10)\t Batch(1200/3375): \tTrain Loss  2.1594\n",
            "Epoch(9/10)\t Batch(1400/3375): \tTrain Loss  2.2200\n",
            "Epoch(9/10)\t Batch(1600/3375): \tTrain Loss  2.2076\n",
            "Epoch(9/10)\t Batch(1800/3375): \tTrain Loss  2.4560\n",
            "Epoch(9/10)\t Batch(2000/3375): \tTrain Loss  2.2070\n",
            "Epoch(9/10)\t Batch(2200/3375): \tTrain Loss  2.4376\n",
            "Epoch(9/10)\t Batch(2400/3375): \tTrain Loss  2.2025\n",
            "Epoch(9/10)\t Batch(2600/3375): \tTrain Loss  2.1602\n",
            "Epoch(9/10)\t Batch(2800/3375): \tTrain Loss  2.2456\n",
            "Epoch(9/10)\t Batch(3000/3375): \tTrain Loss  2.1931\n",
            "Epoch(9/10)\t Batch(3200/3375): \tTrain Loss  2.1741\n",
            "End of Epoch: 9 Train Accuracy: 0.3099 Val Loss: 2.1906 Val Accuracy: 0.3113\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/3375): \tTrain Loss  2.1774\n",
            "Epoch(10/10)\t Batch(200/3375): \tTrain Loss  2.1342\n",
            "Epoch(10/10)\t Batch(400/3375): \tTrain Loss  2.1797\n",
            "Epoch(10/10)\t Batch(600/3375): \tTrain Loss  2.1837\n",
            "Epoch(10/10)\t Batch(800/3375): \tTrain Loss  2.1619\n",
            "Epoch(10/10)\t Batch(1000/3375): \tTrain Loss  2.1940\n",
            "Epoch(10/10)\t Batch(1200/3375): \tTrain Loss  2.1713\n",
            "Epoch(10/10)\t Batch(1400/3375): \tTrain Loss  2.1718\n",
            "Epoch(10/10)\t Batch(1600/3375): \tTrain Loss  2.1861\n",
            "Epoch(10/10)\t Batch(1800/3375): \tTrain Loss  2.1775\n",
            "Epoch(10/10)\t Batch(2000/3375): \tTrain Loss  2.1814\n",
            "Epoch(10/10)\t Batch(2200/3375): \tTrain Loss  2.1741\n",
            "Epoch(10/10)\t Batch(2400/3375): \tTrain Loss  2.2098\n",
            "Epoch(10/10)\t Batch(2600/3375): \tTrain Loss  2.1773\n",
            "Epoch(10/10)\t Batch(2800/3375): \tTrain Loss  2.1910\n",
            "Epoch(10/10)\t Batch(3000/3375): \tTrain Loss  2.1501\n",
            "Epoch(10/10)\t Batch(3200/3375): \tTrain Loss  2.1243\n",
            "End of Epoch: 10 Train Accuracy: 0.3155 Val Loss: 2.1865 Val Accuracy: 0.3142\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▃▅▅▆▆▇▇▇██</td></tr><tr><td>Train Loss</td><td>█▆▄▃▂▃▂▁▄▁</td></tr><tr><td>Val Accuracy</td><td>▁▂▅▅▆▆▇▇▇██</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.31552</td></tr><tr><td>Train Loss</td><td>2.17155</td></tr><tr><td>Val Accuracy</td><td>0.31417</td></tr><tr><td>Val Loss</td><td>2.18651</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_momentum|loss_cross_entropy|lr=0.0001|batch_16|act_tanh|hid_3|neurons_64|nrns_10|init_Random163</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/36ut075e' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/36ut075e</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_181533-36ut075e/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tkvs4wxs with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_181704-tkvs4wxs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/tkvs4wxs' target=\"_blank\">rosy-sweep-77</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/tkvs4wxs' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/tkvs4wxs</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.1080 Val Loss: 0.1646 Val Accuracy: 0.1093\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/3375): \tTrain Loss  0.1532\n",
            "Epoch(1/10)\t Batch(200/3375): \tTrain Loss  0.1474\n",
            "Epoch(1/10)\t Batch(400/3375): \tTrain Loss  0.1746\n",
            "Epoch(1/10)\t Batch(600/3375): \tTrain Loss  0.1771\n",
            "Epoch(1/10)\t Batch(800/3375): \tTrain Loss  0.1801\n",
            "Epoch(1/10)\t Batch(1000/3375): \tTrain Loss  0.1654\n",
            "Epoch(1/10)\t Batch(1200/3375): \tTrain Loss  0.1303\n",
            "Epoch(1/10)\t Batch(1400/3375): \tTrain Loss  0.1908\n",
            "Epoch(1/10)\t Batch(1600/3375): \tTrain Loss  0.1717\n",
            "Epoch(1/10)\t Batch(1800/3375): \tTrain Loss  0.1296\n",
            "Epoch(1/10)\t Batch(2000/3375): \tTrain Loss  0.1390\n",
            "Epoch(1/10)\t Batch(2200/3375): \tTrain Loss  0.1533\n",
            "Epoch(1/10)\t Batch(2400/3375): \tTrain Loss  0.1681\n",
            "Epoch(1/10)\t Batch(2600/3375): \tTrain Loss  0.1668\n",
            "Epoch(1/10)\t Batch(2800/3375): \tTrain Loss  0.1356\n",
            "Epoch(1/10)\t Batch(3000/3375): \tTrain Loss  0.1592\n",
            "Epoch(1/10)\t Batch(3200/3375): \tTrain Loss  0.1426\n",
            "End of Epoch: 1 Train Accuracy: 0.1628 Val Loss: 0.1501 Val Accuracy: 0.1652\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/3375): \tTrain Loss  0.1340\n",
            "Epoch(2/10)\t Batch(200/3375): \tTrain Loss  0.1573\n",
            "Epoch(2/10)\t Batch(400/3375): \tTrain Loss  0.1509\n",
            "Epoch(2/10)\t Batch(600/3375): \tTrain Loss  0.1348\n",
            "Epoch(2/10)\t Batch(800/3375): \tTrain Loss  0.1310\n",
            "Epoch(2/10)\t Batch(1000/3375): \tTrain Loss  0.1669\n",
            "Epoch(2/10)\t Batch(1200/3375): \tTrain Loss  0.1467\n",
            "Epoch(2/10)\t Batch(1400/3375): \tTrain Loss  0.1460\n",
            "Epoch(2/10)\t Batch(1600/3375): \tTrain Loss  0.1236\n",
            "Epoch(2/10)\t Batch(1800/3375): \tTrain Loss  0.1110\n",
            "Epoch(2/10)\t Batch(2000/3375): \tTrain Loss  0.1540\n",
            "Epoch(2/10)\t Batch(2200/3375): \tTrain Loss  0.0984\n",
            "Epoch(2/10)\t Batch(2400/3375): \tTrain Loss  0.1112\n",
            "Epoch(2/10)\t Batch(2600/3375): \tTrain Loss  0.0912\n",
            "Epoch(2/10)\t Batch(2800/3375): \tTrain Loss  0.1137\n",
            "Epoch(2/10)\t Batch(3000/3375): \tTrain Loss  0.1398\n",
            "Epoch(2/10)\t Batch(3200/3375): \tTrain Loss  0.0892\n",
            "End of Epoch: 2 Train Accuracy: 0.2911 Val Loss: 0.1240 Val Accuracy: 0.2933\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/3375): \tTrain Loss  0.1483\n",
            "Epoch(3/10)\t Batch(200/3375): \tTrain Loss  0.0944\n",
            "Epoch(3/10)\t Batch(400/3375): \tTrain Loss  0.1465\n",
            "Epoch(3/10)\t Batch(600/3375): \tTrain Loss  0.0947\n",
            "Epoch(3/10)\t Batch(800/3375): \tTrain Loss  0.1045\n",
            "Epoch(3/10)\t Batch(1000/3375): \tTrain Loss  0.1206\n",
            "Epoch(3/10)\t Batch(1200/3375): \tTrain Loss  0.1196\n",
            "Epoch(3/10)\t Batch(1400/3375): \tTrain Loss  0.0750\n",
            "Epoch(3/10)\t Batch(1600/3375): \tTrain Loss  0.0988\n",
            "Epoch(3/10)\t Batch(1800/3375): \tTrain Loss  0.0970\n",
            "Epoch(3/10)\t Batch(2000/3375): \tTrain Loss  0.1270\n",
            "Epoch(3/10)\t Batch(2200/3375): \tTrain Loss  0.1043\n",
            "Epoch(3/10)\t Batch(2400/3375): \tTrain Loss  0.0890\n",
            "Epoch(3/10)\t Batch(2600/3375): \tTrain Loss  0.1160\n",
            "Epoch(3/10)\t Batch(2800/3375): \tTrain Loss  0.0788\n",
            "Epoch(3/10)\t Batch(3000/3375): \tTrain Loss  0.0697\n",
            "Epoch(3/10)\t Batch(3200/3375): \tTrain Loss  0.0984\n",
            "End of Epoch: 3 Train Accuracy: 0.4474 Val Loss: 0.0967 Val Accuracy: 0.4380\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/3375): \tTrain Loss  0.1072\n",
            "Epoch(4/10)\t Batch(200/3375): \tTrain Loss  0.0958\n",
            "Epoch(4/10)\t Batch(400/3375): \tTrain Loss  0.1005\n",
            "Epoch(4/10)\t Batch(600/3375): \tTrain Loss  0.0666\n",
            "Epoch(4/10)\t Batch(800/3375): \tTrain Loss  0.0811\n",
            "Epoch(4/10)\t Batch(1000/3375): \tTrain Loss  0.0828\n",
            "Epoch(4/10)\t Batch(1200/3375): \tTrain Loss  0.1150\n",
            "Epoch(4/10)\t Batch(1400/3375): \tTrain Loss  0.1156\n",
            "Epoch(4/10)\t Batch(1600/3375): \tTrain Loss  0.0527\n",
            "Epoch(4/10)\t Batch(1800/3375): \tTrain Loss  0.0342\n",
            "Epoch(4/10)\t Batch(2000/3375): \tTrain Loss  0.0602\n",
            "Epoch(4/10)\t Batch(2200/3375): \tTrain Loss  0.1015\n",
            "Epoch(4/10)\t Batch(2400/3375): \tTrain Loss  0.0942\n",
            "Epoch(4/10)\t Batch(2600/3375): \tTrain Loss  0.0907\n",
            "Epoch(4/10)\t Batch(2800/3375): \tTrain Loss  0.1060\n",
            "Epoch(4/10)\t Batch(3000/3375): \tTrain Loss  0.0676\n",
            "Epoch(4/10)\t Batch(3200/3375): \tTrain Loss  0.0831\n",
            "End of Epoch: 4 Train Accuracy: 0.5696 Val Loss: 0.0743 Val Accuracy: 0.5640\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/3375): \tTrain Loss  0.0775\n",
            "Epoch(5/10)\t Batch(200/3375): \tTrain Loss  0.0745\n",
            "Epoch(5/10)\t Batch(400/3375): \tTrain Loss  0.0798\n",
            "Epoch(5/10)\t Batch(600/3375): \tTrain Loss  0.0688\n",
            "Epoch(5/10)\t Batch(800/3375): \tTrain Loss  0.0544\n",
            "Epoch(5/10)\t Batch(1000/3375): \tTrain Loss  0.0720\n",
            "Epoch(5/10)\t Batch(1200/3375): \tTrain Loss  0.0501\n",
            "Epoch(5/10)\t Batch(1400/3375): \tTrain Loss  0.0865\n",
            "Epoch(5/10)\t Batch(1600/3375): \tTrain Loss  0.0322\n",
            "Epoch(5/10)\t Batch(1800/3375): \tTrain Loss  0.0357\n",
            "Epoch(5/10)\t Batch(2000/3375): \tTrain Loss  0.0553\n",
            "Epoch(5/10)\t Batch(2200/3375): \tTrain Loss  0.0711\n",
            "Epoch(5/10)\t Batch(2400/3375): \tTrain Loss  0.0735\n",
            "Epoch(5/10)\t Batch(2600/3375): \tTrain Loss  0.1032\n",
            "Epoch(5/10)\t Batch(2800/3375): \tTrain Loss  0.0518\n",
            "Epoch(5/10)\t Batch(3000/3375): \tTrain Loss  0.0476\n",
            "Epoch(5/10)\t Batch(3200/3375): \tTrain Loss  0.0521\n",
            "End of Epoch: 5 Train Accuracy: 0.6562 Val Loss: 0.0600 Val Accuracy: 0.6437\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/3375): \tTrain Loss  0.0688\n",
            "Epoch(6/10)\t Batch(200/3375): \tTrain Loss  0.0550\n",
            "Epoch(6/10)\t Batch(400/3375): \tTrain Loss  0.0229\n",
            "Epoch(6/10)\t Batch(600/3375): \tTrain Loss  0.0884\n",
            "Epoch(6/10)\t Batch(800/3375): \tTrain Loss  0.0325\n",
            "Epoch(6/10)\t Batch(1000/3375): \tTrain Loss  0.0870\n",
            "Epoch(6/10)\t Batch(1200/3375): \tTrain Loss  0.0881\n",
            "Epoch(6/10)\t Batch(1400/3375): \tTrain Loss  0.0544\n",
            "Epoch(6/10)\t Batch(1600/3375): \tTrain Loss  0.0614\n",
            "Epoch(6/10)\t Batch(1800/3375): \tTrain Loss  0.0475\n",
            "Epoch(6/10)\t Batch(2000/3375): \tTrain Loss  0.0541\n",
            "Epoch(6/10)\t Batch(2200/3375): \tTrain Loss  0.0615\n",
            "Epoch(6/10)\t Batch(2400/3375): \tTrain Loss  0.0214\n",
            "Epoch(6/10)\t Batch(2600/3375): \tTrain Loss  0.0473\n",
            "Epoch(6/10)\t Batch(2800/3375): \tTrain Loss  0.0498\n",
            "Epoch(6/10)\t Batch(3000/3375): \tTrain Loss  0.0980\n",
            "Epoch(6/10)\t Batch(3200/3375): \tTrain Loss  0.0346\n",
            "End of Epoch: 6 Train Accuracy: 0.7098 Val Loss: 0.0477 Val Accuracy: 0.6927\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/3375): \tTrain Loss  0.0685\n",
            "Epoch(7/10)\t Batch(200/3375): \tTrain Loss  0.0434\n",
            "Epoch(7/10)\t Batch(400/3375): \tTrain Loss  0.0439\n",
            "Epoch(7/10)\t Batch(600/3375): \tTrain Loss  0.0241\n",
            "Epoch(7/10)\t Batch(800/3375): \tTrain Loss  0.0097\n",
            "Epoch(7/10)\t Batch(1000/3375): \tTrain Loss  0.0560\n",
            "Epoch(7/10)\t Batch(1200/3375): \tTrain Loss  0.0339\n",
            "Epoch(7/10)\t Batch(1400/3375): \tTrain Loss  0.0314\n",
            "Epoch(7/10)\t Batch(1600/3375): \tTrain Loss  0.0048\n",
            "Epoch(7/10)\t Batch(1800/3375): \tTrain Loss  0.0137\n",
            "Epoch(7/10)\t Batch(2000/3375): \tTrain Loss  0.0443\n",
            "Epoch(7/10)\t Batch(2200/3375): \tTrain Loss  0.0381\n",
            "Epoch(7/10)\t Batch(2400/3375): \tTrain Loss  0.0448\n",
            "Epoch(7/10)\t Batch(2600/3375): \tTrain Loss  0.0080\n",
            "Epoch(7/10)\t Batch(2800/3375): \tTrain Loss  0.0160\n",
            "Epoch(7/10)\t Batch(3000/3375): \tTrain Loss  0.0487\n",
            "Epoch(7/10)\t Batch(3200/3375): \tTrain Loss  0.0294\n",
            "End of Epoch: 7 Train Accuracy: 0.7704 Val Loss: 0.0367 Val Accuracy: 0.7520\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/3375): \tTrain Loss  0.0363\n",
            "Epoch(8/10)\t Batch(200/3375): \tTrain Loss  0.0477\n",
            "Epoch(8/10)\t Batch(400/3375): \tTrain Loss  0.0237\n",
            "Epoch(8/10)\t Batch(600/3375): \tTrain Loss  0.0398\n",
            "Epoch(8/10)\t Batch(800/3375): \tTrain Loss  0.0325\n",
            "Epoch(8/10)\t Batch(1000/3375): \tTrain Loss  0.0239\n",
            "Epoch(8/10)\t Batch(1200/3375): \tTrain Loss  0.0168\n",
            "Epoch(8/10)\t Batch(1400/3375): \tTrain Loss  0.0606\n",
            "Epoch(8/10)\t Batch(1600/3375): \tTrain Loss  0.0155\n",
            "Epoch(8/10)\t Batch(1800/3375): \tTrain Loss  0.0255\n",
            "Epoch(8/10)\t Batch(2000/3375): \tTrain Loss  0.0152\n",
            "Epoch(8/10)\t Batch(2200/3375): \tTrain Loss  0.0375\n",
            "Epoch(8/10)\t Batch(2400/3375): \tTrain Loss  0.0264\n",
            "Epoch(8/10)\t Batch(2600/3375): \tTrain Loss  0.0356\n",
            "Epoch(8/10)\t Batch(2800/3375): \tTrain Loss  0.0046\n",
            "Epoch(8/10)\t Batch(3000/3375): \tTrain Loss  0.0154\n",
            "Epoch(8/10)\t Batch(3200/3375): \tTrain Loss  0.0268\n",
            "End of Epoch: 8 Train Accuracy: 0.8109 Val Loss: 0.0302 Val Accuracy: 0.7893\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/3375): \tTrain Loss  0.0401\n",
            "Epoch(9/10)\t Batch(200/3375): \tTrain Loss  0.0605\n",
            "Epoch(9/10)\t Batch(400/3375): \tTrain Loss  0.0280\n",
            "Epoch(9/10)\t Batch(600/3375): \tTrain Loss  0.0252\n",
            "Epoch(9/10)\t Batch(800/3375): \tTrain Loss  0.0122\n",
            "Epoch(9/10)\t Batch(1000/3375): \tTrain Loss  0.0196\n",
            "Epoch(9/10)\t Batch(1200/3375): \tTrain Loss  0.0071\n",
            "Epoch(9/10)\t Batch(1400/3375): \tTrain Loss  0.0462\n",
            "Epoch(9/10)\t Batch(1600/3375): \tTrain Loss  0.0126\n",
            "Epoch(9/10)\t Batch(1800/3375): \tTrain Loss  0.0200\n",
            "Epoch(9/10)\t Batch(2000/3375): \tTrain Loss  0.0228\n",
            "Epoch(9/10)\t Batch(2200/3375): \tTrain Loss  0.0262\n",
            "Epoch(9/10)\t Batch(2400/3375): \tTrain Loss  0.0232\n",
            "Epoch(9/10)\t Batch(2600/3375): \tTrain Loss  0.0223\n",
            "Epoch(9/10)\t Batch(2800/3375): \tTrain Loss  0.0329\n",
            "Epoch(9/10)\t Batch(3000/3375): \tTrain Loss  0.0231\n",
            "Epoch(9/10)\t Batch(3200/3375): \tTrain Loss  0.0494\n",
            "End of Epoch: 9 Train Accuracy: 0.8441 Val Loss: 0.0254 Val Accuracy: 0.8217\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/3375): \tTrain Loss  0.0268\n",
            "Epoch(10/10)\t Batch(200/3375): \tTrain Loss  0.0326\n",
            "Epoch(10/10)\t Batch(400/3375): \tTrain Loss  0.0272\n",
            "Epoch(10/10)\t Batch(600/3375): \tTrain Loss  0.0283\n",
            "Epoch(10/10)\t Batch(800/3375): \tTrain Loss  0.0229\n",
            "Epoch(10/10)\t Batch(1000/3375): \tTrain Loss  0.0268\n",
            "Epoch(10/10)\t Batch(1200/3375): \tTrain Loss  0.0240\n",
            "Epoch(10/10)\t Batch(1400/3375): \tTrain Loss  0.0175\n",
            "Epoch(10/10)\t Batch(1600/3375): \tTrain Loss  0.0204\n",
            "Epoch(10/10)\t Batch(1800/3375): \tTrain Loss  0.0089\n",
            "Epoch(10/10)\t Batch(2000/3375): \tTrain Loss  0.0118\n",
            "Epoch(10/10)\t Batch(2200/3375): \tTrain Loss  0.0205\n",
            "Epoch(10/10)\t Batch(2400/3375): \tTrain Loss  0.0321\n",
            "Epoch(10/10)\t Batch(2600/3375): \tTrain Loss  0.0324\n",
            "Epoch(10/10)\t Batch(2800/3375): \tTrain Loss  0.0260\n",
            "Epoch(10/10)\t Batch(3000/3375): \tTrain Loss  0.0182\n",
            "Epoch(10/10)\t Batch(3200/3375): \tTrain Loss  0.0046\n",
            "End of Epoch: 10 Train Accuracy: 0.8600 Val Loss: 0.0228 Val Accuracy: 0.8420\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▄▅▆▇▇███</td></tr><tr><td>Train Loss</td><td>█▆▄▅▂▂▂▂▁▁</td></tr><tr><td>Val Accuracy</td><td>▁▂▃▄▅▆▇▇▇██</td></tr><tr><td>Val Loss</td><td>█▇▆▅▄▃▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.86004</td></tr><tr><td>Train Loss</td><td>0.00783</td></tr><tr><td>Val Accuracy</td><td>0.842</td></tr><tr><td>Val Loss</td><td>0.02276</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_mse|lr=0.0001|batch_16|act_tanh|hid_5|neurons_128|nrns_10|init_Random199</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/tkvs4wxs' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/tkvs4wxs</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_181704-tkvs4wxs/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2trvw54h with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_182151-2trvw54h</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/2trvw54h' target=\"_blank\">northern-sweep-78</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/2trvw54h' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/2trvw54h</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.1000 Val Loss: 24.9278 Val Accuracy: 0.0978\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/1687): \tTrain Loss  24.1771\n",
            "Epoch(1/5)\t Batch(200/1687): \tTrain Loss  25.9041\n",
            "Epoch(1/5)\t Batch(400/1687): \tTrain Loss  24.1771\n",
            "Epoch(1/5)\t Batch(600/1687): \tTrain Loss  25.0406\n",
            "Epoch(1/5)\t Batch(800/1687): \tTrain Loss  25.0406\n",
            "Epoch(1/5)\t Batch(1000/1687): \tTrain Loss  24.1771\n",
            "Epoch(1/5)\t Batch(1200/1687): \tTrain Loss  25.0406\n",
            "Epoch(1/5)\t Batch(1400/1687): \tTrain Loss  25.0406\n",
            "Epoch(1/5)\t Batch(1600/1687): \tTrain Loss  26.7676\n",
            "End of Epoch: 1 Train Accuracy: 0.0985 Val Loss: 25.0324 Val Accuracy: 0.0937\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/1687): \tTrain Loss  23.3137\n",
            "Epoch(2/5)\t Batch(200/1687): \tTrain Loss  24.1771\n",
            "Epoch(2/5)\t Batch(400/1687): \tTrain Loss  26.7676\n",
            "Epoch(2/5)\t Batch(600/1687): \tTrain Loss  25.9041\n",
            "Epoch(2/5)\t Batch(800/1687): \tTrain Loss  27.6310\n",
            "Epoch(2/5)\t Batch(1000/1687): \tTrain Loss  24.1771\n",
            "Epoch(2/5)\t Batch(1200/1687): \tTrain Loss  25.0406\n",
            "Epoch(2/5)\t Batch(1400/1687): \tTrain Loss  25.0406\n",
            "Epoch(2/5)\t Batch(1600/1687): \tTrain Loss  25.9041\n",
            "End of Epoch: 2 Train Accuracy: 0.0897 Val Loss: 25.1611 Val Accuracy: 0.0893\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/1687): \tTrain Loss  25.9041\n",
            "Epoch(3/5)\t Batch(200/1687): \tTrain Loss  26.7676\n",
            "Epoch(3/5)\t Batch(400/1687): \tTrain Loss  25.9041\n",
            "Epoch(3/5)\t Batch(600/1687): \tTrain Loss  25.9041\n",
            "Epoch(3/5)\t Batch(800/1687): \tTrain Loss  25.9041\n",
            "Epoch(3/5)\t Batch(1000/1687): \tTrain Loss  25.9041\n",
            "Epoch(3/5)\t Batch(1200/1687): \tTrain Loss  25.9041\n",
            "Epoch(3/5)\t Batch(1400/1687): \tTrain Loss  25.0406\n",
            "Epoch(3/5)\t Batch(1600/1687): \tTrain Loss  27.6310\n",
            "End of Epoch: 3 Train Accuracy: 0.0916 Val Loss: 25.0682 Val Accuracy: 0.0927\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/1687): \tTrain Loss  25.0406\n",
            "Epoch(4/5)\t Batch(200/1687): \tTrain Loss  26.7676\n",
            "Epoch(4/5)\t Batch(400/1687): \tTrain Loss  25.9041\n",
            "Epoch(4/5)\t Batch(600/1687): \tTrain Loss  25.9041\n",
            "Epoch(4/5)\t Batch(800/1687): \tTrain Loss  25.9041\n",
            "Epoch(4/5)\t Batch(1000/1687): \tTrain Loss  25.0406\n",
            "Epoch(4/5)\t Batch(1200/1687): \tTrain Loss  25.9041\n",
            "Epoch(4/5)\t Batch(1400/1687): \tTrain Loss  20.7233\n",
            "Epoch(4/5)\t Batch(1600/1687): \tTrain Loss  25.0406\n",
            "End of Epoch: 4 Train Accuracy: 0.0931 Val Loss: 24.9234 Val Accuracy: 0.0975\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/1687): \tTrain Loss  23.3137\n",
            "Epoch(5/5)\t Batch(200/1687): \tTrain Loss  25.0406\n",
            "Epoch(5/5)\t Batch(400/1687): \tTrain Loss  26.7676\n",
            "Epoch(5/5)\t Batch(600/1687): \tTrain Loss  24.1771\n",
            "Epoch(5/5)\t Batch(800/1687): \tTrain Loss  26.7676\n",
            "Epoch(5/5)\t Batch(1000/1687): \tTrain Loss  25.9041\n",
            "Epoch(5/5)\t Batch(1200/1687): \tTrain Loss  25.0406\n",
            "Epoch(5/5)\t Batch(1400/1687): \tTrain Loss  27.6310\n",
            "Epoch(5/5)\t Batch(1600/1687): \tTrain Loss  24.1771\n",
            "End of Epoch: 5 Train Accuracy: 0.0905 Val Loss: 25.0556 Val Accuracy: 0.0930\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>█▇▁▂▃▂</td></tr><tr><td>Train Loss</td><td>▁█▅▅█</td></tr><tr><td>Val Accuracy</td><td>█▅▁▄█▄</td></tr><tr><td>Val Loss</td><td>▁▄█▅▁▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.0905</td></tr><tr><td>Train Loss</td><td>25.90408</td></tr><tr><td>Val Accuracy</td><td>0.093</td></tr><tr><td>Val Loss</td><td>25.05555</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_adam|loss_cross_entropy|lr=0.0001|batch_32|act_relu|hid_5|neurons_64|nrns_5|init_Random941</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/2trvw54h' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/2trvw54h</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_182151-2trvw54h/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 38r7nxu2 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nag\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_182237-38r7nxu2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/38r7nxu2' target=\"_blank\">dauntless-sweep-79</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/38r7nxu2' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/38r7nxu2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.1048 Val Loss: 0.0901 Val Accuracy: 0.1045\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/3375): \tTrain Loss  0.0894\n",
            "Epoch(1/10)\t Batch(200/3375): \tTrain Loss  0.0893\n",
            "Epoch(1/10)\t Batch(400/3375): \tTrain Loss  0.0890\n",
            "Epoch(1/10)\t Batch(600/3375): \tTrain Loss  0.0895\n",
            "Epoch(1/10)\t Batch(800/3375): \tTrain Loss  0.0893\n",
            "Epoch(1/10)\t Batch(1000/3375): \tTrain Loss  0.0887\n",
            "Epoch(1/10)\t Batch(1200/3375): \tTrain Loss  0.0868\n",
            "Epoch(1/10)\t Batch(1400/3375): \tTrain Loss  0.0877\n",
            "Epoch(1/10)\t Batch(1600/3375): \tTrain Loss  0.0879\n",
            "Epoch(1/10)\t Batch(1800/3375): \tTrain Loss  0.0882\n",
            "Epoch(1/10)\t Batch(2000/3375): \tTrain Loss  0.0866\n",
            "Epoch(1/10)\t Batch(2200/3375): \tTrain Loss  0.0854\n",
            "Epoch(1/10)\t Batch(2400/3375): \tTrain Loss  0.0873\n",
            "Epoch(1/10)\t Batch(2600/3375): \tTrain Loss  0.0856\n",
            "Epoch(1/10)\t Batch(2800/3375): \tTrain Loss  0.0841\n",
            "Epoch(1/10)\t Batch(3000/3375): \tTrain Loss  0.0850\n",
            "Epoch(1/10)\t Batch(3200/3375): \tTrain Loss  0.0824\n",
            "End of Epoch: 1 Train Accuracy: 0.5521 Val Loss: 0.0833 Val Accuracy: 0.5530\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/3375): \tTrain Loss  0.0848\n",
            "Epoch(2/10)\t Batch(200/3375): \tTrain Loss  0.0811\n",
            "Epoch(2/10)\t Batch(400/3375): \tTrain Loss  0.0780\n",
            "Epoch(2/10)\t Batch(600/3375): \tTrain Loss  0.0801\n",
            "Epoch(2/10)\t Batch(800/3375): \tTrain Loss  0.0783\n",
            "Epoch(2/10)\t Batch(1000/3375): \tTrain Loss  0.0768\n",
            "Epoch(2/10)\t Batch(1200/3375): \tTrain Loss  0.0776\n",
            "Epoch(2/10)\t Batch(1400/3375): \tTrain Loss  0.0798\n",
            "Epoch(2/10)\t Batch(1600/3375): \tTrain Loss  0.0744\n",
            "Epoch(2/10)\t Batch(1800/3375): \tTrain Loss  0.0849\n",
            "Epoch(2/10)\t Batch(2000/3375): \tTrain Loss  0.0781\n",
            "Epoch(2/10)\t Batch(2200/3375): \tTrain Loss  0.0681\n",
            "Epoch(2/10)\t Batch(2400/3375): \tTrain Loss  0.0735\n",
            "Epoch(2/10)\t Batch(2600/3375): \tTrain Loss  0.0646\n",
            "Epoch(2/10)\t Batch(2800/3375): \tTrain Loss  0.0772\n",
            "Epoch(2/10)\t Batch(3000/3375): \tTrain Loss  0.0714\n",
            "Epoch(2/10)\t Batch(3200/3375): \tTrain Loss  0.0700\n",
            "End of Epoch: 2 Train Accuracy: 0.5301 Val Loss: 0.0683 Val Accuracy: 0.5297\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/3375): \tTrain Loss  0.0594\n",
            "Epoch(3/10)\t Batch(200/3375): \tTrain Loss  0.0687\n",
            "Epoch(3/10)\t Batch(400/3375): \tTrain Loss  0.0696\n",
            "Epoch(3/10)\t Batch(600/3375): \tTrain Loss  0.0592\n",
            "Epoch(3/10)\t Batch(800/3375): \tTrain Loss  0.0628\n",
            "Epoch(3/10)\t Batch(1000/3375): \tTrain Loss  0.0685\n",
            "Epoch(3/10)\t Batch(1200/3375): \tTrain Loss  0.0659\n",
            "Epoch(3/10)\t Batch(1400/3375): \tTrain Loss  0.0524\n",
            "Epoch(3/10)\t Batch(1600/3375): \tTrain Loss  0.0587\n",
            "Epoch(3/10)\t Batch(1800/3375): \tTrain Loss  0.0622\n",
            "Epoch(3/10)\t Batch(2000/3375): \tTrain Loss  0.0564\n",
            "Epoch(3/10)\t Batch(2200/3375): \tTrain Loss  0.0513\n",
            "Epoch(3/10)\t Batch(2400/3375): \tTrain Loss  0.0585\n",
            "Epoch(3/10)\t Batch(2600/3375): \tTrain Loss  0.0720\n",
            "Epoch(3/10)\t Batch(2800/3375): \tTrain Loss  0.0520\n",
            "Epoch(3/10)\t Batch(3000/3375): \tTrain Loss  0.0633\n",
            "Epoch(3/10)\t Batch(3200/3375): \tTrain Loss  0.0548\n",
            "End of Epoch: 3 Train Accuracy: 0.6319 Val Loss: 0.0543 Val Accuracy: 0.6278\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/3375): \tTrain Loss  0.0629\n",
            "Epoch(4/10)\t Batch(200/3375): \tTrain Loss  0.0546\n",
            "Epoch(4/10)\t Batch(400/3375): \tTrain Loss  0.0632\n",
            "Epoch(4/10)\t Batch(600/3375): \tTrain Loss  0.0408\n",
            "Epoch(4/10)\t Batch(800/3375): \tTrain Loss  0.0616\n",
            "Epoch(4/10)\t Batch(1000/3375): \tTrain Loss  0.0336\n",
            "Epoch(4/10)\t Batch(1200/3375): \tTrain Loss  0.0542\n",
            "Epoch(4/10)\t Batch(1400/3375): \tTrain Loss  0.0592\n",
            "Epoch(4/10)\t Batch(1600/3375): \tTrain Loss  0.0383\n",
            "Epoch(4/10)\t Batch(1800/3375): \tTrain Loss  0.0498\n",
            "Epoch(4/10)\t Batch(2000/3375): \tTrain Loss  0.0627\n",
            "Epoch(4/10)\t Batch(2200/3375): \tTrain Loss  0.0386\n",
            "Epoch(4/10)\t Batch(2400/3375): \tTrain Loss  0.0199\n",
            "Epoch(4/10)\t Batch(2600/3375): \tTrain Loss  0.0450\n",
            "Epoch(4/10)\t Batch(2800/3375): \tTrain Loss  0.0488\n",
            "Epoch(4/10)\t Batch(3000/3375): \tTrain Loss  0.0506\n",
            "Epoch(4/10)\t Batch(3200/3375): \tTrain Loss  0.0425\n",
            "End of Epoch: 4 Train Accuracy: 0.6608 Val Loss: 0.0465 Val Accuracy: 0.6615\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/3375): \tTrain Loss  0.0464\n",
            "Epoch(5/10)\t Batch(200/3375): \tTrain Loss  0.0557\n",
            "Epoch(5/10)\t Batch(400/3375): \tTrain Loss  0.0372\n",
            "Epoch(5/10)\t Batch(600/3375): \tTrain Loss  0.0354\n",
            "Epoch(5/10)\t Batch(800/3375): \tTrain Loss  0.0522\n",
            "Epoch(5/10)\t Batch(1000/3375): \tTrain Loss  0.0338\n",
            "Epoch(5/10)\t Batch(1200/3375): \tTrain Loss  0.0324\n",
            "Epoch(5/10)\t Batch(1400/3375): \tTrain Loss  0.0386\n",
            "Epoch(5/10)\t Batch(1600/3375): \tTrain Loss  0.0528\n",
            "Epoch(5/10)\t Batch(1800/3375): \tTrain Loss  0.0413\n",
            "Epoch(5/10)\t Batch(2000/3375): \tTrain Loss  0.0292\n",
            "Epoch(5/10)\t Batch(2200/3375): \tTrain Loss  0.0423\n",
            "Epoch(5/10)\t Batch(2400/3375): \tTrain Loss  0.0451\n",
            "Epoch(5/10)\t Batch(2600/3375): \tTrain Loss  0.0433\n",
            "Epoch(5/10)\t Batch(2800/3375): \tTrain Loss  0.0391\n",
            "Epoch(5/10)\t Batch(3000/3375): \tTrain Loss  0.0403\n",
            "Epoch(5/10)\t Batch(3200/3375): \tTrain Loss  0.0440\n",
            "End of Epoch: 5 Train Accuracy: 0.6992 Val Loss: 0.0421 Val Accuracy: 0.6990\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/3375): \tTrain Loss  0.0416\n",
            "Epoch(6/10)\t Batch(200/3375): \tTrain Loss  0.0370\n",
            "Epoch(6/10)\t Batch(400/3375): \tTrain Loss  0.0366\n",
            "Epoch(6/10)\t Batch(600/3375): \tTrain Loss  0.0407\n",
            "Epoch(6/10)\t Batch(800/3375): \tTrain Loss  0.0417\n",
            "Epoch(6/10)\t Batch(1000/3375): \tTrain Loss  0.0454\n",
            "Epoch(6/10)\t Batch(1200/3375): \tTrain Loss  0.0585\n",
            "Epoch(6/10)\t Batch(1400/3375): \tTrain Loss  0.0405\n",
            "Epoch(6/10)\t Batch(1600/3375): \tTrain Loss  0.0245\n",
            "Epoch(6/10)\t Batch(1800/3375): \tTrain Loss  0.0468\n",
            "Epoch(6/10)\t Batch(2000/3375): \tTrain Loss  0.0517\n",
            "Epoch(6/10)\t Batch(2200/3375): \tTrain Loss  0.0284\n",
            "Epoch(6/10)\t Batch(2400/3375): \tTrain Loss  0.0655\n",
            "Epoch(6/10)\t Batch(2600/3375): \tTrain Loss  0.0463\n",
            "Epoch(6/10)\t Batch(2800/3375): \tTrain Loss  0.0216\n",
            "Epoch(6/10)\t Batch(3000/3375): \tTrain Loss  0.0296\n",
            "Epoch(6/10)\t Batch(3200/3375): \tTrain Loss  0.0449\n",
            "End of Epoch: 6 Train Accuracy: 0.7442 Val Loss: 0.0386 Val Accuracy: 0.7420\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/3375): \tTrain Loss  0.0290\n",
            "Epoch(7/10)\t Batch(200/3375): \tTrain Loss  0.0251\n",
            "Epoch(7/10)\t Batch(400/3375): \tTrain Loss  0.0471\n",
            "Epoch(7/10)\t Batch(600/3375): \tTrain Loss  0.0435\n",
            "Epoch(7/10)\t Batch(800/3375): \tTrain Loss  0.0338\n",
            "Epoch(7/10)\t Batch(1000/3375): \tTrain Loss  0.0383\n",
            "Epoch(7/10)\t Batch(1200/3375): \tTrain Loss  0.0404\n",
            "Epoch(7/10)\t Batch(1400/3375): \tTrain Loss  0.0329\n",
            "Epoch(7/10)\t Batch(1600/3375): \tTrain Loss  0.0213\n",
            "Epoch(7/10)\t Batch(1800/3375): \tTrain Loss  0.0406\n",
            "Epoch(7/10)\t Batch(2000/3375): \tTrain Loss  0.0369\n",
            "Epoch(7/10)\t Batch(2200/3375): \tTrain Loss  0.0318\n",
            "Epoch(7/10)\t Batch(2400/3375): \tTrain Loss  0.0440\n",
            "Epoch(7/10)\t Batch(2600/3375): \tTrain Loss  0.0509\n",
            "Epoch(7/10)\t Batch(2800/3375): \tTrain Loss  0.0442\n",
            "Epoch(7/10)\t Batch(3000/3375): \tTrain Loss  0.0346\n",
            "Epoch(7/10)\t Batch(3200/3375): \tTrain Loss  0.0280\n",
            "End of Epoch: 7 Train Accuracy: 0.7656 Val Loss: 0.0356 Val Accuracy: 0.7618\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/3375): \tTrain Loss  0.0157\n",
            "Epoch(8/10)\t Batch(200/3375): \tTrain Loss  0.0343\n",
            "Epoch(8/10)\t Batch(400/3375): \tTrain Loss  0.0418\n",
            "Epoch(8/10)\t Batch(600/3375): \tTrain Loss  0.0274\n",
            "Epoch(8/10)\t Batch(800/3375): \tTrain Loss  0.0370\n",
            "Epoch(8/10)\t Batch(1000/3375): \tTrain Loss  0.0385\n",
            "Epoch(8/10)\t Batch(1200/3375): \tTrain Loss  0.0155\n",
            "Epoch(8/10)\t Batch(1400/3375): \tTrain Loss  0.0490\n",
            "Epoch(8/10)\t Batch(1600/3375): \tTrain Loss  0.0269\n",
            "Epoch(8/10)\t Batch(1800/3375): \tTrain Loss  0.0408\n",
            "Epoch(8/10)\t Batch(2000/3375): \tTrain Loss  0.0316\n",
            "Epoch(8/10)\t Batch(2200/3375): \tTrain Loss  0.0524\n",
            "Epoch(8/10)\t Batch(2400/3375): \tTrain Loss  0.0324\n",
            "Epoch(8/10)\t Batch(2600/3375): \tTrain Loss  0.0286\n",
            "Epoch(8/10)\t Batch(2800/3375): \tTrain Loss  0.0359\n",
            "Epoch(8/10)\t Batch(3000/3375): \tTrain Loss  0.0099\n",
            "Epoch(8/10)\t Batch(3200/3375): \tTrain Loss  0.0258\n",
            "End of Epoch: 8 Train Accuracy: 0.7837 Val Loss: 0.0331 Val Accuracy: 0.7802\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/3375): \tTrain Loss  0.0345\n",
            "Epoch(9/10)\t Batch(200/3375): \tTrain Loss  0.0450\n",
            "Epoch(9/10)\t Batch(400/3375): \tTrain Loss  0.0283\n",
            "Epoch(9/10)\t Batch(600/3375): \tTrain Loss  0.0446\n",
            "Epoch(9/10)\t Batch(800/3375): \tTrain Loss  0.0390\n",
            "Epoch(9/10)\t Batch(1000/3375): \tTrain Loss  0.0234\n",
            "Epoch(9/10)\t Batch(1200/3375): \tTrain Loss  0.0386\n",
            "Epoch(9/10)\t Batch(1400/3375): \tTrain Loss  0.0319\n",
            "Epoch(9/10)\t Batch(1600/3375): \tTrain Loss  0.0279\n",
            "Epoch(9/10)\t Batch(1800/3375): \tTrain Loss  0.0205\n",
            "Epoch(9/10)\t Batch(2000/3375): \tTrain Loss  0.0259\n",
            "Epoch(9/10)\t Batch(2200/3375): \tTrain Loss  0.0327\n",
            "Epoch(9/10)\t Batch(2400/3375): \tTrain Loss  0.0423\n",
            "Epoch(9/10)\t Batch(2600/3375): \tTrain Loss  0.0385\n",
            "Epoch(9/10)\t Batch(2800/3375): \tTrain Loss  0.0394\n",
            "Epoch(9/10)\t Batch(3000/3375): \tTrain Loss  0.0369\n",
            "Epoch(9/10)\t Batch(3200/3375): \tTrain Loss  0.0356\n",
            "End of Epoch: 9 Train Accuracy: 0.7961 Val Loss: 0.0313 Val Accuracy: 0.7897\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/3375): \tTrain Loss  0.0448\n",
            "Epoch(10/10)\t Batch(200/3375): \tTrain Loss  0.0221\n",
            "Epoch(10/10)\t Batch(400/3375): \tTrain Loss  0.0611\n",
            "Epoch(10/10)\t Batch(600/3375): \tTrain Loss  0.0379\n",
            "Epoch(10/10)\t Batch(800/3375): \tTrain Loss  0.0221\n",
            "Epoch(10/10)\t Batch(1000/3375): \tTrain Loss  0.0187\n",
            "Epoch(10/10)\t Batch(1200/3375): \tTrain Loss  0.0427\n",
            "Epoch(10/10)\t Batch(1400/3375): \tTrain Loss  0.0190\n",
            "Epoch(10/10)\t Batch(1600/3375): \tTrain Loss  0.0192\n",
            "Epoch(10/10)\t Batch(1800/3375): \tTrain Loss  0.0272\n",
            "Epoch(10/10)\t Batch(2000/3375): \tTrain Loss  0.0472\n",
            "Epoch(10/10)\t Batch(2200/3375): \tTrain Loss  0.0118\n",
            "Epoch(10/10)\t Batch(2400/3375): \tTrain Loss  0.0346\n",
            "Epoch(10/10)\t Batch(2600/3375): \tTrain Loss  0.0198\n",
            "Epoch(10/10)\t Batch(2800/3375): \tTrain Loss  0.0282\n",
            "Epoch(10/10)\t Batch(3000/3375): \tTrain Loss  0.0257\n",
            "Epoch(10/10)\t Batch(3200/3375): \tTrain Loss  0.0456\n",
            "End of Epoch: 10 Train Accuracy: 0.8044 Val Loss: 0.0298 Val Accuracy: 0.7955\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▅▅▆▇▇▇████</td></tr><tr><td>Train Loss</td><td>█▆▅▄▃▃▃▂▁▂</td></tr><tr><td>Val Accuracy</td><td>▁▆▅▆▇▇▇████</td></tr><tr><td>Val Loss</td><td>█▇▅▄▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.80443</td></tr><tr><td>Train Loss</td><td>0.02924</td></tr><tr><td>Val Accuracy</td><td>0.7955</td></tr><tr><td>Val Loss</td><td>0.02984</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nag|loss_mse|lr=0.0001|batch_16|act_tanh|hid_4|neurons_128|nrns_10|init_Xavier497</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/38r7nxu2' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/38r7nxu2</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_182237-38r7nxu2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7w5rwxdi with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_182623-7w5rwxdi</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/7w5rwxdi' target=\"_blank\">solar-sweep-80</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/7w5rwxdi' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/7w5rwxdi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.0560 Val Loss: 0.0903 Val Accuracy: 0.0548\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/843): \tTrain Loss  0.0903\n",
            "Epoch(1/5)\t Batch(200/843): \tTrain Loss  0.0904\n",
            "Epoch(1/5)\t Batch(400/843): \tTrain Loss  0.0904\n",
            "Epoch(1/5)\t Batch(600/843): \tTrain Loss  0.0904\n",
            "Epoch(1/5)\t Batch(800/843): \tTrain Loss  0.0903\n",
            "End of Epoch: 1 Train Accuracy: 0.0649 Val Loss: 0.0902 Val Accuracy: 0.0615\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/843): \tTrain Loss  0.0901\n",
            "Epoch(2/5)\t Batch(200/843): \tTrain Loss  0.0898\n",
            "Epoch(2/5)\t Batch(400/843): \tTrain Loss  0.0901\n",
            "Epoch(2/5)\t Batch(600/843): \tTrain Loss  0.0901\n",
            "Epoch(2/5)\t Batch(800/843): \tTrain Loss  0.0901\n",
            "End of Epoch: 2 Train Accuracy: 0.0747 Val Loss: 0.0902 Val Accuracy: 0.0712\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/843): \tTrain Loss  0.0901\n",
            "Epoch(3/5)\t Batch(200/843): \tTrain Loss  0.0902\n",
            "Epoch(3/5)\t Batch(400/843): \tTrain Loss  0.0901\n",
            "Epoch(3/5)\t Batch(600/843): \tTrain Loss  0.0903\n",
            "Epoch(3/5)\t Batch(800/843): \tTrain Loss  0.0900\n",
            "End of Epoch: 3 Train Accuracy: 0.0839 Val Loss: 0.0901 Val Accuracy: 0.0813\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/843): \tTrain Loss  0.0900\n",
            "Epoch(4/5)\t Batch(200/843): \tTrain Loss  0.0898\n",
            "Epoch(4/5)\t Batch(400/843): \tTrain Loss  0.0897\n",
            "Epoch(4/5)\t Batch(600/843): \tTrain Loss  0.0900\n",
            "Epoch(4/5)\t Batch(800/843): \tTrain Loss  0.0900\n",
            "End of Epoch: 4 Train Accuracy: 0.0927 Val Loss: 0.0901 Val Accuracy: 0.0917\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/843): \tTrain Loss  0.0900\n",
            "Epoch(5/5)\t Batch(200/843): \tTrain Loss  0.0899\n",
            "Epoch(5/5)\t Batch(400/843): \tTrain Loss  0.0900\n",
            "Epoch(5/5)\t Batch(600/843): \tTrain Loss  0.0900\n",
            "Epoch(5/5)\t Batch(800/843): \tTrain Loss  0.0901\n",
            "End of Epoch: 5 Train Accuracy: 0.1016 Val Loss: 0.0900 Val Accuracy: 0.1003\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▂▄▅▇█</td></tr><tr><td>Train Loss</td><td>▆▁█▆▆</td></tr><tr><td>Val Accuracy</td><td>▁▂▄▅▇█</td></tr><tr><td>Val Loss</td><td>█▇▅▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.10163</td></tr><tr><td>Train Loss</td><td>0.0902</td></tr><tr><td>Val Accuracy</td><td>0.10033</td></tr><tr><td>Val Loss</td><td>0.09</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_sgd|loss_mse|lr=0.0001|batch_64|act_tanh|hid_5|neurons_128|nrns_5|init_Xavier480</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/7w5rwxdi' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/7w5rwxdi</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_182623-7w5rwxdi/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8965terp with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_182754-8965terp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8965terp' target=\"_blank\">flowing-sweep-81</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8965terp' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8965terp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.1004 Val Loss: 0.0914 Val Accuracy: 0.0963\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  0.0935\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  0.0767\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  0.0556\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  0.0336\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  0.0283\n",
            "End of Epoch: 1 Train Accuracy: 0.7346 Val Loss: 0.0340 Val Accuracy: 0.7385\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  0.0338\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  0.0324\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  0.0264\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  0.0225\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  0.0245\n",
            "End of Epoch: 2 Train Accuracy: 0.8051 Val Loss: 0.0271 Val Accuracy: 0.8048\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  0.0254\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  0.0221\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  0.0239\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  0.0198\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  0.0311\n",
            "End of Epoch: 3 Train Accuracy: 0.8277 Val Loss: 0.0257 Val Accuracy: 0.8197\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  0.0172\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  0.0177\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  0.0226\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  0.0200\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  0.0108\n",
            "End of Epoch: 4 Train Accuracy: 0.8501 Val Loss: 0.0232 Val Accuracy: 0.8417\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  0.0195\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  0.0137\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  0.0211\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  0.0281\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  0.0276\n",
            "End of Epoch: 5 Train Accuracy: 0.8681 Val Loss: 0.0212 Val Accuracy: 0.8552\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  0.0138\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  0.0286\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  0.0175\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  0.0136\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  0.0136\n",
            "End of Epoch: 6 Train Accuracy: 0.8696 Val Loss: 0.0211 Val Accuracy: 0.8557\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  0.0274\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  0.0137\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  0.0122\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  0.0266\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  0.0209\n",
            "End of Epoch: 7 Train Accuracy: 0.8724 Val Loss: 0.0207 Val Accuracy: 0.8575\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  0.0194\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  0.0199\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  0.0107\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  0.0113\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  0.0207\n",
            "End of Epoch: 8 Train Accuracy: 0.8813 Val Loss: 0.0202 Val Accuracy: 0.8632\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  0.0172\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  0.0163\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  0.0167\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  0.0167\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  0.0199\n",
            "End of Epoch: 9 Train Accuracy: 0.8855 Val Loss: 0.0196 Val Accuracy: 0.8660\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  0.0123\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  0.0154\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  0.0166\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  0.0205\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  0.0139\n",
            "End of Epoch: 10 Train Accuracy: 0.8864 Val Loss: 0.0193 Val Accuracy: 0.8648\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▇▇▇███████</td></tr><tr><td>Train Loss</td><td>█▆▅▆▄▄▅▁▃▄</td></tr><tr><td>Val Accuracy</td><td>▁▇▇████████</td></tr><tr><td>Val Loss</td><td>█▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.88644</td></tr><tr><td>Train Loss</td><td>0.01843</td></tr><tr><td>Val Accuracy</td><td>0.86483</td></tr><tr><td>Val Loss</td><td>0.01932</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_mse|lr=0.001|batch_64|act_sigmoid|hid_3|neurons_64|nrns_10|init_Xavier213</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8965terp' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/8965terp</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_182754-8965terp/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e4sknsxj with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_182904-e4sknsxj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/e4sknsxj' target=\"_blank\">soft-sweep-82</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/e4sknsxj' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/e4sknsxj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.1235 Val Loss: 0.0900 Val Accuracy: 0.1205\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/1687): \tTrain Loss  0.0905\n",
            "Epoch(1/5)\t Batch(200/1687): \tTrain Loss  0.0475\n",
            "Epoch(1/5)\t Batch(400/1687): \tTrain Loss  0.0414\n",
            "Epoch(1/5)\t Batch(600/1687): \tTrain Loss  0.0242\n",
            "Epoch(1/5)\t Batch(800/1687): \tTrain Loss  0.0314\n",
            "Epoch(1/5)\t Batch(1000/1687): \tTrain Loss  0.0310\n",
            "Epoch(1/5)\t Batch(1200/1687): \tTrain Loss  0.0214\n",
            "Epoch(1/5)\t Batch(1400/1687): \tTrain Loss  0.0334\n",
            "Epoch(1/5)\t Batch(1600/1687): \tTrain Loss  0.0125\n",
            "End of Epoch: 1 Train Accuracy: 0.8491 Val Loss: 0.0225 Val Accuracy: 0.8435\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/1687): \tTrain Loss  0.0351\n",
            "Epoch(2/5)\t Batch(200/1687): \tTrain Loss  0.0379\n",
            "Epoch(2/5)\t Batch(400/1687): \tTrain Loss  0.0182\n",
            "Epoch(2/5)\t Batch(600/1687): \tTrain Loss  0.0175\n",
            "Epoch(2/5)\t Batch(800/1687): \tTrain Loss  0.0210\n",
            "Epoch(2/5)\t Batch(1000/1687): \tTrain Loss  0.0245\n",
            "Epoch(2/5)\t Batch(1200/1687): \tTrain Loss  0.0237\n",
            "Epoch(2/5)\t Batch(1400/1687): \tTrain Loss  0.0325\n",
            "Epoch(2/5)\t Batch(1600/1687): \tTrain Loss  0.0146\n",
            "End of Epoch: 2 Train Accuracy: 0.8637 Val Loss: 0.0206 Val Accuracy: 0.8587\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/1687): \tTrain Loss  0.0171\n",
            "Epoch(3/5)\t Batch(200/1687): \tTrain Loss  0.0145\n",
            "Epoch(3/5)\t Batch(400/1687): \tTrain Loss  0.0035\n",
            "Epoch(3/5)\t Batch(600/1687): \tTrain Loss  0.0189\n",
            "Epoch(3/5)\t Batch(800/1687): \tTrain Loss  0.0223\n",
            "Epoch(3/5)\t Batch(1000/1687): \tTrain Loss  0.0230\n",
            "Epoch(3/5)\t Batch(1200/1687): \tTrain Loss  0.0143\n",
            "Epoch(3/5)\t Batch(1400/1687): \tTrain Loss  0.0418\n",
            "Epoch(3/5)\t Batch(1600/1687): \tTrain Loss  0.0226\n",
            "End of Epoch: 3 Train Accuracy: 0.8746 Val Loss: 0.0196 Val Accuracy: 0.8648\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/1687): \tTrain Loss  0.0133\n",
            "Epoch(4/5)\t Batch(200/1687): \tTrain Loss  0.0082\n",
            "Epoch(4/5)\t Batch(400/1687): \tTrain Loss  0.0115\n",
            "Epoch(4/5)\t Batch(600/1687): \tTrain Loss  0.0140\n",
            "Epoch(4/5)\t Batch(800/1687): \tTrain Loss  0.0243\n",
            "Epoch(4/5)\t Batch(1000/1687): \tTrain Loss  0.0215\n",
            "Epoch(4/5)\t Batch(1200/1687): \tTrain Loss  0.0141\n",
            "Epoch(4/5)\t Batch(1400/1687): \tTrain Loss  0.0159\n",
            "Epoch(4/5)\t Batch(1600/1687): \tTrain Loss  0.0209\n",
            "End of Epoch: 4 Train Accuracy: 0.8820 Val Loss: 0.0190 Val Accuracy: 0.8672\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/1687): \tTrain Loss  0.0180\n",
            "Epoch(5/5)\t Batch(200/1687): \tTrain Loss  0.0180\n",
            "Epoch(5/5)\t Batch(400/1687): \tTrain Loss  0.0111\n",
            "Epoch(5/5)\t Batch(600/1687): \tTrain Loss  0.0125\n",
            "Epoch(5/5)\t Batch(800/1687): \tTrain Loss  0.0205\n",
            "Epoch(5/5)\t Batch(1000/1687): \tTrain Loss  0.0127\n",
            "Epoch(5/5)\t Batch(1200/1687): \tTrain Loss  0.0160\n",
            "Epoch(5/5)\t Batch(1400/1687): \tTrain Loss  0.0240\n",
            "Epoch(5/5)\t Batch(1600/1687): \tTrain Loss  0.0213\n",
            "End of Epoch: 5 Train Accuracy: 0.8885 Val Loss: 0.0186 Val Accuracy: 0.8712\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁█████</td></tr><tr><td>Train Loss</td><td>▁█▁▆▂</td></tr><tr><td>Val Accuracy</td><td>▁█████</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.88852</td></tr><tr><td>Train Loss</td><td>0.01962</td></tr><tr><td>Val Accuracy</td><td>0.87117</td></tr><tr><td>Val Loss</td><td>0.01858</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nadam|loss_mse|lr=0.0001|batch_32|act_tanh|hid_5|neurons_64|nrns_5|init_Xavier888</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/e4sknsxj' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/e4sknsxj</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_182904-e4sknsxj/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 35qna8k3 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_183020-35qna8k3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/35qna8k3' target=\"_blank\">graceful-sweep-83</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/35qna8k3' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/35qna8k3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.0996 Val Loss: 0.0922 Val Accuracy: 0.1032\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/3375): \tTrain Loss  0.0924\n",
            "Epoch(1/5)\t Batch(200/3375): \tTrain Loss  0.0902\n",
            "Epoch(1/5)\t Batch(400/3375): \tTrain Loss  0.0896\n",
            "Epoch(1/5)\t Batch(600/3375): \tTrain Loss  0.0896\n",
            "Epoch(1/5)\t Batch(800/3375): \tTrain Loss  0.0892\n",
            "Epoch(1/5)\t Batch(1000/3375): \tTrain Loss  0.0879\n",
            "Epoch(1/5)\t Batch(1200/3375): \tTrain Loss  0.0865\n",
            "Epoch(1/5)\t Batch(1400/3375): \tTrain Loss  0.0837\n",
            "Epoch(1/5)\t Batch(1600/3375): \tTrain Loss  0.0830\n",
            "Epoch(1/5)\t Batch(1800/3375): \tTrain Loss  0.0764\n",
            "Epoch(1/5)\t Batch(2000/3375): \tTrain Loss  0.0754\n",
            "Epoch(1/5)\t Batch(2200/3375): \tTrain Loss  0.0689\n",
            "Epoch(1/5)\t Batch(2400/3375): \tTrain Loss  0.0685\n",
            "Epoch(1/5)\t Batch(2600/3375): \tTrain Loss  0.0545\n",
            "Epoch(1/5)\t Batch(2800/3375): \tTrain Loss  0.0602\n",
            "Epoch(1/5)\t Batch(3000/3375): \tTrain Loss  0.0579\n",
            "Epoch(1/5)\t Batch(3200/3375): \tTrain Loss  0.0605\n",
            "End of Epoch: 1 Train Accuracy: 0.6336 Val Loss: 0.0553 Val Accuracy: 0.6288\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/3375): \tTrain Loss  0.0587\n",
            "Epoch(2/5)\t Batch(200/3375): \tTrain Loss  0.0583\n",
            "Epoch(2/5)\t Batch(400/3375): \tTrain Loss  0.0502\n",
            "Epoch(2/5)\t Batch(600/3375): \tTrain Loss  0.0392\n",
            "Epoch(2/5)\t Batch(800/3375): \tTrain Loss  0.0394\n",
            "Epoch(2/5)\t Batch(1000/3375): \tTrain Loss  0.0445\n",
            "Epoch(2/5)\t Batch(1200/3375): \tTrain Loss  0.0697\n",
            "Epoch(2/5)\t Batch(1400/3375): \tTrain Loss  0.0516\n",
            "Epoch(2/5)\t Batch(1600/3375): \tTrain Loss  0.0492\n",
            "Epoch(2/5)\t Batch(1800/3375): \tTrain Loss  0.0392\n",
            "Epoch(2/5)\t Batch(2000/3375): \tTrain Loss  0.0397\n",
            "Epoch(2/5)\t Batch(2200/3375): \tTrain Loss  0.0529\n",
            "Epoch(2/5)\t Batch(2400/3375): \tTrain Loss  0.0474\n",
            "Epoch(2/5)\t Batch(2600/3375): \tTrain Loss  0.0337\n",
            "Epoch(2/5)\t Batch(2800/3375): \tTrain Loss  0.0339\n",
            "Epoch(2/5)\t Batch(3000/3375): \tTrain Loss  0.0143\n",
            "Epoch(2/5)\t Batch(3200/3375): \tTrain Loss  0.0215\n",
            "End of Epoch: 2 Train Accuracy: 0.7479 Val Loss: 0.0365 Val Accuracy: 0.7353\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/3375): \tTrain Loss  0.0271\n",
            "Epoch(3/5)\t Batch(200/3375): \tTrain Loss  0.0222\n",
            "Epoch(3/5)\t Batch(400/3375): \tTrain Loss  0.0403\n",
            "Epoch(3/5)\t Batch(600/3375): \tTrain Loss  0.0369\n",
            "Epoch(3/5)\t Batch(800/3375): \tTrain Loss  0.0416\n",
            "Epoch(3/5)\t Batch(1000/3375): \tTrain Loss  0.0349\n",
            "Epoch(3/5)\t Batch(1200/3375): \tTrain Loss  0.0460\n",
            "Epoch(3/5)\t Batch(1400/3375): \tTrain Loss  0.0267\n",
            "Epoch(3/5)\t Batch(1600/3375): \tTrain Loss  0.0298\n",
            "Epoch(3/5)\t Batch(1800/3375): \tTrain Loss  0.0350\n",
            "Epoch(3/5)\t Batch(2000/3375): \tTrain Loss  0.0267\n",
            "Epoch(3/5)\t Batch(2200/3375): \tTrain Loss  0.0178\n",
            "Epoch(3/5)\t Batch(2400/3375): \tTrain Loss  0.0350\n",
            "Epoch(3/5)\t Batch(2600/3375): \tTrain Loss  0.0418\n",
            "Epoch(3/5)\t Batch(2800/3375): \tTrain Loss  0.0437\n",
            "Epoch(3/5)\t Batch(3000/3375): \tTrain Loss  0.0268\n",
            "Epoch(3/5)\t Batch(3200/3375): \tTrain Loss  0.0261\n",
            "End of Epoch: 3 Train Accuracy: 0.7808 Val Loss: 0.0317 Val Accuracy: 0.7688\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/3375): \tTrain Loss  0.0219\n",
            "Epoch(4/5)\t Batch(200/3375): \tTrain Loss  0.0123\n",
            "Epoch(4/5)\t Batch(400/3375): \tTrain Loss  0.0312\n",
            "Epoch(4/5)\t Batch(600/3375): \tTrain Loss  0.0465\n",
            "Epoch(4/5)\t Batch(800/3375): \tTrain Loss  0.0283\n",
            "Epoch(4/5)\t Batch(1000/3375): \tTrain Loss  0.0096\n",
            "Epoch(4/5)\t Batch(1200/3375): \tTrain Loss  0.0197\n",
            "Epoch(4/5)\t Batch(1400/3375): \tTrain Loss  0.0296\n",
            "Epoch(4/5)\t Batch(1600/3375): \tTrain Loss  0.0318\n",
            "Epoch(4/5)\t Batch(1800/3375): \tTrain Loss  0.0355\n",
            "Epoch(4/5)\t Batch(2000/3375): \tTrain Loss  0.0331\n",
            "Epoch(4/5)\t Batch(2200/3375): \tTrain Loss  0.0249\n",
            "Epoch(4/5)\t Batch(2400/3375): \tTrain Loss  0.0422\n",
            "Epoch(4/5)\t Batch(2600/3375): \tTrain Loss  0.0569\n",
            "Epoch(4/5)\t Batch(2800/3375): \tTrain Loss  0.0222\n",
            "Epoch(4/5)\t Batch(3000/3375): \tTrain Loss  0.0280\n",
            "Epoch(4/5)\t Batch(3200/3375): \tTrain Loss  0.0487\n",
            "End of Epoch: 4 Train Accuracy: 0.8024 Val Loss: 0.0288 Val Accuracy: 0.7955\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/3375): \tTrain Loss  0.0257\n",
            "Epoch(5/5)\t Batch(200/3375): \tTrain Loss  0.0341\n",
            "Epoch(5/5)\t Batch(400/3375): \tTrain Loss  0.0300\n",
            "Epoch(5/5)\t Batch(600/3375): \tTrain Loss  0.0319\n",
            "Epoch(5/5)\t Batch(800/3375): \tTrain Loss  0.0138\n",
            "Epoch(5/5)\t Batch(1000/3375): \tTrain Loss  0.0286\n",
            "Epoch(5/5)\t Batch(1200/3375): \tTrain Loss  0.0342\n",
            "Epoch(5/5)\t Batch(1400/3375): \tTrain Loss  0.0268\n",
            "Epoch(5/5)\t Batch(1600/3375): \tTrain Loss  0.0254\n",
            "Epoch(5/5)\t Batch(1800/3375): \tTrain Loss  0.0402\n",
            "Epoch(5/5)\t Batch(2000/3375): \tTrain Loss  0.0190\n",
            "Epoch(5/5)\t Batch(2200/3375): \tTrain Loss  0.0402\n",
            "Epoch(5/5)\t Batch(2400/3375): \tTrain Loss  0.0345\n",
            "Epoch(5/5)\t Batch(2600/3375): \tTrain Loss  0.0148\n",
            "Epoch(5/5)\t Batch(2800/3375): \tTrain Loss  0.0182\n",
            "Epoch(5/5)\t Batch(3000/3375): \tTrain Loss  0.0204\n",
            "Epoch(5/5)\t Batch(3200/3375): \tTrain Loss  0.0321\n",
            "End of Epoch: 5 Train Accuracy: 0.8224 Val Loss: 0.0266 Val Accuracy: 0.8150\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▆▇███</td></tr><tr><td>Train Loss</td><td>▇▄█▁▆</td></tr><tr><td>Val Accuracy</td><td>▁▆▇███</td></tr><tr><td>Val Loss</td><td>█▄▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.82237</td></tr><tr><td>Train Loss</td><td>0.04521</td></tr><tr><td>Val Accuracy</td><td>0.815</td></tr><tr><td>Val Loss</td><td>0.02661</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_mse|lr=0.0001|batch_16|act_sigmoid|hid_3|neurons_128|nrns_5|init_Xavier44</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/35qna8k3' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/35qna8k3</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_183020-35qna8k3/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nuzhnhv4 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_183200-nuzhnhv4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/nuzhnhv4' target=\"_blank\">iconic-sweep-84</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/nuzhnhv4' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/nuzhnhv4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.0777 Val Loss: 2.3168 Val Accuracy: 0.0763\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/3375): \tTrain Loss  2.3385\n",
            "Epoch(1/10)\t Batch(200/3375): \tTrain Loss  1.9275\n",
            "Epoch(1/10)\t Batch(400/3375): \tTrain Loss  1.9152\n",
            "Epoch(1/10)\t Batch(600/3375): \tTrain Loss  1.7221\n",
            "Epoch(1/10)\t Batch(800/3375): \tTrain Loss  1.8554\n",
            "Epoch(1/10)\t Batch(1000/3375): \tTrain Loss  1.8173\n",
            "Epoch(1/10)\t Batch(1200/3375): \tTrain Loss  1.7552\n",
            "Epoch(1/10)\t Batch(1400/3375): \tTrain Loss  1.7867\n",
            "Epoch(1/10)\t Batch(1600/3375): \tTrain Loss  1.8866\n",
            "Epoch(1/10)\t Batch(1800/3375): \tTrain Loss  1.7636\n",
            "Epoch(1/10)\t Batch(2000/3375): \tTrain Loss  1.7605\n",
            "Epoch(1/10)\t Batch(2200/3375): \tTrain Loss  1.7601\n",
            "Epoch(1/10)\t Batch(2400/3375): \tTrain Loss  1.6853\n",
            "Epoch(1/10)\t Batch(2600/3375): \tTrain Loss  1.7825\n",
            "Epoch(1/10)\t Batch(2800/3375): \tTrain Loss  1.8292\n",
            "Epoch(1/10)\t Batch(3000/3375): \tTrain Loss  1.7502\n",
            "Epoch(1/10)\t Batch(3200/3375): \tTrain Loss  1.6816\n",
            "End of Epoch: 1 Train Accuracy: 0.8322 Val Loss: 1.7438 Val Accuracy: 0.8282\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/3375): \tTrain Loss  1.7179\n",
            "Epoch(2/10)\t Batch(200/3375): \tTrain Loss  1.8032\n",
            "Epoch(2/10)\t Batch(400/3375): \tTrain Loss  1.7064\n",
            "Epoch(2/10)\t Batch(600/3375): \tTrain Loss  1.6877\n",
            "Epoch(2/10)\t Batch(800/3375): \tTrain Loss  1.8350\n",
            "Epoch(2/10)\t Batch(1000/3375): \tTrain Loss  1.7252\n",
            "Epoch(2/10)\t Batch(1200/3375): \tTrain Loss  1.7038\n",
            "Epoch(2/10)\t Batch(1400/3375): \tTrain Loss  1.7616\n",
            "Epoch(2/10)\t Batch(1600/3375): \tTrain Loss  1.6724\n",
            "Epoch(2/10)\t Batch(1800/3375): \tTrain Loss  1.6834\n",
            "Epoch(2/10)\t Batch(2000/3375): \tTrain Loss  1.6283\n",
            "Epoch(2/10)\t Batch(2200/3375): \tTrain Loss  1.7271\n",
            "Epoch(2/10)\t Batch(2400/3375): \tTrain Loss  1.7579\n",
            "Epoch(2/10)\t Batch(2600/3375): \tTrain Loss  1.6187\n",
            "Epoch(2/10)\t Batch(2800/3375): \tTrain Loss  1.6955\n",
            "Epoch(2/10)\t Batch(3000/3375): \tTrain Loss  1.7229\n",
            "Epoch(2/10)\t Batch(3200/3375): \tTrain Loss  1.7549\n",
            "End of Epoch: 2 Train Accuracy: 0.8431 Val Loss: 1.7131 Val Accuracy: 0.8402\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/3375): \tTrain Loss  1.5937\n",
            "Epoch(3/10)\t Batch(200/3375): \tTrain Loss  1.6397\n",
            "Epoch(3/10)\t Batch(400/3375): \tTrain Loss  1.6522\n",
            "Epoch(3/10)\t Batch(600/3375): \tTrain Loss  1.7258\n",
            "Epoch(3/10)\t Batch(800/3375): \tTrain Loss  1.6826\n",
            "Epoch(3/10)\t Batch(1000/3375): \tTrain Loss  1.7593\n",
            "Epoch(3/10)\t Batch(1200/3375): \tTrain Loss  1.6778\n",
            "Epoch(3/10)\t Batch(1400/3375): \tTrain Loss  1.7557\n",
            "Epoch(3/10)\t Batch(1600/3375): \tTrain Loss  1.6266\n",
            "Epoch(3/10)\t Batch(1800/3375): \tTrain Loss  1.6011\n",
            "Epoch(3/10)\t Batch(2000/3375): \tTrain Loss  1.7532\n",
            "Epoch(3/10)\t Batch(2200/3375): \tTrain Loss  1.6943\n",
            "Epoch(3/10)\t Batch(2400/3375): \tTrain Loss  1.7499\n",
            "Epoch(3/10)\t Batch(2600/3375): \tTrain Loss  1.8767\n",
            "Epoch(3/10)\t Batch(2800/3375): \tTrain Loss  1.7979\n",
            "Epoch(3/10)\t Batch(3000/3375): \tTrain Loss  1.7377\n",
            "Epoch(3/10)\t Batch(3200/3375): \tTrain Loss  1.6147\n",
            "End of Epoch: 3 Train Accuracy: 0.8521 Val Loss: 1.7004 Val Accuracy: 0.8438\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/3375): \tTrain Loss  1.5329\n",
            "Epoch(4/10)\t Batch(200/3375): \tTrain Loss  1.8062\n",
            "Epoch(4/10)\t Batch(400/3375): \tTrain Loss  1.7999\n",
            "Epoch(4/10)\t Batch(600/3375): \tTrain Loss  1.7547\n",
            "Epoch(4/10)\t Batch(800/3375): \tTrain Loss  1.6876\n",
            "Epoch(4/10)\t Batch(1000/3375): \tTrain Loss  1.6259\n",
            "Epoch(4/10)\t Batch(1200/3375): \tTrain Loss  1.6970\n",
            "Epoch(4/10)\t Batch(1400/3375): \tTrain Loss  1.7180\n",
            "Epoch(4/10)\t Batch(1600/3375): \tTrain Loss  1.6930\n",
            "Epoch(4/10)\t Batch(1800/3375): \tTrain Loss  1.6835\n",
            "Epoch(4/10)\t Batch(2000/3375): \tTrain Loss  1.7928\n",
            "Epoch(4/10)\t Batch(2200/3375): \tTrain Loss  1.6923\n",
            "Epoch(4/10)\t Batch(2400/3375): \tTrain Loss  1.7040\n",
            "Epoch(4/10)\t Batch(2600/3375): \tTrain Loss  1.7094\n",
            "Epoch(4/10)\t Batch(2800/3375): \tTrain Loss  1.7143\n",
            "Epoch(4/10)\t Batch(3000/3375): \tTrain Loss  1.6500\n",
            "Epoch(4/10)\t Batch(3200/3375): \tTrain Loss  1.7190\n",
            "End of Epoch: 4 Train Accuracy: 0.8516 Val Loss: 1.6952 Val Accuracy: 0.8468\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/3375): \tTrain Loss  1.6275\n",
            "Epoch(5/10)\t Batch(200/3375): \tTrain Loss  1.6173\n",
            "Epoch(5/10)\t Batch(400/3375): \tTrain Loss  1.6741\n",
            "Epoch(5/10)\t Batch(600/3375): \tTrain Loss  1.5876\n",
            "Epoch(5/10)\t Batch(800/3375): \tTrain Loss  1.6548\n",
            "Epoch(5/10)\t Batch(1000/3375): \tTrain Loss  1.5006\n",
            "Epoch(5/10)\t Batch(1200/3375): \tTrain Loss  1.6563\n",
            "Epoch(5/10)\t Batch(1400/3375): \tTrain Loss  1.6825\n",
            "Epoch(5/10)\t Batch(1600/3375): \tTrain Loss  1.6291\n",
            "Epoch(5/10)\t Batch(1800/3375): \tTrain Loss  1.6984\n",
            "Epoch(5/10)\t Batch(2000/3375): \tTrain Loss  1.6962\n",
            "Epoch(5/10)\t Batch(2200/3375): \tTrain Loss  1.6178\n",
            "Epoch(5/10)\t Batch(2400/3375): \tTrain Loss  1.6179\n",
            "Epoch(5/10)\t Batch(2600/3375): \tTrain Loss  1.6853\n",
            "Epoch(5/10)\t Batch(2800/3375): \tTrain Loss  1.6692\n",
            "Epoch(5/10)\t Batch(3000/3375): \tTrain Loss  1.6465\n",
            "Epoch(5/10)\t Batch(3200/3375): \tTrain Loss  1.5386\n",
            "End of Epoch: 5 Train Accuracy: 0.8584 Val Loss: 1.6814 Val Accuracy: 0.8518\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/3375): \tTrain Loss  1.6179\n",
            "Epoch(6/10)\t Batch(200/3375): \tTrain Loss  1.6787\n",
            "Epoch(6/10)\t Batch(400/3375): \tTrain Loss  1.7020\n",
            "Epoch(6/10)\t Batch(600/3375): \tTrain Loss  1.6781\n",
            "Epoch(6/10)\t Batch(800/3375): \tTrain Loss  1.6002\n",
            "Epoch(6/10)\t Batch(1000/3375): \tTrain Loss  1.6663\n",
            "Epoch(6/10)\t Batch(1200/3375): \tTrain Loss  1.7485\n",
            "Epoch(6/10)\t Batch(1400/3375): \tTrain Loss  1.6459\n",
            "Epoch(6/10)\t Batch(1600/3375): \tTrain Loss  1.5746\n",
            "Epoch(6/10)\t Batch(1800/3375): \tTrain Loss  1.6750\n",
            "Epoch(6/10)\t Batch(2000/3375): \tTrain Loss  1.6267\n",
            "Epoch(6/10)\t Batch(2200/3375): \tTrain Loss  1.6390\n",
            "Epoch(6/10)\t Batch(2400/3375): \tTrain Loss  1.6370\n",
            "Epoch(6/10)\t Batch(2600/3375): \tTrain Loss  1.6688\n",
            "Epoch(6/10)\t Batch(2800/3375): \tTrain Loss  1.6628\n",
            "Epoch(6/10)\t Batch(3000/3375): \tTrain Loss  1.6954\n",
            "Epoch(6/10)\t Batch(3200/3375): \tTrain Loss  1.6453\n",
            "End of Epoch: 6 Train Accuracy: 0.8611 Val Loss: 1.6822 Val Accuracy: 0.8543\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/3375): \tTrain Loss  1.7474\n",
            "Epoch(7/10)\t Batch(200/3375): \tTrain Loss  1.5226\n",
            "Epoch(7/10)\t Batch(400/3375): \tTrain Loss  1.6815\n",
            "Epoch(7/10)\t Batch(600/3375): \tTrain Loss  1.7983\n",
            "Epoch(7/10)\t Batch(800/3375): \tTrain Loss  1.6538\n",
            "Epoch(7/10)\t Batch(1000/3375): \tTrain Loss  1.8013\n",
            "Epoch(7/10)\t Batch(1200/3375): \tTrain Loss  1.5868\n",
            "Epoch(7/10)\t Batch(1400/3375): \tTrain Loss  1.7047\n",
            "Epoch(7/10)\t Batch(1600/3375): \tTrain Loss  1.6950\n",
            "Epoch(7/10)\t Batch(1800/3375): \tTrain Loss  1.6384\n",
            "Epoch(7/10)\t Batch(2000/3375): \tTrain Loss  1.7260\n",
            "Epoch(7/10)\t Batch(2200/3375): \tTrain Loss  1.5749\n",
            "Epoch(7/10)\t Batch(2400/3375): \tTrain Loss  1.5921\n",
            "Epoch(7/10)\t Batch(2600/3375): \tTrain Loss  1.5813\n",
            "Epoch(7/10)\t Batch(2800/3375): \tTrain Loss  1.5709\n",
            "Epoch(7/10)\t Batch(3000/3375): \tTrain Loss  1.7421\n",
            "Epoch(7/10)\t Batch(3200/3375): \tTrain Loss  1.7765\n",
            "End of Epoch: 7 Train Accuracy: 0.8585 Val Loss: 1.6831 Val Accuracy: 0.8530\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/3375): \tTrain Loss  1.6221\n",
            "Epoch(8/10)\t Batch(200/3375): \tTrain Loss  1.7226\n",
            "Epoch(8/10)\t Batch(400/3375): \tTrain Loss  1.5449\n",
            "Epoch(8/10)\t Batch(600/3375): \tTrain Loss  1.6111\n",
            "Epoch(8/10)\t Batch(800/3375): \tTrain Loss  1.5938\n",
            "Epoch(8/10)\t Batch(1000/3375): \tTrain Loss  1.6317\n",
            "Epoch(8/10)\t Batch(1200/3375): \tTrain Loss  1.6749\n",
            "Epoch(8/10)\t Batch(1400/3375): \tTrain Loss  1.6953\n",
            "Epoch(8/10)\t Batch(1600/3375): \tTrain Loss  1.6435\n",
            "Epoch(8/10)\t Batch(1800/3375): \tTrain Loss  1.6422\n",
            "Epoch(8/10)\t Batch(2000/3375): \tTrain Loss  1.6274\n",
            "Epoch(8/10)\t Batch(2200/3375): \tTrain Loss  1.7119\n",
            "Epoch(8/10)\t Batch(2400/3375): \tTrain Loss  1.6155\n",
            "Epoch(8/10)\t Batch(2600/3375): \tTrain Loss  1.7042\n",
            "Epoch(8/10)\t Batch(2800/3375): \tTrain Loss  1.6092\n",
            "Epoch(8/10)\t Batch(3000/3375): \tTrain Loss  1.7170\n",
            "Epoch(8/10)\t Batch(3200/3375): \tTrain Loss  1.7044\n",
            "End of Epoch: 8 Train Accuracy: 0.8617 Val Loss: 1.6787 Val Accuracy: 0.8533\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/3375): \tTrain Loss  1.6115\n",
            "Epoch(9/10)\t Batch(200/3375): \tTrain Loss  1.8169\n",
            "Epoch(9/10)\t Batch(400/3375): \tTrain Loss  1.7920\n",
            "Epoch(9/10)\t Batch(600/3375): \tTrain Loss  1.6376\n",
            "Epoch(9/10)\t Batch(800/3375): \tTrain Loss  1.8189\n",
            "Epoch(9/10)\t Batch(1000/3375): \tTrain Loss  1.7405\n",
            "Epoch(9/10)\t Batch(1200/3375): \tTrain Loss  1.5945\n",
            "Epoch(9/10)\t Batch(1400/3375): \tTrain Loss  1.7543\n",
            "Epoch(9/10)\t Batch(1600/3375): \tTrain Loss  1.6601\n",
            "Epoch(9/10)\t Batch(1800/3375): \tTrain Loss  1.5739\n",
            "Epoch(9/10)\t Batch(2000/3375): \tTrain Loss  1.6543\n",
            "Epoch(9/10)\t Batch(2200/3375): \tTrain Loss  1.7267\n",
            "Epoch(9/10)\t Batch(2400/3375): \tTrain Loss  1.7974\n",
            "Epoch(9/10)\t Batch(2600/3375): \tTrain Loss  1.6218\n",
            "Epoch(9/10)\t Batch(2800/3375): \tTrain Loss  1.6381\n",
            "Epoch(9/10)\t Batch(3000/3375): \tTrain Loss  1.6870\n",
            "Epoch(9/10)\t Batch(3200/3375): \tTrain Loss  1.7386\n",
            "End of Epoch: 9 Train Accuracy: 0.8630 Val Loss: 1.6793 Val Accuracy: 0.8528\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/3375): \tTrain Loss  1.7467\n",
            "Epoch(10/10)\t Batch(200/3375): \tTrain Loss  1.5570\n",
            "Epoch(10/10)\t Batch(400/3375): \tTrain Loss  1.5713\n",
            "Epoch(10/10)\t Batch(600/3375): \tTrain Loss  1.7204\n",
            "Epoch(10/10)\t Batch(800/3375): \tTrain Loss  1.6829\n",
            "Epoch(10/10)\t Batch(1000/3375): \tTrain Loss  1.7521\n",
            "Epoch(10/10)\t Batch(1200/3375): \tTrain Loss  1.6462\n",
            "Epoch(10/10)\t Batch(1400/3375): \tTrain Loss  1.7039\n",
            "Epoch(10/10)\t Batch(1600/3375): \tTrain Loss  1.6448\n",
            "Epoch(10/10)\t Batch(1800/3375): \tTrain Loss  1.6823\n",
            "Epoch(10/10)\t Batch(2000/3375): \tTrain Loss  1.6552\n",
            "Epoch(10/10)\t Batch(2200/3375): \tTrain Loss  1.6585\n",
            "Epoch(10/10)\t Batch(2400/3375): \tTrain Loss  1.7959\n",
            "Epoch(10/10)\t Batch(2600/3375): \tTrain Loss  1.7162\n",
            "Epoch(10/10)\t Batch(2800/3375): \tTrain Loss  1.6301\n",
            "Epoch(10/10)\t Batch(3000/3375): \tTrain Loss  1.6014\n",
            "Epoch(10/10)\t Batch(3200/3375): \tTrain Loss  1.8383\n",
            "End of Epoch: 10 Train Accuracy: 0.8674 Val Loss: 1.6836 Val Accuracy: 0.8602\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁██████████</td></tr><tr><td>Train Loss</td><td>▇█▅▄▃▆▁▆▄▇</td></tr><tr><td>Val Accuracy</td><td>▁██████████</td></tr><tr><td>Val Loss</td><td>█▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.86739</td></tr><tr><td>Train Loss</td><td>1.74965</td></tr><tr><td>Val Accuracy</td><td>0.86017</td></tr><tr><td>Val Loss</td><td>1.68364</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_cross_entropy|lr=0.0001|batch_16|act_tanh|hid_5|neurons_128|nrns_10|init_Xavier74</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/nuzhnhv4' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/nuzhnhv4</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_183200-nuzhnhv4/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ivd5m0sc with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_183617-ivd5m0sc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ivd5m0sc' target=\"_blank\">tough-sweep-85</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ivd5m0sc' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ivd5m0sc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.0993 Val Loss: 0.0917 Val Accuracy: 0.1065\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/3375): \tTrain Loss  0.0903\n",
            "Epoch(1/10)\t Batch(200/3375): \tTrain Loss  0.0907\n",
            "Epoch(1/10)\t Batch(400/3375): \tTrain Loss  0.0901\n",
            "Epoch(1/10)\t Batch(600/3375): \tTrain Loss  0.0839\n",
            "Epoch(1/10)\t Batch(800/3375): \tTrain Loss  0.0813\n",
            "Epoch(1/10)\t Batch(1000/3375): \tTrain Loss  0.0780\n",
            "Epoch(1/10)\t Batch(1200/3375): \tTrain Loss  0.0681\n",
            "Epoch(1/10)\t Batch(1400/3375): \tTrain Loss  0.0453\n",
            "Epoch(1/10)\t Batch(1600/3375): \tTrain Loss  0.0600\n",
            "Epoch(1/10)\t Batch(1800/3375): \tTrain Loss  0.0664\n",
            "Epoch(1/10)\t Batch(2000/3375): \tTrain Loss  0.0481\n",
            "Epoch(1/10)\t Batch(2200/3375): \tTrain Loss  0.0454\n",
            "Epoch(1/10)\t Batch(2400/3375): \tTrain Loss  0.0497\n",
            "Epoch(1/10)\t Batch(2600/3375): \tTrain Loss  0.0408\n",
            "Epoch(1/10)\t Batch(2800/3375): \tTrain Loss  0.0320\n",
            "Epoch(1/10)\t Batch(3000/3375): \tTrain Loss  0.0387\n",
            "Epoch(1/10)\t Batch(3200/3375): \tTrain Loss  0.0322\n",
            "End of Epoch: 1 Train Accuracy: 0.6896 Val Loss: 0.0412 Val Accuracy: 0.6743\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/3375): \tTrain Loss  0.0158\n",
            "Epoch(2/10)\t Batch(200/3375): \tTrain Loss  0.0504\n",
            "Epoch(2/10)\t Batch(400/3375): \tTrain Loss  0.0429\n",
            "Epoch(2/10)\t Batch(600/3375): \tTrain Loss  0.0356\n",
            "Epoch(2/10)\t Batch(800/3375): \tTrain Loss  0.0197\n",
            "Epoch(2/10)\t Batch(1000/3375): \tTrain Loss  0.0228\n",
            "Epoch(2/10)\t Batch(1200/3375): \tTrain Loss  0.0403\n",
            "Epoch(2/10)\t Batch(1400/3375): \tTrain Loss  0.0548\n",
            "Epoch(2/10)\t Batch(1600/3375): \tTrain Loss  0.0365\n",
            "Epoch(2/10)\t Batch(1800/3375): \tTrain Loss  0.0399\n",
            "Epoch(2/10)\t Batch(2000/3375): \tTrain Loss  0.0452\n",
            "Epoch(2/10)\t Batch(2200/3375): \tTrain Loss  0.0183\n",
            "Epoch(2/10)\t Batch(2400/3375): \tTrain Loss  0.0205\n",
            "Epoch(2/10)\t Batch(2600/3375): \tTrain Loss  0.0292\n",
            "Epoch(2/10)\t Batch(2800/3375): \tTrain Loss  0.0385\n",
            "Epoch(2/10)\t Batch(3000/3375): \tTrain Loss  0.0251\n",
            "Epoch(2/10)\t Batch(3200/3375): \tTrain Loss  0.0271\n",
            "End of Epoch: 2 Train Accuracy: 0.7889 Val Loss: 0.0328 Val Accuracy: 0.7682\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/3375): \tTrain Loss  0.0374\n",
            "Epoch(3/10)\t Batch(200/3375): \tTrain Loss  0.0395\n",
            "Epoch(3/10)\t Batch(400/3375): \tTrain Loss  0.0239\n",
            "Epoch(3/10)\t Batch(600/3375): \tTrain Loss  0.0369\n",
            "Epoch(3/10)\t Batch(800/3375): \tTrain Loss  0.0491\n",
            "Epoch(3/10)\t Batch(1000/3375): \tTrain Loss  0.0435\n",
            "Epoch(3/10)\t Batch(1200/3375): \tTrain Loss  0.0181\n",
            "Epoch(3/10)\t Batch(1400/3375): \tTrain Loss  0.0348\n",
            "Epoch(3/10)\t Batch(1600/3375): \tTrain Loss  0.0320\n",
            "Epoch(3/10)\t Batch(1800/3375): \tTrain Loss  0.0272\n",
            "Epoch(3/10)\t Batch(2000/3375): \tTrain Loss  0.0281\n",
            "Epoch(3/10)\t Batch(2200/3375): \tTrain Loss  0.0295\n",
            "Epoch(3/10)\t Batch(2400/3375): \tTrain Loss  0.0245\n",
            "Epoch(3/10)\t Batch(2600/3375): \tTrain Loss  0.0488\n",
            "Epoch(3/10)\t Batch(2800/3375): \tTrain Loss  0.0211\n",
            "Epoch(3/10)\t Batch(3000/3375): \tTrain Loss  0.0326\n",
            "Epoch(3/10)\t Batch(3200/3375): \tTrain Loss  0.0272\n",
            "End of Epoch: 3 Train Accuracy: 0.8069 Val Loss: 0.0297 Val Accuracy: 0.7863\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/3375): \tTrain Loss  0.0329\n",
            "Epoch(4/10)\t Batch(200/3375): \tTrain Loss  0.0233\n",
            "Epoch(4/10)\t Batch(400/3375): \tTrain Loss  0.0172\n",
            "Epoch(4/10)\t Batch(600/3375): \tTrain Loss  0.0185\n",
            "Epoch(4/10)\t Batch(800/3375): \tTrain Loss  0.0294\n",
            "Epoch(4/10)\t Batch(1000/3375): \tTrain Loss  0.0152\n",
            "Epoch(4/10)\t Batch(1200/3375): \tTrain Loss  0.0258\n",
            "Epoch(4/10)\t Batch(1400/3375): \tTrain Loss  0.0527\n",
            "Epoch(4/10)\t Batch(1600/3375): \tTrain Loss  0.0208\n",
            "Epoch(4/10)\t Batch(1800/3375): \tTrain Loss  0.0098\n",
            "Epoch(4/10)\t Batch(2000/3375): \tTrain Loss  0.0292\n",
            "Epoch(4/10)\t Batch(2200/3375): \tTrain Loss  0.0353\n",
            "Epoch(4/10)\t Batch(2400/3375): \tTrain Loss  0.0347\n",
            "Epoch(4/10)\t Batch(2600/3375): \tTrain Loss  0.0274\n",
            "Epoch(4/10)\t Batch(2800/3375): \tTrain Loss  0.0245\n",
            "Epoch(4/10)\t Batch(3000/3375): \tTrain Loss  0.0308\n",
            "Epoch(4/10)\t Batch(3200/3375): \tTrain Loss  0.0327\n",
            "End of Epoch: 4 Train Accuracy: 0.8270 Val Loss: 0.0270 Val Accuracy: 0.8093\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/3375): \tTrain Loss  0.0229\n",
            "Epoch(5/10)\t Batch(200/3375): \tTrain Loss  0.0225\n",
            "Epoch(5/10)\t Batch(400/3375): \tTrain Loss  0.0181\n",
            "Epoch(5/10)\t Batch(600/3375): \tTrain Loss  0.0349\n",
            "Epoch(5/10)\t Batch(800/3375): \tTrain Loss  0.0170\n",
            "Epoch(5/10)\t Batch(1000/3375): \tTrain Loss  0.0239\n",
            "Epoch(5/10)\t Batch(1200/3375): \tTrain Loss  0.0358\n",
            "Epoch(5/10)\t Batch(1400/3375): \tTrain Loss  0.0171\n",
            "Epoch(5/10)\t Batch(1600/3375): \tTrain Loss  0.0076\n",
            "Epoch(5/10)\t Batch(1800/3375): \tTrain Loss  0.0261\n",
            "Epoch(5/10)\t Batch(2000/3375): \tTrain Loss  0.0456\n",
            "Epoch(5/10)\t Batch(2200/3375): \tTrain Loss  0.0351\n",
            "Epoch(5/10)\t Batch(2400/3375): \tTrain Loss  0.0107\n",
            "Epoch(5/10)\t Batch(2600/3375): \tTrain Loss  0.0138\n",
            "Epoch(5/10)\t Batch(2800/3375): \tTrain Loss  0.0141\n",
            "Epoch(5/10)\t Batch(3000/3375): \tTrain Loss  0.0123\n",
            "Epoch(5/10)\t Batch(3200/3375): \tTrain Loss  0.0156\n",
            "End of Epoch: 5 Train Accuracy: 0.8409 Val Loss: 0.0253 Val Accuracy: 0.8245\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/3375): \tTrain Loss  0.0190\n",
            "Epoch(6/10)\t Batch(200/3375): \tTrain Loss  0.0224\n",
            "Epoch(6/10)\t Batch(400/3375): \tTrain Loss  0.0205\n",
            "Epoch(6/10)\t Batch(600/3375): \tTrain Loss  0.0058\n",
            "Epoch(6/10)\t Batch(800/3375): \tTrain Loss  0.0146\n",
            "Epoch(6/10)\t Batch(1000/3375): \tTrain Loss  0.0334\n",
            "Epoch(6/10)\t Batch(1200/3375): \tTrain Loss  0.0194\n",
            "Epoch(6/10)\t Batch(1400/3375): \tTrain Loss  0.0271\n",
            "Epoch(6/10)\t Batch(1600/3375): \tTrain Loss  0.0364\n",
            "Epoch(6/10)\t Batch(1800/3375): \tTrain Loss  0.0118\n",
            "Epoch(6/10)\t Batch(2000/3375): \tTrain Loss  0.0148\n",
            "Epoch(6/10)\t Batch(2200/3375): \tTrain Loss  0.0142\n",
            "Epoch(6/10)\t Batch(2400/3375): \tTrain Loss  0.0245\n",
            "Epoch(6/10)\t Batch(2600/3375): \tTrain Loss  0.0119\n",
            "Epoch(6/10)\t Batch(2800/3375): \tTrain Loss  0.0261\n",
            "Epoch(6/10)\t Batch(3000/3375): \tTrain Loss  0.0196\n",
            "Epoch(6/10)\t Batch(3200/3375): \tTrain Loss  0.0291\n",
            "End of Epoch: 6 Train Accuracy: 0.8423 Val Loss: 0.0254 Val Accuracy: 0.8250\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/3375): \tTrain Loss  0.0204\n",
            "Epoch(7/10)\t Batch(200/3375): \tTrain Loss  0.0202\n",
            "Epoch(7/10)\t Batch(400/3375): \tTrain Loss  0.0207\n",
            "Epoch(7/10)\t Batch(600/3375): \tTrain Loss  0.0253\n",
            "Epoch(7/10)\t Batch(800/3375): \tTrain Loss  0.0197\n",
            "Epoch(7/10)\t Batch(1000/3375): \tTrain Loss  0.0140\n",
            "Epoch(7/10)\t Batch(1200/3375): \tTrain Loss  0.0188\n",
            "Epoch(7/10)\t Batch(1400/3375): \tTrain Loss  0.0043\n",
            "Epoch(7/10)\t Batch(1600/3375): \tTrain Loss  0.0402\n",
            "Epoch(7/10)\t Batch(1800/3375): \tTrain Loss  0.0157\n",
            "Epoch(7/10)\t Batch(2000/3375): \tTrain Loss  0.0316\n",
            "Epoch(7/10)\t Batch(2200/3375): \tTrain Loss  0.0019\n",
            "Epoch(7/10)\t Batch(2400/3375): \tTrain Loss  0.0564\n",
            "Epoch(7/10)\t Batch(2600/3375): \tTrain Loss  0.0167\n",
            "Epoch(7/10)\t Batch(2800/3375): \tTrain Loss  0.0347\n",
            "Epoch(7/10)\t Batch(3000/3375): \tTrain Loss  0.0086\n",
            "Epoch(7/10)\t Batch(3200/3375): \tTrain Loss  0.0156\n",
            "End of Epoch: 7 Train Accuracy: 0.8578 Val Loss: 0.0234 Val Accuracy: 0.8377\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/3375): \tTrain Loss  0.0197\n",
            "Epoch(8/10)\t Batch(200/3375): \tTrain Loss  0.0096\n",
            "Epoch(8/10)\t Batch(400/3375): \tTrain Loss  0.0185\n",
            "Epoch(8/10)\t Batch(600/3375): \tTrain Loss  0.0203\n",
            "Epoch(8/10)\t Batch(800/3375): \tTrain Loss  0.0142\n",
            "Epoch(8/10)\t Batch(1000/3375): \tTrain Loss  0.0344\n",
            "Epoch(8/10)\t Batch(1200/3375): \tTrain Loss  0.0077\n",
            "Epoch(8/10)\t Batch(1400/3375): \tTrain Loss  0.0302\n",
            "Epoch(8/10)\t Batch(1600/3375): \tTrain Loss  0.0145\n",
            "Epoch(8/10)\t Batch(1800/3375): \tTrain Loss  0.0292\n",
            "Epoch(8/10)\t Batch(2000/3375): \tTrain Loss  0.0316\n",
            "Epoch(8/10)\t Batch(2200/3375): \tTrain Loss  0.0205\n",
            "Epoch(8/10)\t Batch(2400/3375): \tTrain Loss  0.0276\n",
            "Epoch(8/10)\t Batch(2600/3375): \tTrain Loss  0.0174\n",
            "Epoch(8/10)\t Batch(2800/3375): \tTrain Loss  0.0184\n",
            "Epoch(8/10)\t Batch(3000/3375): \tTrain Loss  0.0374\n",
            "Epoch(8/10)\t Batch(3200/3375): \tTrain Loss  0.0447\n",
            "End of Epoch: 8 Train Accuracy: 0.8545 Val Loss: 0.0237 Val Accuracy: 0.8350\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/3375): \tTrain Loss  0.0140\n",
            "Epoch(9/10)\t Batch(200/3375): \tTrain Loss  0.0326\n",
            "Epoch(9/10)\t Batch(400/3375): \tTrain Loss  0.0171\n",
            "Epoch(9/10)\t Batch(600/3375): \tTrain Loss  0.0225\n",
            "Epoch(9/10)\t Batch(800/3375): \tTrain Loss  0.0159\n",
            "Epoch(9/10)\t Batch(1000/3375): \tTrain Loss  0.0133\n",
            "Epoch(9/10)\t Batch(1200/3375): \tTrain Loss  0.0137\n",
            "Epoch(9/10)\t Batch(1400/3375): \tTrain Loss  0.0101\n",
            "Epoch(9/10)\t Batch(1600/3375): \tTrain Loss  0.0171\n",
            "Epoch(9/10)\t Batch(1800/3375): \tTrain Loss  0.0153\n",
            "Epoch(9/10)\t Batch(2000/3375): \tTrain Loss  0.0250\n",
            "Epoch(9/10)\t Batch(2200/3375): \tTrain Loss  0.0411\n",
            "Epoch(9/10)\t Batch(2400/3375): \tTrain Loss  0.0185\n",
            "Epoch(9/10)\t Batch(2600/3375): \tTrain Loss  0.0117\n",
            "Epoch(9/10)\t Batch(2800/3375): \tTrain Loss  0.0084\n",
            "Epoch(9/10)\t Batch(3000/3375): \tTrain Loss  0.0276\n",
            "Epoch(9/10)\t Batch(3200/3375): \tTrain Loss  0.0137\n",
            "End of Epoch: 9 Train Accuracy: 0.8615 Val Loss: 0.0232 Val Accuracy: 0.8410\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/3375): \tTrain Loss  0.0257\n",
            "Epoch(10/10)\t Batch(200/3375): \tTrain Loss  0.0104\n",
            "Epoch(10/10)\t Batch(400/3375): \tTrain Loss  0.0006\n",
            "Epoch(10/10)\t Batch(600/3375): \tTrain Loss  0.0182\n",
            "Epoch(10/10)\t Batch(800/3375): \tTrain Loss  0.0166\n",
            "Epoch(10/10)\t Batch(1000/3375): \tTrain Loss  0.0028\n",
            "Epoch(10/10)\t Batch(1200/3375): \tTrain Loss  0.0299\n",
            "Epoch(10/10)\t Batch(1400/3375): \tTrain Loss  0.0123\n",
            "Epoch(10/10)\t Batch(1600/3375): \tTrain Loss  0.0139\n",
            "Epoch(10/10)\t Batch(1800/3375): \tTrain Loss  0.0165\n",
            "Epoch(10/10)\t Batch(2000/3375): \tTrain Loss  0.0101\n",
            "Epoch(10/10)\t Batch(2200/3375): \tTrain Loss  0.0183\n",
            "Epoch(10/10)\t Batch(2400/3375): \tTrain Loss  0.0171\n",
            "Epoch(10/10)\t Batch(2600/3375): \tTrain Loss  0.0073\n",
            "Epoch(10/10)\t Batch(2800/3375): \tTrain Loss  0.0146\n",
            "Epoch(10/10)\t Batch(3000/3375): \tTrain Loss  0.0247\n",
            "Epoch(10/10)\t Batch(3200/3375): \tTrain Loss  0.0213\n",
            "End of Epoch: 10 Train Accuracy: 0.8698 Val Loss: 0.0222 Val Accuracy: 0.8495\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▆▇▇███████</td></tr><tr><td>Train Loss</td><td>█▄▇▃█▄▂▁▃▅</td></tr><tr><td>Val Accuracy</td><td>▁▆▇▇███████</td></tr><tr><td>Val Loss</td><td>█▃▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.8698</td></tr><tr><td>Train Loss</td><td>0.02347</td></tr><tr><td>Val Accuracy</td><td>0.8495</td></tr><tr><td>Val Loss</td><td>0.02223</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_adam|loss_mse|lr=0.001|batch_16|act_sigmoid|hid_5|neurons_128|nrns_10|init_Xavier410</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ivd5m0sc' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ivd5m0sc</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_183617-ivd5m0sc/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ypz5q3id with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_184119-ypz5q3id</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ypz5q3id' target=\"_blank\">bumbling-sweep-86</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ypz5q3id' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ypz5q3id</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.0901 Val Loss: 12.6850 Val Accuracy: 0.0888\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  13.5457\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  4.7137\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  3.0432\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  2.5650\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  2.7613\n",
            "End of Epoch: 1 Train Accuracy: 0.1251 Val Loss: 2.4584 Val Accuracy: 0.1238\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  2.4822\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  2.3745\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  2.2792\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  2.4327\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  2.2992\n",
            "End of Epoch: 2 Train Accuracy: 0.1122 Val Loss: 2.3391 Val Accuracy: 0.1140\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  2.3909\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  2.3876\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  2.2430\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  2.2895\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  2.2975\n",
            "End of Epoch: 3 Train Accuracy: 0.1089 Val Loss: 2.3275 Val Accuracy: 0.1020\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  2.3164\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  2.2778\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  2.2908\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  2.3324\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  2.3206\n",
            "End of Epoch: 4 Train Accuracy: 0.1006 Val Loss: 2.3205 Val Accuracy: 0.0982\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  2.3738\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  2.3233\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  2.3196\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  2.3575\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  2.2922\n",
            "End of Epoch: 5 Train Accuracy: 0.1433 Val Loss: 2.3181 Val Accuracy: 0.1342\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  2.3469\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  2.2998\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  2.2999\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  2.2822\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  2.3019\n",
            "End of Epoch: 6 Train Accuracy: 0.1080 Val Loss: 2.3146 Val Accuracy: 0.1048\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  2.3323\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  2.3016\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  2.2756\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  2.3283\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  2.3748\n",
            "End of Epoch: 7 Train Accuracy: 0.1066 Val Loss: 2.3123 Val Accuracy: 0.1068\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  2.3060\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  2.3030\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  2.3108\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  2.2981\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  2.2669\n",
            "End of Epoch: 8 Train Accuracy: 0.1066 Val Loss: 2.3104 Val Accuracy: 0.1050\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  2.2998\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  2.3038\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  2.3029\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  2.2996\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  2.3033\n",
            "End of Epoch: 9 Train Accuracy: 0.1071 Val Loss: 2.3095 Val Accuracy: 0.1075\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  2.3061\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  2.3023\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  2.3424\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  2.3021\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  2.3043\n",
            "End of Epoch: 10 Train Accuracy: 0.1012 Val Loss: 2.3097 Val Accuracy: 0.0970\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▆▄▃▂█▃▃▃▃▂</td></tr><tr><td>Train Loss</td><td>█▅▁▅▃▂▂▃▆▃</td></tr><tr><td>Val Accuracy</td><td>▁▆▅▃▂█▃▄▃▄▂</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.10117</td></tr><tr><td>Train Loss</td><td>2.30301</td></tr><tr><td>Val Accuracy</td><td>0.097</td></tr><tr><td>Val Loss</td><td>2.30968</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_sgd|loss_cross_entropy|lr=0.001|batch_64|act_tanh|hid_4|neurons_64|nrns_10|init_Random605</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ypz5q3id' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ypz5q3id</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_184119-ypz5q3id/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lis5xf8y with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_184224-lis5xf8y</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/lis5xf8y' target=\"_blank\">sandy-sweep-87</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/lis5xf8y' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/lis5xf8y</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.1000 Val Loss: 0.0916 Val Accuracy: 0.1003\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/3375): \tTrain Loss  0.0913\n",
            "Epoch(1/10)\t Batch(200/3375): \tTrain Loss  0.0918\n",
            "Epoch(1/10)\t Batch(400/3375): \tTrain Loss  0.0901\n",
            "Epoch(1/10)\t Batch(600/3375): \tTrain Loss  0.0898\n",
            "Epoch(1/10)\t Batch(800/3375): \tTrain Loss  0.0900\n",
            "Epoch(1/10)\t Batch(1000/3375): \tTrain Loss  0.0903\n",
            "Epoch(1/10)\t Batch(1200/3375): \tTrain Loss  0.0901\n",
            "Epoch(1/10)\t Batch(1400/3375): \tTrain Loss  0.0901\n",
            "Epoch(1/10)\t Batch(1600/3375): \tTrain Loss  0.0899\n",
            "Epoch(1/10)\t Batch(1800/3375): \tTrain Loss  0.0897\n",
            "Epoch(1/10)\t Batch(2000/3375): \tTrain Loss  0.0900\n",
            "Epoch(1/10)\t Batch(2200/3375): \tTrain Loss  0.0901\n",
            "Epoch(1/10)\t Batch(2400/3375): \tTrain Loss  0.0901\n",
            "Epoch(1/10)\t Batch(2600/3375): \tTrain Loss  0.0901\n",
            "Epoch(1/10)\t Batch(2800/3375): \tTrain Loss  0.0898\n",
            "Epoch(1/10)\t Batch(3000/3375): \tTrain Loss  0.0903\n",
            "Epoch(1/10)\t Batch(3200/3375): \tTrain Loss  0.0896\n",
            "End of Epoch: 1 Train Accuracy: 0.0997 Val Loss: 0.0899 Val Accuracy: 0.1027\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/3375): \tTrain Loss  0.0897\n",
            "Epoch(2/10)\t Batch(200/3375): \tTrain Loss  0.0895\n",
            "Epoch(2/10)\t Batch(400/3375): \tTrain Loss  0.0898\n",
            "Epoch(2/10)\t Batch(600/3375): \tTrain Loss  0.0894\n",
            "Epoch(2/10)\t Batch(800/3375): \tTrain Loss  0.0893\n",
            "Epoch(2/10)\t Batch(1000/3375): \tTrain Loss  0.0873\n",
            "Epoch(2/10)\t Batch(1200/3375): \tTrain Loss  0.0868\n",
            "Epoch(2/10)\t Batch(1400/3375): \tTrain Loss  0.0852\n",
            "Epoch(2/10)\t Batch(1600/3375): \tTrain Loss  0.0809\n",
            "Epoch(2/10)\t Batch(1800/3375): \tTrain Loss  0.0845\n",
            "Epoch(2/10)\t Batch(2000/3375): \tTrain Loss  0.0805\n",
            "Epoch(2/10)\t Batch(2200/3375): \tTrain Loss  0.0813\n",
            "Epoch(2/10)\t Batch(2400/3375): \tTrain Loss  0.0825\n",
            "Epoch(2/10)\t Batch(2600/3375): \tTrain Loss  0.0814\n",
            "Epoch(2/10)\t Batch(2800/3375): \tTrain Loss  0.0819\n",
            "Epoch(2/10)\t Batch(3000/3375): \tTrain Loss  0.0808\n",
            "Epoch(2/10)\t Batch(3200/3375): \tTrain Loss  0.0760\n",
            "End of Epoch: 2 Train Accuracy: 0.2753 Val Loss: 0.0785 Val Accuracy: 0.2787\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/3375): \tTrain Loss  0.0774\n",
            "Epoch(3/10)\t Batch(200/3375): \tTrain Loss  0.0750\n",
            "Epoch(3/10)\t Batch(400/3375): \tTrain Loss  0.0751\n",
            "Epoch(3/10)\t Batch(600/3375): \tTrain Loss  0.0748\n",
            "Epoch(3/10)\t Batch(800/3375): \tTrain Loss  0.0759\n",
            "Epoch(3/10)\t Batch(1000/3375): \tTrain Loss  0.0783\n",
            "Epoch(3/10)\t Batch(1200/3375): \tTrain Loss  0.0777\n",
            "Epoch(3/10)\t Batch(1400/3375): \tTrain Loss  0.0762\n",
            "Epoch(3/10)\t Batch(1600/3375): \tTrain Loss  0.0656\n",
            "Epoch(3/10)\t Batch(1800/3375): \tTrain Loss  0.0692\n",
            "Epoch(3/10)\t Batch(2000/3375): \tTrain Loss  0.0700\n",
            "Epoch(3/10)\t Batch(2200/3375): \tTrain Loss  0.0720\n",
            "Epoch(3/10)\t Batch(2400/3375): \tTrain Loss  0.0792\n",
            "Epoch(3/10)\t Batch(2600/3375): \tTrain Loss  0.0722\n",
            "Epoch(3/10)\t Batch(2800/3375): \tTrain Loss  0.0777\n",
            "Epoch(3/10)\t Batch(3000/3375): \tTrain Loss  0.0727\n",
            "Epoch(3/10)\t Batch(3200/3375): \tTrain Loss  0.0589\n",
            "End of Epoch: 3 Train Accuracy: 0.3662 Val Loss: 0.0699 Val Accuracy: 0.3747\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/3375): \tTrain Loss  0.0734\n",
            "Epoch(4/10)\t Batch(200/3375): \tTrain Loss  0.0672\n",
            "Epoch(4/10)\t Batch(400/3375): \tTrain Loss  0.0692\n",
            "Epoch(4/10)\t Batch(600/3375): \tTrain Loss  0.0713\n",
            "Epoch(4/10)\t Batch(800/3375): \tTrain Loss  0.0596\n",
            "Epoch(4/10)\t Batch(1000/3375): \tTrain Loss  0.0730\n",
            "Epoch(4/10)\t Batch(1200/3375): \tTrain Loss  0.0653\n",
            "Epoch(4/10)\t Batch(1400/3375): \tTrain Loss  0.0699\n",
            "Epoch(4/10)\t Batch(1600/3375): \tTrain Loss  0.0681\n",
            "Epoch(4/10)\t Batch(1800/3375): \tTrain Loss  0.0700\n",
            "Epoch(4/10)\t Batch(2000/3375): \tTrain Loss  0.0612\n",
            "Epoch(4/10)\t Batch(2200/3375): \tTrain Loss  0.0671\n",
            "Epoch(4/10)\t Batch(2400/3375): \tTrain Loss  0.0642\n",
            "Epoch(4/10)\t Batch(2600/3375): \tTrain Loss  0.0733\n",
            "Epoch(4/10)\t Batch(2800/3375): \tTrain Loss  0.0740\n",
            "Epoch(4/10)\t Batch(3000/3375): \tTrain Loss  0.0457\n",
            "Epoch(4/10)\t Batch(3200/3375): \tTrain Loss  0.0751\n",
            "End of Epoch: 4 Train Accuracy: 0.4347 Val Loss: 0.0649 Val Accuracy: 0.4440\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/3375): \tTrain Loss  0.0544\n",
            "Epoch(5/10)\t Batch(200/3375): \tTrain Loss  0.0670\n",
            "Epoch(5/10)\t Batch(400/3375): \tTrain Loss  0.0581\n",
            "Epoch(5/10)\t Batch(600/3375): \tTrain Loss  0.0620\n",
            "Epoch(5/10)\t Batch(800/3375): \tTrain Loss  0.0515\n",
            "Epoch(5/10)\t Batch(1000/3375): \tTrain Loss  0.0710\n",
            "Epoch(5/10)\t Batch(1200/3375): \tTrain Loss  0.0648\n",
            "Epoch(5/10)\t Batch(1400/3375): \tTrain Loss  0.0694\n",
            "Epoch(5/10)\t Batch(1600/3375): \tTrain Loss  0.0790\n",
            "Epoch(5/10)\t Batch(1800/3375): \tTrain Loss  0.0698\n",
            "Epoch(5/10)\t Batch(2000/3375): \tTrain Loss  0.0709\n",
            "Epoch(5/10)\t Batch(2200/3375): \tTrain Loss  0.0460\n",
            "Epoch(5/10)\t Batch(2400/3375): \tTrain Loss  0.0696\n",
            "Epoch(5/10)\t Batch(2600/3375): \tTrain Loss  0.0752\n",
            "Epoch(5/10)\t Batch(2800/3375): \tTrain Loss  0.0519\n",
            "Epoch(5/10)\t Batch(3000/3375): \tTrain Loss  0.0685\n",
            "Epoch(5/10)\t Batch(3200/3375): \tTrain Loss  0.0538\n",
            "End of Epoch: 5 Train Accuracy: 0.4667 Val Loss: 0.0623 Val Accuracy: 0.4757\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/3375): \tTrain Loss  0.0577\n",
            "Epoch(6/10)\t Batch(200/3375): \tTrain Loss  0.0695\n",
            "Epoch(6/10)\t Batch(400/3375): \tTrain Loss  0.0606\n",
            "Epoch(6/10)\t Batch(600/3375): \tTrain Loss  0.0700\n",
            "Epoch(6/10)\t Batch(800/3375): \tTrain Loss  0.0610\n",
            "Epoch(6/10)\t Batch(1000/3375): \tTrain Loss  0.0638\n",
            "Epoch(6/10)\t Batch(1200/3375): \tTrain Loss  0.0731\n",
            "Epoch(6/10)\t Batch(1400/3375): \tTrain Loss  0.0664\n",
            "Epoch(6/10)\t Batch(1600/3375): \tTrain Loss  0.0674\n",
            "Epoch(6/10)\t Batch(1800/3375): \tTrain Loss  0.0680\n",
            "Epoch(6/10)\t Batch(2000/3375): \tTrain Loss  0.0576\n",
            "Epoch(6/10)\t Batch(2200/3375): \tTrain Loss  0.0646\n",
            "Epoch(6/10)\t Batch(2400/3375): \tTrain Loss  0.0551\n",
            "Epoch(6/10)\t Batch(2600/3375): \tTrain Loss  0.0500\n",
            "Epoch(6/10)\t Batch(2800/3375): \tTrain Loss  0.0641\n",
            "Epoch(6/10)\t Batch(3000/3375): \tTrain Loss  0.0655\n",
            "Epoch(6/10)\t Batch(3200/3375): \tTrain Loss  0.0603\n",
            "End of Epoch: 6 Train Accuracy: 0.4812 Val Loss: 0.0609 Val Accuracy: 0.4920\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/3375): \tTrain Loss  0.0633\n",
            "Epoch(7/10)\t Batch(200/3375): \tTrain Loss  0.0708\n",
            "Epoch(7/10)\t Batch(400/3375): \tTrain Loss  0.0611\n",
            "Epoch(7/10)\t Batch(600/3375): \tTrain Loss  0.0714\n",
            "Epoch(7/10)\t Batch(800/3375): \tTrain Loss  0.0699\n",
            "Epoch(7/10)\t Batch(1000/3375): \tTrain Loss  0.0580\n",
            "Epoch(7/10)\t Batch(1200/3375): \tTrain Loss  0.0438\n",
            "Epoch(7/10)\t Batch(1400/3375): \tTrain Loss  0.0528\n",
            "Epoch(7/10)\t Batch(1600/3375): \tTrain Loss  0.0755\n",
            "Epoch(7/10)\t Batch(1800/3375): \tTrain Loss  0.0631\n",
            "Epoch(7/10)\t Batch(2000/3375): \tTrain Loss  0.0572\n",
            "Epoch(7/10)\t Batch(2200/3375): \tTrain Loss  0.0831\n",
            "Epoch(7/10)\t Batch(2400/3375): \tTrain Loss  0.0654\n",
            "Epoch(7/10)\t Batch(2600/3375): \tTrain Loss  0.0605\n",
            "Epoch(7/10)\t Batch(2800/3375): \tTrain Loss  0.0582\n",
            "Epoch(7/10)\t Batch(3000/3375): \tTrain Loss  0.0618\n",
            "Epoch(7/10)\t Batch(3200/3375): \tTrain Loss  0.0530\n",
            "End of Epoch: 7 Train Accuracy: 0.5117 Val Loss: 0.0584 Val Accuracy: 0.5170\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/3375): \tTrain Loss  0.0614\n",
            "Epoch(8/10)\t Batch(200/3375): \tTrain Loss  0.0642\n",
            "Epoch(8/10)\t Batch(400/3375): \tTrain Loss  0.0608\n",
            "Epoch(8/10)\t Batch(600/3375): \tTrain Loss  0.0648\n",
            "Epoch(8/10)\t Batch(800/3375): \tTrain Loss  0.0519\n",
            "Epoch(8/10)\t Batch(1000/3375): \tTrain Loss  0.0572\n",
            "Epoch(8/10)\t Batch(1200/3375): \tTrain Loss  0.0452\n",
            "Epoch(8/10)\t Batch(1400/3375): \tTrain Loss  0.0499\n",
            "Epoch(8/10)\t Batch(1600/3375): \tTrain Loss  0.0728\n",
            "Epoch(8/10)\t Batch(1800/3375): \tTrain Loss  0.0578\n",
            "Epoch(8/10)\t Batch(2000/3375): \tTrain Loss  0.0414\n",
            "Epoch(8/10)\t Batch(2200/3375): \tTrain Loss  0.0555\n",
            "Epoch(8/10)\t Batch(2400/3375): \tTrain Loss  0.0515\n",
            "Epoch(8/10)\t Batch(2600/3375): \tTrain Loss  0.0590\n",
            "Epoch(8/10)\t Batch(2800/3375): \tTrain Loss  0.0529\n",
            "Epoch(8/10)\t Batch(3000/3375): \tTrain Loss  0.0586\n",
            "Epoch(8/10)\t Batch(3200/3375): \tTrain Loss  0.0628\n",
            "End of Epoch: 8 Train Accuracy: 0.5582 Val Loss: 0.0541 Val Accuracy: 0.5713\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/3375): \tTrain Loss  0.0451\n",
            "Epoch(9/10)\t Batch(200/3375): \tTrain Loss  0.0550\n",
            "Epoch(9/10)\t Batch(400/3375): \tTrain Loss  0.0511\n",
            "Epoch(9/10)\t Batch(600/3375): \tTrain Loss  0.0499\n",
            "Epoch(9/10)\t Batch(800/3375): \tTrain Loss  0.0602\n",
            "Epoch(9/10)\t Batch(1000/3375): \tTrain Loss  0.0573\n",
            "Epoch(9/10)\t Batch(1200/3375): \tTrain Loss  0.0501\n",
            "Epoch(9/10)\t Batch(1400/3375): \tTrain Loss  0.0488\n",
            "Epoch(9/10)\t Batch(1600/3375): \tTrain Loss  0.0585\n",
            "Epoch(9/10)\t Batch(1800/3375): \tTrain Loss  0.0621\n",
            "Epoch(9/10)\t Batch(2000/3375): \tTrain Loss  0.0571\n",
            "Epoch(9/10)\t Batch(2200/3375): \tTrain Loss  0.0540\n",
            "Epoch(9/10)\t Batch(2400/3375): \tTrain Loss  0.0492\n",
            "Epoch(9/10)\t Batch(2600/3375): \tTrain Loss  0.0465\n",
            "Epoch(9/10)\t Batch(2800/3375): \tTrain Loss  0.0365\n",
            "Epoch(9/10)\t Batch(3000/3375): \tTrain Loss  0.0515\n",
            "Epoch(9/10)\t Batch(3200/3375): \tTrain Loss  0.0601\n",
            "End of Epoch: 9 Train Accuracy: 0.5755 Val Loss: 0.0520 Val Accuracy: 0.5767\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/3375): \tTrain Loss  0.0570\n",
            "Epoch(10/10)\t Batch(200/3375): \tTrain Loss  0.0618\n",
            "Epoch(10/10)\t Batch(400/3375): \tTrain Loss  0.0460\n",
            "Epoch(10/10)\t Batch(600/3375): \tTrain Loss  0.0509\n",
            "Epoch(10/10)\t Batch(800/3375): \tTrain Loss  0.0562\n",
            "Epoch(10/10)\t Batch(1000/3375): \tTrain Loss  0.0527\n",
            "Epoch(10/10)\t Batch(1200/3375): \tTrain Loss  0.0462\n",
            "Epoch(10/10)\t Batch(1400/3375): \tTrain Loss  0.0361\n",
            "Epoch(10/10)\t Batch(1600/3375): \tTrain Loss  0.0457\n",
            "Epoch(10/10)\t Batch(1800/3375): \tTrain Loss  0.0441\n",
            "Epoch(10/10)\t Batch(2000/3375): \tTrain Loss  0.0468\n",
            "Epoch(10/10)\t Batch(2200/3375): \tTrain Loss  0.0501\n",
            "Epoch(10/10)\t Batch(2400/3375): \tTrain Loss  0.0502\n",
            "Epoch(10/10)\t Batch(2600/3375): \tTrain Loss  0.0450\n",
            "Epoch(10/10)\t Batch(2800/3375): \tTrain Loss  0.0403\n",
            "Epoch(10/10)\t Batch(3000/3375): \tTrain Loss  0.0436\n",
            "Epoch(10/10)\t Batch(3200/3375): \tTrain Loss  0.0505\n",
            "End of Epoch: 10 Train Accuracy: 0.6099 Val Loss: 0.0506 Val Accuracy: 0.6120\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▁▃▅▆▆▆▇▇██</td></tr><tr><td>Train Loss</td><td>█▆▄▃▂▆▁▂▂▂</td></tr><tr><td>Val Accuracy</td><td>▁▁▃▅▆▆▆▇▇██</td></tr><tr><td>Val Loss</td><td>██▆▄▃▃▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.60991</td></tr><tr><td>Train Loss</td><td>0.04818</td></tr><tr><td>Val Accuracy</td><td>0.612</td></tr><tr><td>Val Loss</td><td>0.05061</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_mse|lr=0.0001|batch_16|act_sigmoid|hid_4|neurons_64|nrns_10|init_Xavier368</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/lis5xf8y' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/lis5xf8y</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_184224-lis5xf8y/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pha121ri with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nag\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_184440-pha121ri</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/pha121ri' target=\"_blank\">major-sweep-88</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/pha121ri' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/pha121ri</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.0871 Val Loss: 4.4656 Val Accuracy: 0.0887\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  4.5421\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  2.3456\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  2.3438\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  2.3642\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  2.2856\n",
            "End of Epoch: 1 Train Accuracy: 0.1165 Val Loss: 2.2949 Val Accuracy: 0.1132\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  2.2816\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  2.3276\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  2.3416\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  2.3307\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  2.2834\n",
            "End of Epoch: 2 Train Accuracy: 0.1350 Val Loss: 2.2792 Val Accuracy: 0.1317\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  2.2975\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  2.2942\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  2.2723\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  2.2909\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  2.2768\n",
            "End of Epoch: 3 Train Accuracy: 0.1488 Val Loss: 2.2732 Val Accuracy: 0.1430\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  2.2919\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  2.2695\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  2.2731\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  2.2775\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  2.2738\n",
            "End of Epoch: 4 Train Accuracy: 0.1654 Val Loss: 2.2701 Val Accuracy: 0.1598\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  2.2795\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  2.2646\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  2.2700\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  2.2825\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  2.2742\n",
            "End of Epoch: 5 Train Accuracy: 0.1809 Val Loss: 2.2686 Val Accuracy: 0.1742\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  2.2598\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  2.2953\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  2.2643\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  2.2866\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  2.2780\n",
            "End of Epoch: 6 Train Accuracy: 0.1960 Val Loss: 2.2676 Val Accuracy: 0.1947\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  2.2465\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  2.2748\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  2.2699\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  2.2528\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  2.2692\n",
            "End of Epoch: 7 Train Accuracy: 0.2097 Val Loss: 2.2672 Val Accuracy: 0.2080\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  2.2753\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  2.2603\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  2.2701\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  2.2552\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  2.2733\n",
            "End of Epoch: 8 Train Accuracy: 0.2232 Val Loss: 2.2669 Val Accuracy: 0.2253\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  2.2672\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  2.2888\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  2.2577\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  2.2471\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  2.2641\n",
            "End of Epoch: 9 Train Accuracy: 0.2391 Val Loss: 2.2669 Val Accuracy: 0.2413\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  2.2698\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  2.2662\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  2.2741\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  2.2719\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  2.2622\n",
            "End of Epoch: 10 Train Accuracy: 0.2484 Val Loss: 2.2672 Val Accuracy: 0.2498\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▄▄▅▆▆▇██</td></tr><tr><td>Train Loss</td><td>█▅▆▃▃▂▃▁▄▂</td></tr><tr><td>Val Accuracy</td><td>▁▂▃▃▄▅▆▆▇██</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.24844</td></tr><tr><td>Train Loss</td><td>2.25818</td></tr><tr><td>Val Accuracy</td><td>0.24983</td></tr><tr><td>Val Loss</td><td>2.26723</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nag|loss_cross_entropy|lr=0.0001|batch_64|act_sigmoid|hid_4|neurons_32|nrns_10|init_Random280</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/pha121ri' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/pha121ri</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_184440-pha121ri/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gvl34kcj with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_184515-gvl34kcj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/gvl34kcj' target=\"_blank\">solar-sweep-89</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/gvl34kcj' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/gvl34kcj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.1404 Val Loss: 0.0900 Val Accuracy: 0.1447\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  0.0899\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  0.0900\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  0.0901\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  0.0900\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  0.0900\n",
            "End of Epoch: 1 Train Accuracy: 0.1451 Val Loss: 0.0900 Val Accuracy: 0.1498\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  0.0900\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  0.0899\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  0.0900\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  0.0900\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  0.0901\n",
            "End of Epoch: 2 Train Accuracy: 0.1504 Val Loss: 0.0900 Val Accuracy: 0.1537\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  0.0898\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  0.0901\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  0.0899\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  0.0900\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  0.0899\n",
            "End of Epoch: 3 Train Accuracy: 0.1552 Val Loss: 0.0900 Val Accuracy: 0.1580\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  0.0899\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  0.0899\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  0.0900\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  0.0900\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  0.0899\n",
            "End of Epoch: 4 Train Accuracy: 0.1593 Val Loss: 0.0899 Val Accuracy: 0.1623\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  0.0899\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  0.0899\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  0.0900\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  0.0898\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  0.0899\n",
            "End of Epoch: 5 Train Accuracy: 0.1634 Val Loss: 0.0899 Val Accuracy: 0.1657\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  0.0900\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  0.0900\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  0.0899\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  0.0899\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  0.0900\n",
            "End of Epoch: 6 Train Accuracy: 0.1670 Val Loss: 0.0899 Val Accuracy: 0.1702\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  0.0900\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  0.0898\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  0.0900\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  0.0900\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  0.0900\n",
            "End of Epoch: 7 Train Accuracy: 0.1707 Val Loss: 0.0899 Val Accuracy: 0.1748\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  0.0900\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  0.0899\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  0.0900\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  0.0900\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  0.0900\n",
            "End of Epoch: 8 Train Accuracy: 0.1745 Val Loss: 0.0900 Val Accuracy: 0.1792\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  0.0899\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  0.0900\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  0.0899\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  0.0900\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  0.0899\n",
            "End of Epoch: 9 Train Accuracy: 0.1778 Val Loss: 0.0900 Val Accuracy: 0.1830\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  0.0900\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  0.0900\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  0.0900\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  0.0899\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  0.0899\n",
            "End of Epoch: 10 Train Accuracy: 0.1813 Val Loss: 0.0900 Val Accuracy: 0.1857\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▄▄▅▆▆▇▇█</td></tr><tr><td>Train Loss</td><td>▄▁▇█▁▅▄▄▄▄</td></tr><tr><td>Val Accuracy</td><td>▁▂▃▃▄▅▅▆▇██</td></tr><tr><td>Val Loss</td><td>█▅▃▂▁▁▁▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.1813</td></tr><tr><td>Train Loss</td><td>0.08992</td></tr><tr><td>Val Accuracy</td><td>0.18567</td></tr><tr><td>Val Loss</td><td>0.08996</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_sgd|loss_mse|lr=0.0001|batch_64|act_relu|hid_3|neurons_128|nrns_10|init_Xavier549</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/gvl34kcj' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/gvl34kcj</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_184515-gvl34kcj/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yphnlwsn with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nag\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_184641-yphnlwsn</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/yphnlwsn' target=\"_blank\">worthy-sweep-90</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/yphnlwsn' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/yphnlwsn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0595 Val Loss: 25.8352 Val Accuracy: 0.0648\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  25.9041\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-4f78d507ffd0>:118: RuntimeWarning: invalid value encountered in subtract\n",
            "  exp_y = np.exp(y - np.max(y, axis=1, keepdims=True))\n",
            "<ipython-input-2-4f78d507ffd0>:36: RuntimeWarning: invalid value encountered in matmul\n",
            "  dzdx = d_out@self.weight.T\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  nan\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  nan\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  nan\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  nan\n",
            "End of Epoch: 1 Train Accuracy: 0.1003 Val Loss: nan Val Accuracy: 0.0977\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  nan\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  nan\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  nan\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  nan\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  nan\n",
            "End of Epoch: 2 Train Accuracy: 0.1003 Val Loss: nan Val Accuracy: 0.0977\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  nan\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  nan\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  nan\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  nan\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  nan\n",
            "End of Epoch: 3 Train Accuracy: 0.1003 Val Loss: nan Val Accuracy: 0.0977\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  nan\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  nan\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  nan\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  nan\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  nan\n",
            "End of Epoch: 4 Train Accuracy: 0.1003 Val Loss: nan Val Accuracy: 0.0977\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  nan\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  nan\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  nan\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  nan\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  nan\n",
            "End of Epoch: 5 Train Accuracy: 0.1003 Val Loss: nan Val Accuracy: 0.0977\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  nan\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  nan\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  nan\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  nan\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  nan\n",
            "End of Epoch: 6 Train Accuracy: 0.1003 Val Loss: nan Val Accuracy: 0.0977\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  nan\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  nan\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  nan\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  nan\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  nan\n",
            "End of Epoch: 7 Train Accuracy: 0.1003 Val Loss: nan Val Accuracy: 0.0977\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  nan\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  nan\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  nan\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  nan\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  nan\n",
            "End of Epoch: 8 Train Accuracy: 0.1003 Val Loss: nan Val Accuracy: 0.0977\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  nan\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  nan\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  nan\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  nan\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  nan\n",
            "End of Epoch: 9 Train Accuracy: 0.1003 Val Loss: nan Val Accuracy: 0.0977\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  nan\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  nan\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  nan\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  nan\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  nan\n",
            "End of Epoch: 10 Train Accuracy: 0.1003 Val Loss: nan Val Accuracy: 0.0977\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁██████████</td></tr><tr><td>Val Accuracy</td><td>▁██████████</td></tr><tr><td>Val Loss</td><td>▁          </td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.10026</td></tr><tr><td>Train Loss</td><td>nan</td></tr><tr><td>Val Accuracy</td><td>0.09767</td></tr><tr><td>Val Loss</td><td>nan</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nag|loss_cross_entropy|lr=0.0001|batch_64|act_relu|hid_5|neurons_32|nrns_10|init_Random712</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/yphnlwsn' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/yphnlwsn</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_184641-yphnlwsn/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 95qpt2wv with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_184717-95qpt2wv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/95qpt2wv' target=\"_blank\">young-sweep-91</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/95qpt2wv' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/95qpt2wv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.0726 Val Loss: 0.0901 Val Accuracy: 0.0690\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  0.0903\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  0.0270\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  0.0195\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  0.0204\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  0.0206\n",
            "End of Epoch: 1 Train Accuracy: 0.8537 Val Loss: 0.0218 Val Accuracy: 0.8452\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  0.0123\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  0.0294\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  0.0189\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  0.0186\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  0.0133\n",
            "End of Epoch: 2 Train Accuracy: 0.8728 Val Loss: 0.0197 Val Accuracy: 0.8632\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  0.0159\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  0.0142\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  0.0142\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  0.0200\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  0.0180\n",
            "End of Epoch: 3 Train Accuracy: 0.8675 Val Loss: 0.0205 Val Accuracy: 0.8585\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  0.0228\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  0.0174\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  0.0285\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  0.0121\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  0.0182\n",
            "End of Epoch: 4 Train Accuracy: 0.8798 Val Loss: 0.0191 Val Accuracy: 0.8660\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  0.0189\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  0.0176\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  0.0176\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  0.0184\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  0.0179\n",
            "End of Epoch: 5 Train Accuracy: 0.8880 Val Loss: 0.0182 Val Accuracy: 0.8723\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  0.0219\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  0.0275\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  0.0138\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  0.0131\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  0.0208\n",
            "End of Epoch: 6 Train Accuracy: 0.8914 Val Loss: 0.0179 Val Accuracy: 0.8733\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  0.0122\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  0.0051\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  0.0161\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  0.0226\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  0.0086\n",
            "End of Epoch: 7 Train Accuracy: 0.8937 Val Loss: 0.0177 Val Accuracy: 0.8762\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  0.0229\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  0.0138\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  0.0134\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  0.0090\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  0.0247\n",
            "End of Epoch: 8 Train Accuracy: 0.9032 Val Loss: 0.0168 Val Accuracy: 0.8825\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  0.0137\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  0.0214\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  0.0105\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  0.0144\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  0.0202\n",
            "End of Epoch: 9 Train Accuracy: 0.9063 Val Loss: 0.0163 Val Accuracy: 0.8865\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  0.0128\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  0.0178\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  0.0185\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  0.0164\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  0.0231\n",
            "End of Epoch: 10 Train Accuracy: 0.9079 Val Loss: 0.0165 Val Accuracy: 0.8855\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁██████████</td></tr><tr><td>Train Loss</td><td>█▆▇▄▃▄▅▁▃▂</td></tr><tr><td>Val Accuracy</td><td>▁██████████</td></tr><tr><td>Val Loss</td><td>█▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.90791</td></tr><tr><td>Train Loss</td><td>0.00618</td></tr><tr><td>Val Accuracy</td><td>0.8855</td></tr><tr><td>Val Loss</td><td>0.01653</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_adam|loss_mse|lr=0.001|batch_64|act_tanh|hid_4|neurons_128|nrns_10|init_Xavier519</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/95qpt2wv' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/95qpt2wv</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_184717-95qpt2wv/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jngkaup0 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_185003-jngkaup0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/jngkaup0' target=\"_blank\">solar-sweep-92</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/jngkaup0' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/jngkaup0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.1277 Val Loss: 0.1363 Val Accuracy: 0.1247\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/3375): \tTrain Loss  0.1538\n",
            "Epoch(1/5)\t Batch(200/3375): \tTrain Loss  0.0696\n",
            "Epoch(1/5)\t Batch(400/3375): \tTrain Loss  0.0421\n",
            "Epoch(1/5)\t Batch(600/3375): \tTrain Loss  0.0400\n",
            "Epoch(1/5)\t Batch(800/3375): \tTrain Loss  0.0454\n",
            "Epoch(1/5)\t Batch(1000/3375): \tTrain Loss  0.0590\n",
            "Epoch(1/5)\t Batch(1200/3375): \tTrain Loss  0.0297\n",
            "Epoch(1/5)\t Batch(1400/3375): \tTrain Loss  0.0471\n",
            "Epoch(1/5)\t Batch(1600/3375): \tTrain Loss  0.0412\n",
            "Epoch(1/5)\t Batch(1800/3375): \tTrain Loss  0.0237\n",
            "Epoch(1/5)\t Batch(2000/3375): \tTrain Loss  0.0166\n",
            "Epoch(1/5)\t Batch(2200/3375): \tTrain Loss  0.0509\n",
            "Epoch(1/5)\t Batch(2400/3375): \tTrain Loss  0.0307\n",
            "Epoch(1/5)\t Batch(2600/3375): \tTrain Loss  0.0198\n",
            "Epoch(1/5)\t Batch(2800/3375): \tTrain Loss  0.0261\n",
            "Epoch(1/5)\t Batch(3000/3375): \tTrain Loss  0.0203\n",
            "Epoch(1/5)\t Batch(3200/3375): \tTrain Loss  0.0606\n",
            "End of Epoch: 1 Train Accuracy: 0.7993 Val Loss: 0.0298 Val Accuracy: 0.7912\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/3375): \tTrain Loss  0.0257\n",
            "Epoch(2/5)\t Batch(200/3375): \tTrain Loss  0.0297\n",
            "Epoch(2/5)\t Batch(400/3375): \tTrain Loss  0.0291\n",
            "Epoch(2/5)\t Batch(600/3375): \tTrain Loss  0.0151\n",
            "Epoch(2/5)\t Batch(800/3375): \tTrain Loss  0.0116\n",
            "Epoch(2/5)\t Batch(1000/3375): \tTrain Loss  0.0204\n",
            "Epoch(2/5)\t Batch(1200/3375): \tTrain Loss  0.0345\n",
            "Epoch(2/5)\t Batch(1400/3375): \tTrain Loss  0.0328\n",
            "Epoch(2/5)\t Batch(1600/3375): \tTrain Loss  0.0284\n",
            "Epoch(2/5)\t Batch(1800/3375): \tTrain Loss  0.0265\n",
            "Epoch(2/5)\t Batch(2000/3375): \tTrain Loss  0.0211\n",
            "Epoch(2/5)\t Batch(2200/3375): \tTrain Loss  0.0571\n",
            "Epoch(2/5)\t Batch(2400/3375): \tTrain Loss  0.0279\n",
            "Epoch(2/5)\t Batch(2600/3375): \tTrain Loss  0.0188\n",
            "Epoch(2/5)\t Batch(2800/3375): \tTrain Loss  0.0332\n",
            "Epoch(2/5)\t Batch(3000/3375): \tTrain Loss  0.0383\n",
            "Epoch(2/5)\t Batch(3200/3375): \tTrain Loss  0.0276\n",
            "End of Epoch: 2 Train Accuracy: 0.8193 Val Loss: 0.0266 Val Accuracy: 0.8155\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/3375): \tTrain Loss  0.0159\n",
            "Epoch(3/5)\t Batch(200/3375): \tTrain Loss  0.0209\n",
            "Epoch(3/5)\t Batch(400/3375): \tTrain Loss  0.0318\n",
            "Epoch(3/5)\t Batch(600/3375): \tTrain Loss  0.0231\n",
            "Epoch(3/5)\t Batch(800/3375): \tTrain Loss  0.0031\n",
            "Epoch(3/5)\t Batch(1000/3375): \tTrain Loss  0.0171\n",
            "Epoch(3/5)\t Batch(1200/3375): \tTrain Loss  0.0126\n",
            "Epoch(3/5)\t Batch(1400/3375): \tTrain Loss  0.0191\n",
            "Epoch(3/5)\t Batch(1600/3375): \tTrain Loss  0.0418\n",
            "Epoch(3/5)\t Batch(1800/3375): \tTrain Loss  0.0069\n",
            "Epoch(3/5)\t Batch(2000/3375): \tTrain Loss  0.0204\n",
            "Epoch(3/5)\t Batch(2200/3375): \tTrain Loss  0.0256\n",
            "Epoch(3/5)\t Batch(2400/3375): \tTrain Loss  0.0262\n",
            "Epoch(3/5)\t Batch(2600/3375): \tTrain Loss  0.0548\n",
            "Epoch(3/5)\t Batch(2800/3375): \tTrain Loss  0.0097\n",
            "Epoch(3/5)\t Batch(3000/3375): \tTrain Loss  0.0158\n",
            "Epoch(3/5)\t Batch(3200/3375): \tTrain Loss  0.0147\n",
            "End of Epoch: 3 Train Accuracy: 0.8467 Val Loss: 0.0237 Val Accuracy: 0.8378\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/3375): \tTrain Loss  0.0334\n",
            "Epoch(4/5)\t Batch(200/3375): \tTrain Loss  0.0316\n",
            "Epoch(4/5)\t Batch(400/3375): \tTrain Loss  0.0114\n",
            "Epoch(4/5)\t Batch(600/3375): \tTrain Loss  0.0221\n",
            "Epoch(4/5)\t Batch(800/3375): \tTrain Loss  0.0315\n",
            "Epoch(4/5)\t Batch(1000/3375): \tTrain Loss  0.0230\n",
            "Epoch(4/5)\t Batch(1200/3375): \tTrain Loss  0.0238\n",
            "Epoch(4/5)\t Batch(1400/3375): \tTrain Loss  0.0125\n",
            "Epoch(4/5)\t Batch(1600/3375): \tTrain Loss  0.0165\n",
            "Epoch(4/5)\t Batch(1800/3375): \tTrain Loss  0.0341\n",
            "Epoch(4/5)\t Batch(2000/3375): \tTrain Loss  0.0388\n",
            "Epoch(4/5)\t Batch(2200/3375): \tTrain Loss  0.0349\n",
            "Epoch(4/5)\t Batch(2400/3375): \tTrain Loss  0.0184\n",
            "Epoch(4/5)\t Batch(2600/3375): \tTrain Loss  0.0362\n",
            "Epoch(4/5)\t Batch(2800/3375): \tTrain Loss  0.0281\n",
            "Epoch(4/5)\t Batch(3000/3375): \tTrain Loss  0.0345\n",
            "Epoch(4/5)\t Batch(3200/3375): \tTrain Loss  0.0019\n",
            "End of Epoch: 4 Train Accuracy: 0.8489 Val Loss: 0.0238 Val Accuracy: 0.8338\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/3375): \tTrain Loss  0.0179\n",
            "Epoch(5/5)\t Batch(200/3375): \tTrain Loss  0.0236\n",
            "Epoch(5/5)\t Batch(400/3375): \tTrain Loss  0.0175\n",
            "Epoch(5/5)\t Batch(600/3375): \tTrain Loss  0.0127\n",
            "Epoch(5/5)\t Batch(800/3375): \tTrain Loss  0.0113\n",
            "Epoch(5/5)\t Batch(1000/3375): \tTrain Loss  0.0293\n",
            "Epoch(5/5)\t Batch(1200/3375): \tTrain Loss  0.0185\n",
            "Epoch(5/5)\t Batch(1400/3375): \tTrain Loss  0.0119\n",
            "Epoch(5/5)\t Batch(1600/3375): \tTrain Loss  0.0337\n",
            "Epoch(5/5)\t Batch(1800/3375): \tTrain Loss  0.0263\n",
            "Epoch(5/5)\t Batch(2000/3375): \tTrain Loss  0.0264\n",
            "Epoch(5/5)\t Batch(2200/3375): \tTrain Loss  0.0166\n",
            "Epoch(5/5)\t Batch(2400/3375): \tTrain Loss  0.0164\n",
            "Epoch(5/5)\t Batch(2600/3375): \tTrain Loss  0.0203\n",
            "Epoch(5/5)\t Batch(2800/3375): \tTrain Loss  0.0028\n",
            "Epoch(5/5)\t Batch(3000/3375): \tTrain Loss  0.0270\n",
            "Epoch(5/5)\t Batch(3200/3375): \tTrain Loss  0.0108\n",
            "End of Epoch: 5 Train Accuracy: 0.8576 Val Loss: 0.0224 Val Accuracy: 0.8438\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▇████</td></tr><tr><td>Train Loss</td><td>█▁▅▆▃</td></tr><tr><td>Val Accuracy</td><td>▁▇████</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.85759</td></tr><tr><td>Train Loss</td><td>0.02621</td></tr><tr><td>Val Accuracy</td><td>0.84383</td></tr><tr><td>Val Loss</td><td>0.02244</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_mse|lr=0.001|batch_16|act_sigmoid|hid_5|neurons_64|nrns_5|init_Random842</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/jngkaup0' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/jngkaup0</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_185003-jngkaup0/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ddug1900 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_185119-ddug1900</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ddug1900' target=\"_blank\">whole-sweep-93</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ddug1900' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ddug1900</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.1133 Val Loss: 0.1653 Val Accuracy: 0.1085\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/3375): \tTrain Loss  0.1636\n",
            "Epoch(1/10)\t Batch(200/3375): \tTrain Loss  0.1342\n",
            "Epoch(1/10)\t Batch(400/3375): \tTrain Loss  0.1591\n",
            "Epoch(1/10)\t Batch(600/3375): \tTrain Loss  0.1708\n",
            "Epoch(1/10)\t Batch(800/3375): \tTrain Loss  0.1697\n",
            "Epoch(1/10)\t Batch(1000/3375): \tTrain Loss  0.1732\n",
            "Epoch(1/10)\t Batch(1200/3375): \tTrain Loss  0.1644\n",
            "Epoch(1/10)\t Batch(1400/3375): \tTrain Loss  0.1581\n",
            "Epoch(1/10)\t Batch(1600/3375): \tTrain Loss  0.1562\n",
            "Epoch(1/10)\t Batch(1800/3375): \tTrain Loss  0.1259\n",
            "Epoch(1/10)\t Batch(2000/3375): \tTrain Loss  0.1818\n",
            "Epoch(1/10)\t Batch(2200/3375): \tTrain Loss  0.1902\n",
            "Epoch(1/10)\t Batch(2400/3375): \tTrain Loss  0.1699\n",
            "Epoch(1/10)\t Batch(2600/3375): \tTrain Loss  0.1622\n",
            "Epoch(1/10)\t Batch(2800/3375): \tTrain Loss  0.1785\n",
            "Epoch(1/10)\t Batch(3000/3375): \tTrain Loss  0.1425\n",
            "Epoch(1/10)\t Batch(3200/3375): \tTrain Loss  0.1425\n",
            "End of Epoch: 1 Train Accuracy: 0.1174 Val Loss: 0.1642 Val Accuracy: 0.1125\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/3375): \tTrain Loss  0.1569\n",
            "Epoch(2/10)\t Batch(200/3375): \tTrain Loss  0.1774\n",
            "Epoch(2/10)\t Batch(400/3375): \tTrain Loss  0.1696\n",
            "Epoch(2/10)\t Batch(600/3375): \tTrain Loss  0.1588\n",
            "Epoch(2/10)\t Batch(800/3375): \tTrain Loss  0.1486\n",
            "Epoch(2/10)\t Batch(1000/3375): \tTrain Loss  0.1153\n",
            "Epoch(2/10)\t Batch(1200/3375): \tTrain Loss  0.1741\n",
            "Epoch(2/10)\t Batch(1400/3375): \tTrain Loss  0.1472\n",
            "Epoch(2/10)\t Batch(1600/3375): \tTrain Loss  0.1295\n",
            "Epoch(2/10)\t Batch(1800/3375): \tTrain Loss  0.1613\n",
            "Epoch(2/10)\t Batch(2000/3375): \tTrain Loss  0.1749\n",
            "Epoch(2/10)\t Batch(2200/3375): \tTrain Loss  0.1218\n",
            "Epoch(2/10)\t Batch(2400/3375): \tTrain Loss  0.1574\n",
            "Epoch(2/10)\t Batch(2600/3375): \tTrain Loss  0.1562\n",
            "Epoch(2/10)\t Batch(2800/3375): \tTrain Loss  0.1604\n",
            "Epoch(2/10)\t Batch(3000/3375): \tTrain Loss  0.1755\n",
            "Epoch(2/10)\t Batch(3200/3375): \tTrain Loss  0.1528\n",
            "End of Epoch: 2 Train Accuracy: 0.1213 Val Loss: 0.1632 Val Accuracy: 0.1158\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/3375): \tTrain Loss  0.1287\n",
            "Epoch(3/10)\t Batch(200/3375): \tTrain Loss  0.1587\n",
            "Epoch(3/10)\t Batch(400/3375): \tTrain Loss  0.1665\n",
            "Epoch(3/10)\t Batch(600/3375): \tTrain Loss  0.1701\n",
            "Epoch(3/10)\t Batch(800/3375): \tTrain Loss  0.1411\n",
            "Epoch(3/10)\t Batch(1000/3375): \tTrain Loss  0.1504\n",
            "Epoch(3/10)\t Batch(1200/3375): \tTrain Loss  0.1632\n",
            "Epoch(3/10)\t Batch(1400/3375): \tTrain Loss  0.1745\n",
            "Epoch(3/10)\t Batch(1600/3375): \tTrain Loss  0.1533\n",
            "Epoch(3/10)\t Batch(1800/3375): \tTrain Loss  0.1776\n",
            "Epoch(3/10)\t Batch(2000/3375): \tTrain Loss  0.1508\n",
            "Epoch(3/10)\t Batch(2200/3375): \tTrain Loss  0.1756\n",
            "Epoch(3/10)\t Batch(2400/3375): \tTrain Loss  0.1235\n",
            "Epoch(3/10)\t Batch(2600/3375): \tTrain Loss  0.1628\n",
            "Epoch(3/10)\t Batch(2800/3375): \tTrain Loss  0.1286\n",
            "Epoch(3/10)\t Batch(3000/3375): \tTrain Loss  0.1683\n",
            "Epoch(3/10)\t Batch(3200/3375): \tTrain Loss  0.1663\n",
            "End of Epoch: 3 Train Accuracy: 0.1248 Val Loss: 0.1634 Val Accuracy: 0.1120\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/3375): \tTrain Loss  0.1656\n",
            "Epoch(4/10)\t Batch(200/3375): \tTrain Loss  0.1609\n",
            "Epoch(4/10)\t Batch(400/3375): \tTrain Loss  0.1632\n",
            "Epoch(4/10)\t Batch(600/3375): \tTrain Loss  0.1719\n",
            "Epoch(4/10)\t Batch(800/3375): \tTrain Loss  0.1625\n",
            "Epoch(4/10)\t Batch(1000/3375): \tTrain Loss  0.1736\n",
            "Epoch(4/10)\t Batch(1200/3375): \tTrain Loss  0.1188\n",
            "Epoch(4/10)\t Batch(1400/3375): \tTrain Loss  0.1886\n",
            "Epoch(4/10)\t Batch(1600/3375): \tTrain Loss  0.1771\n",
            "Epoch(4/10)\t Batch(1800/3375): \tTrain Loss  0.1683\n",
            "Epoch(4/10)\t Batch(2000/3375): \tTrain Loss  0.1552\n",
            "Epoch(4/10)\t Batch(2200/3375): \tTrain Loss  0.1665\n",
            "Epoch(4/10)\t Batch(2400/3375): \tTrain Loss  0.1155\n",
            "Epoch(4/10)\t Batch(2600/3375): \tTrain Loss  0.1766\n",
            "Epoch(4/10)\t Batch(2800/3375): \tTrain Loss  0.1660\n",
            "Epoch(4/10)\t Batch(3000/3375): \tTrain Loss  0.1388\n",
            "Epoch(4/10)\t Batch(3200/3375): \tTrain Loss  0.1576\n",
            "End of Epoch: 4 Train Accuracy: 0.1277 Val Loss: 0.1637 Val Accuracy: 0.1155\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/3375): \tTrain Loss  0.1707\n",
            "Epoch(5/10)\t Batch(200/3375): \tTrain Loss  0.1641\n",
            "Epoch(5/10)\t Batch(400/3375): \tTrain Loss  0.1935\n",
            "Epoch(5/10)\t Batch(600/3375): \tTrain Loss  0.1745\n",
            "Epoch(5/10)\t Batch(800/3375): \tTrain Loss  0.1502\n",
            "Epoch(5/10)\t Batch(1000/3375): \tTrain Loss  0.1589\n",
            "Epoch(5/10)\t Batch(1200/3375): \tTrain Loss  0.1796\n",
            "Epoch(5/10)\t Batch(1400/3375): \tTrain Loss  0.1814\n",
            "Epoch(5/10)\t Batch(1600/3375): \tTrain Loss  0.1300\n",
            "Epoch(5/10)\t Batch(1800/3375): \tTrain Loss  0.1691\n",
            "Epoch(5/10)\t Batch(2000/3375): \tTrain Loss  0.1469\n",
            "Epoch(5/10)\t Batch(2200/3375): \tTrain Loss  0.1616\n",
            "Epoch(5/10)\t Batch(2400/3375): \tTrain Loss  0.1518\n",
            "Epoch(5/10)\t Batch(2600/3375): \tTrain Loss  0.1522\n",
            "Epoch(5/10)\t Batch(2800/3375): \tTrain Loss  0.1489\n",
            "Epoch(5/10)\t Batch(3000/3375): \tTrain Loss  0.1547\n",
            "Epoch(5/10)\t Batch(3200/3375): \tTrain Loss  0.1490\n",
            "End of Epoch: 5 Train Accuracy: 0.1291 Val Loss: 0.1630 Val Accuracy: 0.1202\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/3375): \tTrain Loss  0.1460\n",
            "Epoch(6/10)\t Batch(200/3375): \tTrain Loss  0.1611\n",
            "Epoch(6/10)\t Batch(400/3375): \tTrain Loss  0.1583\n",
            "Epoch(6/10)\t Batch(600/3375): \tTrain Loss  0.1633\n",
            "Epoch(6/10)\t Batch(800/3375): \tTrain Loss  0.1595\n",
            "Epoch(6/10)\t Batch(1000/3375): \tTrain Loss  0.1711\n",
            "Epoch(6/10)\t Batch(1200/3375): \tTrain Loss  0.1908\n",
            "Epoch(6/10)\t Batch(1400/3375): \tTrain Loss  0.1833\n",
            "Epoch(6/10)\t Batch(1600/3375): \tTrain Loss  0.1367\n",
            "Epoch(6/10)\t Batch(1800/3375): \tTrain Loss  0.1696\n",
            "Epoch(6/10)\t Batch(2000/3375): \tTrain Loss  0.1612\n",
            "Epoch(6/10)\t Batch(2200/3375): \tTrain Loss  0.1518\n",
            "Epoch(6/10)\t Batch(2400/3375): \tTrain Loss  0.1806\n",
            "Epoch(6/10)\t Batch(2600/3375): \tTrain Loss  0.1759\n",
            "Epoch(6/10)\t Batch(2800/3375): \tTrain Loss  0.1381\n",
            "Epoch(6/10)\t Batch(3000/3375): \tTrain Loss  0.1671\n",
            "Epoch(6/10)\t Batch(3200/3375): \tTrain Loss  0.1596\n",
            "End of Epoch: 6 Train Accuracy: 0.1354 Val Loss: 0.1608 Val Accuracy: 0.1262\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/3375): \tTrain Loss  0.1726\n",
            "Epoch(7/10)\t Batch(200/3375): \tTrain Loss  0.1555\n",
            "Epoch(7/10)\t Batch(400/3375): \tTrain Loss  0.1105\n",
            "Epoch(7/10)\t Batch(600/3375): \tTrain Loss  0.1649\n",
            "Epoch(7/10)\t Batch(800/3375): \tTrain Loss  0.1647\n",
            "Epoch(7/10)\t Batch(1000/3375): \tTrain Loss  0.1873\n",
            "Epoch(7/10)\t Batch(1200/3375): \tTrain Loss  0.1629\n",
            "Epoch(7/10)\t Batch(1400/3375): \tTrain Loss  0.1897\n",
            "Epoch(7/10)\t Batch(1600/3375): \tTrain Loss  0.1536\n",
            "Epoch(7/10)\t Batch(1800/3375): \tTrain Loss  0.1764\n",
            "Epoch(7/10)\t Batch(2000/3375): \tTrain Loss  0.1692\n",
            "Epoch(7/10)\t Batch(2200/3375): \tTrain Loss  0.1443\n",
            "Epoch(7/10)\t Batch(2400/3375): \tTrain Loss  0.1641\n",
            "Epoch(7/10)\t Batch(2600/3375): \tTrain Loss  0.1730\n",
            "Epoch(7/10)\t Batch(2800/3375): \tTrain Loss  0.1603\n",
            "Epoch(7/10)\t Batch(3000/3375): \tTrain Loss  0.1962\n",
            "Epoch(7/10)\t Batch(3200/3375): \tTrain Loss  0.1576\n",
            "End of Epoch: 7 Train Accuracy: 0.1346 Val Loss: 0.1606 Val Accuracy: 0.1295\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/3375): \tTrain Loss  0.1578\n",
            "Epoch(8/10)\t Batch(200/3375): \tTrain Loss  0.1724\n",
            "Epoch(8/10)\t Batch(400/3375): \tTrain Loss  0.1463\n",
            "Epoch(8/10)\t Batch(600/3375): \tTrain Loss  0.1619\n",
            "Epoch(8/10)\t Batch(800/3375): \tTrain Loss  0.1698\n",
            "Epoch(8/10)\t Batch(1000/3375): \tTrain Loss  0.1128\n",
            "Epoch(8/10)\t Batch(1200/3375): \tTrain Loss  0.1622\n",
            "Epoch(8/10)\t Batch(1400/3375): \tTrain Loss  0.1640\n",
            "Epoch(8/10)\t Batch(1600/3375): \tTrain Loss  0.1509\n",
            "Epoch(8/10)\t Batch(1800/3375): \tTrain Loss  0.1549\n",
            "Epoch(8/10)\t Batch(2000/3375): \tTrain Loss  0.1497\n",
            "Epoch(8/10)\t Batch(2200/3375): \tTrain Loss  0.1503\n",
            "Epoch(8/10)\t Batch(2400/3375): \tTrain Loss  0.1464\n",
            "Epoch(8/10)\t Batch(2600/3375): \tTrain Loss  0.1064\n",
            "Epoch(8/10)\t Batch(2800/3375): \tTrain Loss  0.1262\n",
            "Epoch(8/10)\t Batch(3000/3375): \tTrain Loss  0.1477\n",
            "Epoch(8/10)\t Batch(3200/3375): \tTrain Loss  0.1512\n",
            "End of Epoch: 8 Train Accuracy: 0.1384 Val Loss: 0.1602 Val Accuracy: 0.1297\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/3375): \tTrain Loss  0.1507\n",
            "Epoch(9/10)\t Batch(200/3375): \tTrain Loss  0.1701\n",
            "Epoch(9/10)\t Batch(400/3375): \tTrain Loss  0.1688\n",
            "Epoch(9/10)\t Batch(600/3375): \tTrain Loss  0.1373\n",
            "Epoch(9/10)\t Batch(800/3375): \tTrain Loss  0.1634\n",
            "Epoch(9/10)\t Batch(1000/3375): \tTrain Loss  0.1606\n",
            "Epoch(9/10)\t Batch(1200/3375): \tTrain Loss  0.1842\n",
            "Epoch(9/10)\t Batch(1400/3375): \tTrain Loss  0.1378\n",
            "Epoch(9/10)\t Batch(1600/3375): \tTrain Loss  0.1441\n",
            "Epoch(9/10)\t Batch(1800/3375): \tTrain Loss  0.1665\n",
            "Epoch(9/10)\t Batch(2000/3375): \tTrain Loss  0.1297\n",
            "Epoch(9/10)\t Batch(2200/3375): \tTrain Loss  0.1942\n",
            "Epoch(9/10)\t Batch(2400/3375): \tTrain Loss  0.1819\n",
            "Epoch(9/10)\t Batch(2600/3375): \tTrain Loss  0.1921\n",
            "Epoch(9/10)\t Batch(2800/3375): \tTrain Loss  0.1701\n",
            "Epoch(9/10)\t Batch(3000/3375): \tTrain Loss  0.1652\n",
            "Epoch(9/10)\t Batch(3200/3375): \tTrain Loss  0.1518\n",
            "End of Epoch: 9 Train Accuracy: 0.1416 Val Loss: 0.1601 Val Accuracy: 0.1338\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/3375): \tTrain Loss  0.1252\n",
            "Epoch(10/10)\t Batch(200/3375): \tTrain Loss  0.1513\n",
            "Epoch(10/10)\t Batch(400/3375): \tTrain Loss  0.1559\n",
            "Epoch(10/10)\t Batch(600/3375): \tTrain Loss  0.1691\n",
            "Epoch(10/10)\t Batch(800/3375): \tTrain Loss  0.1518\n",
            "Epoch(10/10)\t Batch(1000/3375): \tTrain Loss  0.1875\n",
            "Epoch(10/10)\t Batch(1200/3375): \tTrain Loss  0.1781\n",
            "Epoch(10/10)\t Batch(1400/3375): \tTrain Loss  0.1647\n",
            "Epoch(10/10)\t Batch(1600/3375): \tTrain Loss  0.1298\n",
            "Epoch(10/10)\t Batch(1800/3375): \tTrain Loss  0.1785\n",
            "Epoch(10/10)\t Batch(2000/3375): \tTrain Loss  0.1372\n",
            "Epoch(10/10)\t Batch(2200/3375): \tTrain Loss  0.1740\n",
            "Epoch(10/10)\t Batch(2400/3375): \tTrain Loss  0.1824\n",
            "Epoch(10/10)\t Batch(2600/3375): \tTrain Loss  0.1645\n",
            "Epoch(10/10)\t Batch(2800/3375): \tTrain Loss  0.1381\n",
            "Epoch(10/10)\t Batch(3000/3375): \tTrain Loss  0.1582\n",
            "Epoch(10/10)\t Batch(3200/3375): \tTrain Loss  0.1217\n",
            "End of Epoch: 10 Train Accuracy: 0.1429 Val Loss: 0.1605 Val Accuracy: 0.1305\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▄▄▅▆▆▇██</td></tr><tr><td>Train Loss</td><td>▃▅▅▃▇▆▁▄█▇</td></tr><tr><td>Val Accuracy</td><td>▁▂▃▂▃▄▆▇▇█▇</td></tr><tr><td>Val Loss</td><td>█▇▅▅▆▅▂▂▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.14285</td></tr><tr><td>Train Loss</td><td>0.1752</td></tr><tr><td>Val Accuracy</td><td>0.1305</td></tr><tr><td>Val Loss</td><td>0.1605</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_sgd|loss_mse|lr=0.0001|batch_16|act_tanh|hid_5|neurons_128|nrns_10|init_Random827</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ddug1900' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ddug1900</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_185119-ddug1900/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ohpwpi0o with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_185504-ohpwpi0o</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ohpwpi0o' target=\"_blank\">fine-sweep-94</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ohpwpi0o' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ohpwpi0o</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.0899 Val Loss: 2.2879 Val Accuracy: 0.0893\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/843): \tTrain Loss  2.2795\n",
            "Epoch(1/5)\t Batch(200/843): \tTrain Loss  1.8249\n",
            "Epoch(1/5)\t Batch(400/843): \tTrain Loss  1.8473\n",
            "Epoch(1/5)\t Batch(600/843): \tTrain Loss  1.8511\n",
            "Epoch(1/5)\t Batch(800/843): \tTrain Loss  1.8417\n",
            "End of Epoch: 1 Train Accuracy: 0.8050 Val Loss: 1.8219 Val Accuracy: 0.8028\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/843): \tTrain Loss  1.8621\n",
            "Epoch(2/5)\t Batch(200/843): \tTrain Loss  1.8789\n",
            "Epoch(2/5)\t Batch(400/843): \tTrain Loss  1.8127\n",
            "Epoch(2/5)\t Batch(600/843): \tTrain Loss  1.8036\n",
            "Epoch(2/5)\t Batch(800/843): \tTrain Loss  1.7424\n",
            "End of Epoch: 2 Train Accuracy: 0.8168 Val Loss: 1.7913 Val Accuracy: 0.8102\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/843): \tTrain Loss  1.7684\n",
            "Epoch(3/5)\t Batch(200/843): \tTrain Loss  1.7382\n",
            "Epoch(3/5)\t Batch(400/843): \tTrain Loss  1.8390\n",
            "Epoch(3/5)\t Batch(600/843): \tTrain Loss  1.7308\n",
            "Epoch(3/5)\t Batch(800/843): \tTrain Loss  1.7722\n",
            "End of Epoch: 3 Train Accuracy: 0.8241 Val Loss: 1.7767 Val Accuracy: 0.8202\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/843): \tTrain Loss  1.7953\n",
            "Epoch(4/5)\t Batch(200/843): \tTrain Loss  1.7123\n",
            "Epoch(4/5)\t Batch(400/843): \tTrain Loss  1.7455\n",
            "Epoch(4/5)\t Batch(600/843): \tTrain Loss  1.7312\n",
            "Epoch(4/5)\t Batch(800/843): \tTrain Loss  1.7940\n",
            "End of Epoch: 4 Train Accuracy: 0.8290 Val Loss: 1.7682 Val Accuracy: 0.8262\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/843): \tTrain Loss  1.7666\n",
            "Epoch(5/5)\t Batch(200/843): \tTrain Loss  1.7047\n",
            "Epoch(5/5)\t Batch(400/843): \tTrain Loss  1.7234\n",
            "Epoch(5/5)\t Batch(600/843): \tTrain Loss  1.7234\n",
            "Epoch(5/5)\t Batch(800/843): \tTrain Loss  1.7812\n",
            "End of Epoch: 5 Train Accuracy: 0.8308 Val Loss: 1.7609 Val Accuracy: 0.8293\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁█████</td></tr><tr><td>Train Loss</td><td>██▆▁▅</td></tr><tr><td>Val Accuracy</td><td>▁█████</td></tr><tr><td>Val Loss</td><td>█▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.83076</td></tr><tr><td>Train Loss</td><td>1.7721</td></tr><tr><td>Val Accuracy</td><td>0.82933</td></tr><tr><td>Val Loss</td><td>1.76088</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_adam|loss_cross_entropy|lr=0.0001|batch_64|act_tanh|hid_3|neurons_128|nrns_5|init_Xavier928</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ohpwpi0o' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ohpwpi0o</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_185504-ohpwpi0o/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bsstast5 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_185611-bsstast5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bsstast5' target=\"_blank\">cosmic-sweep-95</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bsstast5' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bsstast5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0957 Val Loss: 0.0900 Val Accuracy: 0.0913\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/1687): \tTrain Loss  0.0900\n",
            "Epoch(1/5)\t Batch(200/1687): \tTrain Loss  0.0292\n",
            "Epoch(1/5)\t Batch(400/1687): \tTrain Loss  0.0274\n",
            "Epoch(1/5)\t Batch(600/1687): \tTrain Loss  0.0227\n",
            "Epoch(1/5)\t Batch(800/1687): \tTrain Loss  0.0340\n",
            "Epoch(1/5)\t Batch(1000/1687): \tTrain Loss  0.0274\n",
            "Epoch(1/5)\t Batch(1200/1687): \tTrain Loss  0.0324\n",
            "Epoch(1/5)\t Batch(1400/1687): \tTrain Loss  0.0259\n",
            "Epoch(1/5)\t Batch(1600/1687): \tTrain Loss  0.0230\n",
            "End of Epoch: 1 Train Accuracy: 0.8317 Val Loss: 0.0252 Val Accuracy: 0.8222\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/1687): \tTrain Loss  0.0149\n",
            "Epoch(2/5)\t Batch(200/1687): \tTrain Loss  0.0139\n",
            "Epoch(2/5)\t Batch(400/1687): \tTrain Loss  0.0258\n",
            "Epoch(2/5)\t Batch(600/1687): \tTrain Loss  0.0279\n",
            "Epoch(2/5)\t Batch(800/1687): \tTrain Loss  0.0156\n",
            "Epoch(2/5)\t Batch(1000/1687): \tTrain Loss  0.0113\n",
            "Epoch(2/5)\t Batch(1200/1687): \tTrain Loss  0.0235\n",
            "Epoch(2/5)\t Batch(1400/1687): \tTrain Loss  0.0168\n",
            "Epoch(2/5)\t Batch(1600/1687): \tTrain Loss  0.0392\n",
            "End of Epoch: 2 Train Accuracy: 0.8507 Val Loss: 0.0220 Val Accuracy: 0.8540\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/1687): \tTrain Loss  0.0135\n",
            "Epoch(3/5)\t Batch(200/1687): \tTrain Loss  0.0213\n",
            "Epoch(3/5)\t Batch(400/1687): \tTrain Loss  0.0211\n",
            "Epoch(3/5)\t Batch(600/1687): \tTrain Loss  0.0097\n",
            "Epoch(3/5)\t Batch(800/1687): \tTrain Loss  0.0216\n",
            "Epoch(3/5)\t Batch(1000/1687): \tTrain Loss  0.0360\n",
            "Epoch(3/5)\t Batch(1200/1687): \tTrain Loss  0.0229\n",
            "Epoch(3/5)\t Batch(1400/1687): \tTrain Loss  0.0151\n",
            "Epoch(3/5)\t Batch(1600/1687): \tTrain Loss  0.0344\n",
            "End of Epoch: 3 Train Accuracy: 0.8607 Val Loss: 0.0211 Val Accuracy: 0.8547\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/1687): \tTrain Loss  0.0270\n",
            "Epoch(4/5)\t Batch(200/1687): \tTrain Loss  0.0068\n",
            "Epoch(4/5)\t Batch(400/1687): \tTrain Loss  0.0230\n",
            "Epoch(4/5)\t Batch(600/1687): \tTrain Loss  0.0277\n",
            "Epoch(4/5)\t Batch(800/1687): \tTrain Loss  0.0115\n",
            "Epoch(4/5)\t Batch(1000/1687): \tTrain Loss  0.0250\n",
            "Epoch(4/5)\t Batch(1200/1687): \tTrain Loss  0.0087\n",
            "Epoch(4/5)\t Batch(1400/1687): \tTrain Loss  0.0297\n",
            "Epoch(4/5)\t Batch(1600/1687): \tTrain Loss  0.0200\n",
            "End of Epoch: 4 Train Accuracy: 0.8564 Val Loss: 0.0217 Val Accuracy: 0.8483\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/1687): \tTrain Loss  0.0141\n",
            "Epoch(5/5)\t Batch(200/1687): \tTrain Loss  0.0191\n",
            "Epoch(5/5)\t Batch(400/1687): \tTrain Loss  0.0114\n",
            "Epoch(5/5)\t Batch(600/1687): \tTrain Loss  0.0348\n",
            "Epoch(5/5)\t Batch(800/1687): \tTrain Loss  0.0345\n",
            "Epoch(5/5)\t Batch(1000/1687): \tTrain Loss  0.0179\n",
            "Epoch(5/5)\t Batch(1200/1687): \tTrain Loss  0.0062\n",
            "Epoch(5/5)\t Batch(1400/1687): \tTrain Loss  0.0194\n",
            "Epoch(5/5)\t Batch(1600/1687): \tTrain Loss  0.0157\n",
            "End of Epoch: 5 Train Accuracy: 0.8477 Val Loss: 0.0239 Val Accuracy: 0.8373\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁█████</td></tr><tr><td>Train Loss</td><td>▃▅█▃▁</td></tr><tr><td>Val Accuracy</td><td>▁█████</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.84772</td></tr><tr><td>Train Loss</td><td>0.00533</td></tr><tr><td>Val Accuracy</td><td>0.83733</td></tr><tr><td>Val Loss</td><td>0.02394</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_mse|lr=0.001|batch_32|act_relu|hid_4|neurons_32|nrns_5|init_Xavier992</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bsstast5' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/bsstast5</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_185611-bsstast5/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mtz4idmp with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_185651-mtz4idmp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/mtz4idmp' target=\"_blank\">exalted-sweep-96</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/mtz4idmp' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/mtz4idmp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0520 Val Loss: 2.3072 Val Accuracy: 0.0522\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  2.3008\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  1.7593\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  1.7570\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  1.6540\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  1.7317\n",
            "End of Epoch: 1 Train Accuracy: 0.8454 Val Loss: 1.7128 Val Accuracy: 0.8445\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  1.7125\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  1.6822\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  1.7212\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  1.6801\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  1.6226\n",
            "End of Epoch: 2 Train Accuracy: 0.8579 Val Loss: 1.6801 Val Accuracy: 0.8577\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  1.6647\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  1.6359\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  1.6130\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  1.6803\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  1.6369\n",
            "End of Epoch: 3 Train Accuracy: 0.8576 Val Loss: 1.6805 Val Accuracy: 0.8525\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  1.6106\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  1.6571\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  1.5946\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  1.6152\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  1.6484\n",
            "End of Epoch: 4 Train Accuracy: 0.8774 Val Loss: 1.6390 Val Accuracy: 0.8732\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  1.6795\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  1.6110\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  1.6914\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  1.6172\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  1.6261\n",
            "End of Epoch: 5 Train Accuracy: 0.8778 Val Loss: 1.6520 Val Accuracy: 0.8737\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  1.5968\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  1.6782\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  1.6073\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  1.6758\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  1.6342\n",
            "End of Epoch: 6 Train Accuracy: 0.8886 Val Loss: 1.6256 Val Accuracy: 0.8805\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  1.6615\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  1.5693\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  1.5896\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  1.6241\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  1.5454\n",
            "End of Epoch: 7 Train Accuracy: 0.8914 Val Loss: 1.6264 Val Accuracy: 0.8827\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  1.5640\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  1.5966\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  1.6079\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  1.6079\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  1.6286\n",
            "End of Epoch: 8 Train Accuracy: 0.8830 Val Loss: 1.6432 Val Accuracy: 0.8748\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  1.6228\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  1.5957\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  1.6139\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  1.6411\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  1.6081\n",
            "End of Epoch: 9 Train Accuracy: 0.8810 Val Loss: 1.6299 Val Accuracy: 0.8757\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  1.6218\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  1.6151\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  1.5545\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  1.6455\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  1.6290\n",
            "End of Epoch: 10 Train Accuracy: 0.8883 Val Loss: 1.6340 Val Accuracy: 0.8775\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁██████████</td></tr><tr><td>Train Loss</td><td>█▆▅▁▅▂▃▄▃▃</td></tr><tr><td>Val Accuracy</td><td>▁██████████</td></tr><tr><td>Val Loss</td><td>█▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.88828</td></tr><tr><td>Train Loss</td><td>1.63921</td></tr><tr><td>Val Accuracy</td><td>0.8775</td></tr><tr><td>Val Loss</td><td>1.634</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_cross_entropy|lr=0.001|batch_64|act_relu|hid_3|neurons_32|nrns_10|init_Xavier169</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/mtz4idmp' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/mtz4idmp</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_185651-mtz4idmp/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d9csakgb with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_185726-d9csakgb</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/d9csakgb' target=\"_blank\">lively-sweep-97</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/d9csakgb' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/d9csakgb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.1366 Val Loss: 2.2975 Val Accuracy: 0.1320\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/3375): \tTrain Loss  2.2669\n",
            "Epoch(1/5)\t Batch(200/3375): \tTrain Loss  2.3034\n",
            "Epoch(1/5)\t Batch(400/3375): \tTrain Loss  2.2805\n",
            "Epoch(1/5)\t Batch(600/3375): \tTrain Loss  2.2853\n",
            "Epoch(1/5)\t Batch(800/3375): \tTrain Loss  2.2597\n",
            "Epoch(1/5)\t Batch(1000/3375): \tTrain Loss  2.2987\n",
            "Epoch(1/5)\t Batch(1200/3375): \tTrain Loss  2.2678\n",
            "Epoch(1/5)\t Batch(1400/3375): \tTrain Loss  2.2725\n",
            "Epoch(1/5)\t Batch(1600/3375): \tTrain Loss  2.2972\n",
            "Epoch(1/5)\t Batch(1800/3375): \tTrain Loss  2.2619\n",
            "Epoch(1/5)\t Batch(2000/3375): \tTrain Loss  2.2746\n",
            "Epoch(1/5)\t Batch(2200/3375): \tTrain Loss  2.2533\n",
            "Epoch(1/5)\t Batch(2400/3375): \tTrain Loss  2.2673\n",
            "Epoch(1/5)\t Batch(2600/3375): \tTrain Loss  2.2536\n",
            "Epoch(1/5)\t Batch(2800/3375): \tTrain Loss  2.2564\n",
            "Epoch(1/5)\t Batch(3000/3375): \tTrain Loss  2.2461\n",
            "Epoch(1/5)\t Batch(3200/3375): \tTrain Loss  2.2542\n",
            "End of Epoch: 1 Train Accuracy: 0.3356 Val Loss: 2.2531 Val Accuracy: 0.3335\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/3375): \tTrain Loss  2.2580\n",
            "Epoch(2/5)\t Batch(200/3375): \tTrain Loss  2.2470\n",
            "Epoch(2/5)\t Batch(400/3375): \tTrain Loss  2.2154\n",
            "Epoch(2/5)\t Batch(600/3375): \tTrain Loss  2.2477\n",
            "Epoch(2/5)\t Batch(800/3375): \tTrain Loss  2.2259\n",
            "Epoch(2/5)\t Batch(1000/3375): \tTrain Loss  2.2407\n",
            "Epoch(2/5)\t Batch(1200/3375): \tTrain Loss  2.2308\n",
            "Epoch(2/5)\t Batch(1400/3375): \tTrain Loss  2.2691\n",
            "Epoch(2/5)\t Batch(1600/3375): \tTrain Loss  2.2268\n",
            "Epoch(2/5)\t Batch(1800/3375): \tTrain Loss  2.2402\n",
            "Epoch(2/5)\t Batch(2000/3375): \tTrain Loss  2.2213\n",
            "Epoch(2/5)\t Batch(2200/3375): \tTrain Loss  2.2562\n",
            "Epoch(2/5)\t Batch(2400/3375): \tTrain Loss  2.2327\n",
            "Epoch(2/5)\t Batch(2600/3375): \tTrain Loss  2.2192\n",
            "Epoch(2/5)\t Batch(2800/3375): \tTrain Loss  2.2353\n",
            "Epoch(2/5)\t Batch(3000/3375): \tTrain Loss  2.2634\n",
            "Epoch(2/5)\t Batch(3200/3375): \tTrain Loss  2.2224\n",
            "End of Epoch: 2 Train Accuracy: 0.5175 Val Loss: 2.2183 Val Accuracy: 0.5048\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/3375): \tTrain Loss  2.2270\n",
            "Epoch(3/5)\t Batch(200/3375): \tTrain Loss  2.1985\n",
            "Epoch(3/5)\t Batch(400/3375): \tTrain Loss  2.2296\n",
            "Epoch(3/5)\t Batch(600/3375): \tTrain Loss  2.2063\n",
            "Epoch(3/5)\t Batch(800/3375): \tTrain Loss  2.2028\n",
            "Epoch(3/5)\t Batch(1000/3375): \tTrain Loss  2.2114\n",
            "Epoch(3/5)\t Batch(1200/3375): \tTrain Loss  2.1944\n",
            "Epoch(3/5)\t Batch(1400/3375): \tTrain Loss  2.2085\n",
            "Epoch(3/5)\t Batch(1600/3375): \tTrain Loss  2.1803\n",
            "Epoch(3/5)\t Batch(1800/3375): \tTrain Loss  2.2064\n",
            "Epoch(3/5)\t Batch(2000/3375): \tTrain Loss  2.1790\n",
            "Epoch(3/5)\t Batch(2200/3375): \tTrain Loss  2.1935\n",
            "Epoch(3/5)\t Batch(2400/3375): \tTrain Loss  2.1968\n",
            "Epoch(3/5)\t Batch(2600/3375): \tTrain Loss  2.1849\n",
            "Epoch(3/5)\t Batch(2800/3375): \tTrain Loss  2.2163\n",
            "Epoch(3/5)\t Batch(3000/3375): \tTrain Loss  2.2022\n",
            "Epoch(3/5)\t Batch(3200/3375): \tTrain Loss  2.1606\n",
            "End of Epoch: 3 Train Accuracy: 0.6001 Val Loss: 2.1855 Val Accuracy: 0.5933\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/3375): \tTrain Loss  2.1739\n",
            "Epoch(4/5)\t Batch(200/3375): \tTrain Loss  2.1939\n",
            "Epoch(4/5)\t Batch(400/3375): \tTrain Loss  2.1708\n",
            "Epoch(4/5)\t Batch(600/3375): \tTrain Loss  2.2214\n",
            "Epoch(4/5)\t Batch(800/3375): \tTrain Loss  2.1600\n",
            "Epoch(4/5)\t Batch(1000/3375): \tTrain Loss  2.1663\n",
            "Epoch(4/5)\t Batch(1200/3375): \tTrain Loss  2.1431\n",
            "Epoch(4/5)\t Batch(1400/3375): \tTrain Loss  2.1562\n",
            "Epoch(4/5)\t Batch(1600/3375): \tTrain Loss  2.1975\n",
            "Epoch(4/5)\t Batch(1800/3375): \tTrain Loss  2.1700\n",
            "Epoch(4/5)\t Batch(2000/3375): \tTrain Loss  2.1581\n",
            "Epoch(4/5)\t Batch(2200/3375): \tTrain Loss  2.1355\n",
            "Epoch(4/5)\t Batch(2400/3375): \tTrain Loss  2.1490\n",
            "Epoch(4/5)\t Batch(2600/3375): \tTrain Loss  2.1626\n",
            "Epoch(4/5)\t Batch(2800/3375): \tTrain Loss  2.1942\n",
            "Epoch(4/5)\t Batch(3000/3375): \tTrain Loss  2.1295\n",
            "Epoch(4/5)\t Batch(3200/3375): \tTrain Loss  2.1802\n",
            "End of Epoch: 4 Train Accuracy: 0.6330 Val Loss: 2.1547 Val Accuracy: 0.6253\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/3375): \tTrain Loss  2.1608\n",
            "Epoch(5/5)\t Batch(200/3375): \tTrain Loss  2.1530\n",
            "Epoch(5/5)\t Batch(400/3375): \tTrain Loss  2.1592\n",
            "Epoch(5/5)\t Batch(600/3375): \tTrain Loss  2.1098\n",
            "Epoch(5/5)\t Batch(800/3375): \tTrain Loss  2.1201\n",
            "Epoch(5/5)\t Batch(1000/3375): \tTrain Loss  2.1662\n",
            "Epoch(5/5)\t Batch(1200/3375): \tTrain Loss  2.1293\n",
            "Epoch(5/5)\t Batch(1400/3375): \tTrain Loss  2.1729\n",
            "Epoch(5/5)\t Batch(1600/3375): \tTrain Loss  2.1301\n",
            "Epoch(5/5)\t Batch(1800/3375): \tTrain Loss  2.1212\n",
            "Epoch(5/5)\t Batch(2000/3375): \tTrain Loss  2.1859\n",
            "Epoch(5/5)\t Batch(2200/3375): \tTrain Loss  2.1336\n",
            "Epoch(5/5)\t Batch(2400/3375): \tTrain Loss  2.1153\n",
            "Epoch(5/5)\t Batch(2600/3375): \tTrain Loss  2.1499\n",
            "Epoch(5/5)\t Batch(2800/3375): \tTrain Loss  2.1787\n",
            "Epoch(5/5)\t Batch(3000/3375): \tTrain Loss  2.1521\n",
            "Epoch(5/5)\t Batch(3200/3375): \tTrain Loss  2.0500\n",
            "End of Epoch: 5 Train Accuracy: 0.6487 Val Loss: 2.1265 Val Accuracy: 0.6442\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▄▆▇██</td></tr><tr><td>Train Loss</td><td>█▇▆▂▁</td></tr><tr><td>Val Accuracy</td><td>▁▄▆▇██</td></tr><tr><td>Val Loss</td><td>█▆▅▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.6487</td></tr><tr><td>Train Loss</td><td>2.06729</td></tr><tr><td>Val Accuracy</td><td>0.64417</td></tr><tr><td>Val Loss</td><td>2.12646</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_sgd|loss_cross_entropy|lr=0.0001|batch_16|act_tanh|hid_4|neurons_64|nrns_5|init_Xavier231</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/d9csakgb' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/d9csakgb</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_185726-d9csakgb/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0ud0dcjy with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_185822-0ud0dcjy</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/0ud0dcjy' target=\"_blank\">easy-sweep-98</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/0ud0dcjy' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/0ud0dcjy</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0901 Val Loss: 2.2999 Val Accuracy: 0.0867\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/3375): \tTrain Loss  2.3127\n",
            "Epoch(1/5)\t Batch(200/3375): \tTrain Loss  1.7435\n",
            "Epoch(1/5)\t Batch(400/3375): \tTrain Loss  1.7363\n",
            "Epoch(1/5)\t Batch(600/3375): \tTrain Loss  1.7021\n",
            "Epoch(1/5)\t Batch(800/3375): \tTrain Loss  1.7191\n",
            "Epoch(1/5)\t Batch(1000/3375): \tTrain Loss  1.7511\n",
            "Epoch(1/5)\t Batch(1200/3375): \tTrain Loss  1.6271\n",
            "Epoch(1/5)\t Batch(1400/3375): \tTrain Loss  1.7949\n",
            "Epoch(1/5)\t Batch(1600/3375): \tTrain Loss  1.6398\n",
            "Epoch(1/5)\t Batch(1800/3375): \tTrain Loss  1.6354\n",
            "Epoch(1/5)\t Batch(2000/3375): \tTrain Loss  1.6847\n",
            "Epoch(1/5)\t Batch(2200/3375): \tTrain Loss  1.6889\n",
            "Epoch(1/5)\t Batch(2400/3375): \tTrain Loss  1.7355\n",
            "Epoch(1/5)\t Batch(2600/3375): \tTrain Loss  1.6950\n",
            "Epoch(1/5)\t Batch(2800/3375): \tTrain Loss  1.5949\n",
            "Epoch(1/5)\t Batch(3000/3375): \tTrain Loss  1.8038\n",
            "Epoch(1/5)\t Batch(3200/3375): \tTrain Loss  1.6790\n",
            "End of Epoch: 1 Train Accuracy: 0.8469 Val Loss: 1.6701 Val Accuracy: 0.8482\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/3375): \tTrain Loss  1.6150\n",
            "Epoch(2/5)\t Batch(200/3375): \tTrain Loss  1.8050\n",
            "Epoch(2/5)\t Batch(400/3375): \tTrain Loss  1.7874\n",
            "Epoch(2/5)\t Batch(600/3375): \tTrain Loss  1.5483\n",
            "Epoch(2/5)\t Batch(800/3375): \tTrain Loss  1.7447\n",
            "Epoch(2/5)\t Batch(1000/3375): \tTrain Loss  1.7408\n",
            "Epoch(2/5)\t Batch(1200/3375): \tTrain Loss  1.6886\n",
            "Epoch(2/5)\t Batch(1400/3375): \tTrain Loss  1.5927\n",
            "Epoch(2/5)\t Batch(1600/3375): \tTrain Loss  1.7384\n",
            "Epoch(2/5)\t Batch(1800/3375): \tTrain Loss  1.5629\n",
            "Epoch(2/5)\t Batch(2000/3375): \tTrain Loss  1.6162\n",
            "Epoch(2/5)\t Batch(2200/3375): \tTrain Loss  1.6694\n",
            "Epoch(2/5)\t Batch(2400/3375): \tTrain Loss  1.6072\n",
            "Epoch(2/5)\t Batch(2600/3375): \tTrain Loss  1.5758\n",
            "Epoch(2/5)\t Batch(2800/3375): \tTrain Loss  1.7952\n",
            "Epoch(2/5)\t Batch(3000/3375): \tTrain Loss  1.5700\n",
            "Epoch(2/5)\t Batch(3200/3375): \tTrain Loss  1.7653\n",
            "End of Epoch: 2 Train Accuracy: 0.8360 Val Loss: 1.6949 Val Accuracy: 0.8310\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/3375): \tTrain Loss  1.6449\n",
            "Epoch(3/5)\t Batch(200/3375): \tTrain Loss  1.7090\n",
            "Epoch(3/5)\t Batch(400/3375): \tTrain Loss  1.7179\n",
            "Epoch(3/5)\t Batch(600/3375): \tTrain Loss  1.7630\n",
            "Epoch(3/5)\t Batch(800/3375): \tTrain Loss  1.7955\n",
            "Epoch(3/5)\t Batch(1000/3375): \tTrain Loss  1.5913\n",
            "Epoch(3/5)\t Batch(1200/3375): \tTrain Loss  1.6856\n",
            "Epoch(3/5)\t Batch(1400/3375): \tTrain Loss  1.8261\n",
            "Epoch(3/5)\t Batch(1600/3375): \tTrain Loss  1.7078\n",
            "Epoch(3/5)\t Batch(1800/3375): \tTrain Loss  1.5938\n",
            "Epoch(3/5)\t Batch(2000/3375): \tTrain Loss  1.8382\n",
            "Epoch(3/5)\t Batch(2200/3375): \tTrain Loss  1.8593\n",
            "Epoch(3/5)\t Batch(2400/3375): \tTrain Loss  1.7633\n",
            "Epoch(3/5)\t Batch(2600/3375): \tTrain Loss  1.8873\n",
            "Epoch(3/5)\t Batch(2800/3375): \tTrain Loss  1.6983\n",
            "Epoch(3/5)\t Batch(3000/3375): \tTrain Loss  1.7926\n",
            "Epoch(3/5)\t Batch(3200/3375): \tTrain Loss  1.8408\n",
            "End of Epoch: 3 Train Accuracy: 0.8457 Val Loss: 1.7127 Val Accuracy: 0.8433\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/3375): \tTrain Loss  1.7552\n",
            "Epoch(4/5)\t Batch(200/3375): \tTrain Loss  1.8209\n",
            "Epoch(4/5)\t Batch(400/3375): \tTrain Loss  1.6209\n",
            "Epoch(4/5)\t Batch(600/3375): \tTrain Loss  1.6266\n",
            "Epoch(4/5)\t Batch(800/3375): \tTrain Loss  1.7041\n",
            "Epoch(4/5)\t Batch(1000/3375): \tTrain Loss  1.6434\n",
            "Epoch(4/5)\t Batch(1200/3375): \tTrain Loss  1.7711\n",
            "Epoch(4/5)\t Batch(1400/3375): \tTrain Loss  1.8218\n",
            "Epoch(4/5)\t Batch(1600/3375): \tTrain Loss  1.6091\n",
            "Epoch(4/5)\t Batch(1800/3375): \tTrain Loss  1.7622\n",
            "Epoch(4/5)\t Batch(2000/3375): \tTrain Loss  1.7776\n",
            "Epoch(4/5)\t Batch(2200/3375): \tTrain Loss  1.5936\n",
            "Epoch(4/5)\t Batch(2400/3375): \tTrain Loss  1.9409\n",
            "Epoch(4/5)\t Batch(2600/3375): \tTrain Loss  1.8224\n",
            "Epoch(4/5)\t Batch(2800/3375): \tTrain Loss  1.6522\n",
            "Epoch(4/5)\t Batch(3000/3375): \tTrain Loss  1.4964\n",
            "Epoch(4/5)\t Batch(3200/3375): \tTrain Loss  1.6211\n",
            "End of Epoch: 4 Train Accuracy: 0.8404 Val Loss: 1.6894 Val Accuracy: 0.8373\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/3375): \tTrain Loss  1.5530\n",
            "Epoch(5/5)\t Batch(200/3375): \tTrain Loss  1.5378\n",
            "Epoch(5/5)\t Batch(400/3375): \tTrain Loss  1.6723\n",
            "Epoch(5/5)\t Batch(600/3375): \tTrain Loss  1.7333\n",
            "Epoch(5/5)\t Batch(800/3375): \tTrain Loss  1.5785\n",
            "Epoch(5/5)\t Batch(1000/3375): \tTrain Loss  1.6235\n",
            "Epoch(5/5)\t Batch(1200/3375): \tTrain Loss  1.7761\n",
            "Epoch(5/5)\t Batch(1400/3375): \tTrain Loss  1.6743\n",
            "Epoch(5/5)\t Batch(1600/3375): \tTrain Loss  1.7495\n",
            "Epoch(5/5)\t Batch(1800/3375): \tTrain Loss  1.6467\n",
            "Epoch(5/5)\t Batch(2000/3375): \tTrain Loss  1.7507\n",
            "Epoch(5/5)\t Batch(2200/3375): \tTrain Loss  1.5451\n",
            "Epoch(5/5)\t Batch(2400/3375): \tTrain Loss  1.6972\n",
            "Epoch(5/5)\t Batch(2600/3375): \tTrain Loss  1.6315\n",
            "Epoch(5/5)\t Batch(2800/3375): \tTrain Loss  1.6525\n",
            "Epoch(5/5)\t Batch(3000/3375): \tTrain Loss  1.5519\n",
            "Epoch(5/5)\t Batch(3200/3375): \tTrain Loss  1.6562\n",
            "End of Epoch: 5 Train Accuracy: 0.8414 Val Loss: 1.7166 Val Accuracy: 0.8343\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁█████</td></tr><tr><td>Train Loss</td><td>▁█▃██</td></tr><tr><td>Val Accuracy</td><td>▁█████</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.84137</td></tr><tr><td>Train Loss</td><td>1.79455</td></tr><tr><td>Val Accuracy</td><td>0.83433</td></tr><tr><td>Val Loss</td><td>1.71657</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nadam|loss_cross_entropy|lr=0.001|batch_16|act_relu|hid_3|neurons_64|nrns_5|init_Xavier485</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/0ud0dcjy' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/0ud0dcjy</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_185822-0ud0dcjy/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ffck5g94 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_190048-ffck5g94</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ffck5g94' target=\"_blank\">quiet-sweep-99</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ffck5g94' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ffck5g94</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.0998 Val Loss: 7.3692 Val Accuracy: 0.1007\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/1687): \tTrain Loss  8.8599\n",
            "Epoch(1/10)\t Batch(200/1687): \tTrain Loss  2.1414\n",
            "Epoch(1/10)\t Batch(400/1687): \tTrain Loss  1.9695\n",
            "Epoch(1/10)\t Batch(600/1687): \tTrain Loss  1.8359\n",
            "Epoch(1/10)\t Batch(800/1687): \tTrain Loss  1.7467\n",
            "Epoch(1/10)\t Batch(1000/1687): \tTrain Loss  1.8058\n",
            "Epoch(1/10)\t Batch(1200/1687): \tTrain Loss  1.7496\n",
            "Epoch(1/10)\t Batch(1400/1687): \tTrain Loss  1.7592\n",
            "Epoch(1/10)\t Batch(1600/1687): \tTrain Loss  1.7788\n",
            "End of Epoch: 1 Train Accuracy: 0.7639 Val Loss: 1.7669 Val Accuracy: 0.7622\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/1687): \tTrain Loss  1.8990\n",
            "Epoch(2/10)\t Batch(200/1687): \tTrain Loss  1.7029\n",
            "Epoch(2/10)\t Batch(400/1687): \tTrain Loss  1.7869\n",
            "Epoch(2/10)\t Batch(600/1687): \tTrain Loss  1.7527\n",
            "Epoch(2/10)\t Batch(800/1687): \tTrain Loss  1.7151\n",
            "Epoch(2/10)\t Batch(1000/1687): \tTrain Loss  1.7439\n",
            "Epoch(2/10)\t Batch(1200/1687): \tTrain Loss  1.8401\n",
            "Epoch(2/10)\t Batch(1400/1687): \tTrain Loss  1.6871\n",
            "Epoch(2/10)\t Batch(1600/1687): \tTrain Loss  1.7376\n",
            "End of Epoch: 2 Train Accuracy: 0.8276 Val Loss: 1.7200 Val Accuracy: 0.8215\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/1687): \tTrain Loss  1.7673\n",
            "Epoch(3/10)\t Batch(200/1687): \tTrain Loss  1.7431\n",
            "Epoch(3/10)\t Batch(400/1687): \tTrain Loss  1.7192\n",
            "Epoch(3/10)\t Batch(600/1687): \tTrain Loss  1.7719\n",
            "Epoch(3/10)\t Batch(800/1687): \tTrain Loss  1.6637\n",
            "Epoch(3/10)\t Batch(1000/1687): \tTrain Loss  1.6750\n",
            "Epoch(3/10)\t Batch(1200/1687): \tTrain Loss  1.7714\n",
            "Epoch(3/10)\t Batch(1400/1687): \tTrain Loss  1.6626\n",
            "Epoch(3/10)\t Batch(1600/1687): \tTrain Loss  1.6473\n",
            "End of Epoch: 3 Train Accuracy: 0.8246 Val Loss: 1.7047 Val Accuracy: 0.8188\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/1687): \tTrain Loss  1.6613\n",
            "Epoch(4/10)\t Batch(200/1687): \tTrain Loss  1.7312\n",
            "Epoch(4/10)\t Batch(400/1687): \tTrain Loss  1.6603\n",
            "Epoch(4/10)\t Batch(600/1687): \tTrain Loss  1.7416\n",
            "Epoch(4/10)\t Batch(800/1687): \tTrain Loss  1.6410\n",
            "Epoch(4/10)\t Batch(1000/1687): \tTrain Loss  1.6927\n",
            "Epoch(4/10)\t Batch(1200/1687): \tTrain Loss  1.6105\n",
            "Epoch(4/10)\t Batch(1400/1687): \tTrain Loss  1.7062\n",
            "Epoch(4/10)\t Batch(1600/1687): \tTrain Loss  1.6598\n",
            "End of Epoch: 4 Train Accuracy: 0.8526 Val Loss: 1.6763 Val Accuracy: 0.8395\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/1687): \tTrain Loss  1.6540\n",
            "Epoch(5/10)\t Batch(200/1687): \tTrain Loss  1.7374\n",
            "Epoch(5/10)\t Batch(400/1687): \tTrain Loss  1.6175\n",
            "Epoch(5/10)\t Batch(600/1687): \tTrain Loss  1.7071\n",
            "Epoch(5/10)\t Batch(800/1687): \tTrain Loss  1.6594\n",
            "Epoch(5/10)\t Batch(1000/1687): \tTrain Loss  1.6523\n",
            "Epoch(5/10)\t Batch(1200/1687): \tTrain Loss  1.6032\n",
            "Epoch(5/10)\t Batch(1400/1687): \tTrain Loss  1.7041\n",
            "Epoch(5/10)\t Batch(1600/1687): \tTrain Loss  1.7621\n",
            "End of Epoch: 5 Train Accuracy: 0.8603 Val Loss: 1.6464 Val Accuracy: 0.8475\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/1687): \tTrain Loss  1.6182\n",
            "Epoch(6/10)\t Batch(200/1687): \tTrain Loss  1.6876\n",
            "Epoch(6/10)\t Batch(400/1687): \tTrain Loss  1.6375\n",
            "Epoch(6/10)\t Batch(600/1687): \tTrain Loss  1.5632\n",
            "Epoch(6/10)\t Batch(800/1687): \tTrain Loss  1.6622\n",
            "Epoch(6/10)\t Batch(1000/1687): \tTrain Loss  1.6433\n",
            "Epoch(6/10)\t Batch(1200/1687): \tTrain Loss  1.5631\n",
            "Epoch(6/10)\t Batch(1400/1687): \tTrain Loss  1.6502\n",
            "Epoch(6/10)\t Batch(1600/1687): \tTrain Loss  1.5942\n",
            "End of Epoch: 6 Train Accuracy: 0.8665 Val Loss: 1.6432 Val Accuracy: 0.8527\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/1687): \tTrain Loss  1.6078\n",
            "Epoch(7/10)\t Batch(200/1687): \tTrain Loss  1.6546\n",
            "Epoch(7/10)\t Batch(400/1687): \tTrain Loss  1.6345\n",
            "Epoch(7/10)\t Batch(600/1687): \tTrain Loss  1.6427\n",
            "Epoch(7/10)\t Batch(800/1687): \tTrain Loss  1.6755\n",
            "Epoch(7/10)\t Batch(1000/1687): \tTrain Loss  1.6612\n",
            "Epoch(7/10)\t Batch(1200/1687): \tTrain Loss  1.6710\n",
            "Epoch(7/10)\t Batch(1400/1687): \tTrain Loss  1.5665\n",
            "Epoch(7/10)\t Batch(1600/1687): \tTrain Loss  1.7028\n",
            "End of Epoch: 7 Train Accuracy: 0.8635 Val Loss: 1.6436 Val Accuracy: 0.8503\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/1687): \tTrain Loss  1.6082\n",
            "Epoch(8/10)\t Batch(200/1687): \tTrain Loss  1.6108\n",
            "Epoch(8/10)\t Batch(400/1687): \tTrain Loss  1.6359\n",
            "Epoch(8/10)\t Batch(600/1687): \tTrain Loss  1.5724\n",
            "Epoch(8/10)\t Batch(800/1687): \tTrain Loss  1.6597\n",
            "Epoch(8/10)\t Batch(1000/1687): \tTrain Loss  1.6039\n",
            "Epoch(8/10)\t Batch(1200/1687): \tTrain Loss  1.7197\n",
            "Epoch(8/10)\t Batch(1400/1687): \tTrain Loss  1.5947\n",
            "Epoch(8/10)\t Batch(1600/1687): \tTrain Loss  1.6251\n",
            "End of Epoch: 8 Train Accuracy: 0.8727 Val Loss: 1.6442 Val Accuracy: 0.8597\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/1687): \tTrain Loss  1.6190\n",
            "Epoch(9/10)\t Batch(200/1687): \tTrain Loss  1.5750\n",
            "Epoch(9/10)\t Batch(400/1687): \tTrain Loss  1.6186\n",
            "Epoch(9/10)\t Batch(600/1687): \tTrain Loss  1.6397\n",
            "Epoch(9/10)\t Batch(800/1687): \tTrain Loss  1.7002\n",
            "Epoch(9/10)\t Batch(1000/1687): \tTrain Loss  1.6446\n",
            "Epoch(9/10)\t Batch(1200/1687): \tTrain Loss  1.6727\n",
            "Epoch(9/10)\t Batch(1400/1687): \tTrain Loss  1.6004\n",
            "Epoch(9/10)\t Batch(1600/1687): \tTrain Loss  1.5935\n",
            "End of Epoch: 9 Train Accuracy: 0.8764 Val Loss: 1.6480 Val Accuracy: 0.8588\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/1687): \tTrain Loss  1.6402\n",
            "Epoch(10/10)\t Batch(200/1687): \tTrain Loss  1.6299\n",
            "Epoch(10/10)\t Batch(400/1687): \tTrain Loss  1.5345\n",
            "Epoch(10/10)\t Batch(600/1687): \tTrain Loss  1.7137\n",
            "Epoch(10/10)\t Batch(800/1687): \tTrain Loss  1.6575\n",
            "Epoch(10/10)\t Batch(1000/1687): \tTrain Loss  1.6228\n",
            "Epoch(10/10)\t Batch(1200/1687): \tTrain Loss  1.7178\n",
            "Epoch(10/10)\t Batch(1400/1687): \tTrain Loss  1.7110\n",
            "Epoch(10/10)\t Batch(1600/1687): \tTrain Loss  1.6373\n",
            "End of Epoch: 10 Train Accuracy: 0.8817 Val Loss: 1.6434 Val Accuracy: 0.8600\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▇█▇███████</td></tr><tr><td>Train Loss</td><td>█▇▆▃▃▅▃▁▇▄</td></tr><tr><td>Val Accuracy</td><td>▁▇█████████</td></tr><tr><td>Val Loss</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.8817</td></tr><tr><td>Train Loss</td><td>1.67704</td></tr><tr><td>Val Accuracy</td><td>0.86</td></tr><tr><td>Val Loss</td><td>1.6434</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_cross_entropy|lr=0.001|batch_32|act_sigmoid|hid_5|neurons_128|nrns_10|init_Random544</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ffck5g94' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/ffck5g94</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_190048-ffck5g94/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hf169mr3 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_190309-hf169mr3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/hf169mr3' target=\"_blank\">zany-sweep-100</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/sweeps/cd4pcsed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/hf169mr3' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/hf169mr3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0892 Val Loss: 2.3082 Val Accuracy: 0.0862\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/843): \tTrain Loss  2.3137\n",
            "Epoch(1/5)\t Batch(200/843): \tTrain Loss  2.1526\n",
            "Epoch(1/5)\t Batch(400/843): \tTrain Loss  2.0143\n",
            "Epoch(1/5)\t Batch(600/843): \tTrain Loss  1.9602\n",
            "Epoch(1/5)\t Batch(800/843): \tTrain Loss  1.8199\n",
            "End of Epoch: 1 Train Accuracy: 0.7514 Val Loss: 1.8596 Val Accuracy: 0.7617\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/843): \tTrain Loss  1.8184\n",
            "Epoch(2/5)\t Batch(200/843): \tTrain Loss  1.8744\n",
            "Epoch(2/5)\t Batch(400/843): \tTrain Loss  1.8890\n",
            "Epoch(2/5)\t Batch(600/843): \tTrain Loss  1.8095\n",
            "Epoch(2/5)\t Batch(800/843): \tTrain Loss  1.8456\n",
            "End of Epoch: 2 Train Accuracy: 0.7870 Val Loss: 1.7775 Val Accuracy: 0.7938\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/843): \tTrain Loss  1.8107\n",
            "Epoch(3/5)\t Batch(200/843): \tTrain Loss  1.7049\n",
            "Epoch(3/5)\t Batch(400/843): \tTrain Loss  1.7629\n",
            "Epoch(3/5)\t Batch(600/843): \tTrain Loss  1.7342\n",
            "Epoch(3/5)\t Batch(800/843): \tTrain Loss  1.7611\n",
            "End of Epoch: 3 Train Accuracy: 0.8064 Val Loss: 1.7555 Val Accuracy: 0.8125\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/843): \tTrain Loss  1.7301\n",
            "Epoch(4/5)\t Batch(200/843): \tTrain Loss  1.7529\n",
            "Epoch(4/5)\t Batch(400/843): \tTrain Loss  1.7059\n",
            "Epoch(4/5)\t Batch(600/843): \tTrain Loss  1.7647\n",
            "Epoch(4/5)\t Batch(800/843): \tTrain Loss  1.6944\n",
            "End of Epoch: 4 Train Accuracy: 0.8221 Val Loss: 1.7243 Val Accuracy: 0.8285\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/843): \tTrain Loss  1.7559\n",
            "Epoch(5/5)\t Batch(200/843): \tTrain Loss  1.7956\n",
            "Epoch(5/5)\t Batch(400/843): \tTrain Loss  1.7077\n",
            "Epoch(5/5)\t Batch(600/843): \tTrain Loss  1.7484\n",
            "Epoch(5/5)\t Batch(800/843): \tTrain Loss  1.6689\n",
            "End of Epoch: 5 Train Accuracy: 0.8390 Val Loss: 1.7111 Val Accuracy: 0.8413\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▇████</td></tr><tr><td>Train Loss</td><td>█▄▂▁▂</td></tr><tr><td>Val Accuracy</td><td>▁▇████</td></tr><tr><td>Val Loss</td><td>█▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.83904</td></tr><tr><td>Train Loss</td><td>1.72933</td></tr><tr><td>Val Accuracy</td><td>0.84133</td></tr><tr><td>Val Loss</td><td>1.7111</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_cross_entropy|lr=0.0001|batch_64|act_relu|hid_4|neurons_32|nrns_5|init_Xavier435</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/hf169mr3' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1/runs/hf169mr3</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-assignment1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_190309-hf169mr3/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.datasets import fashion_mnist, mnist\n",
        "\n",
        "# wandb.login(key='e6b43dd118f9a14e83fe12c597ad8d06bdfed432')\n",
        "# Define class names for Fashion-MNIST\n",
        "\n",
        "\n",
        "run = wandb.init(project=\"da6401-asg1\")\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "decay = 0.0005\n",
        "hid_layers = 4\n",
        "hid_size = 128\n",
        "nepoch = 10\n",
        "init = \"Xavier\"\n",
        "activation=\"relu\"\n",
        "# Load dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "# (val_images, ), (val_images, val_labels) = mnist.load_data()\n",
        "# Train-val split\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(\n",
        "    train_images, train_labels, test_size=0.1)\n",
        "\n",
        "# Preprocess data\n",
        "X = train_images.reshape(train_images.shape[0], -1)/ 255.0\n",
        "Y = train_labels\n",
        "Y = np.eye(10)[Y]     #one_hot encoding\n",
        "\n",
        "\n",
        "# X_train = train_images.reshape(train_images.shape[0], -1) / 255.0\n",
        "# X_val = val_images.reshape(val_images.shape[0], -1) / 255.0\n",
        "# X_test = test_images.reshape(test_images.shape[0], -1) / 255.0\n",
        "\n",
        "# # One-hot encode labels\n",
        "# Y_train = np.eye(10)[train_labels]\n",
        "# Y_val = np.eye(10)[val_labels]\n",
        "# Y_test = np.eye(10)[test_labels]\n",
        "\n",
        "# Model configuration\n",
        "# config = wandb.config\n",
        "layer_sizes = [hid_size] * (hid_layers + 1)\n",
        "\n",
        "# Model initialization\n",
        "model = initialize_model(activation, layer_sizes, init)\n",
        "opt = Optimizer(lr=1e-4, optimizer=\"nadam\", decay=0.0005, param=model.parameters())\n",
        "Loss = CrossEntropyLoss() #if config.loss == \"cross_entropy\" else MSE()\n",
        "\n",
        "# Training loop\n",
        "\n",
        "for epoch in range(nepoch):\n",
        "  print(\"-------x-------\")\n",
        "  #Shuffling\n",
        "  indices = np.random.permutation(X.shape[0])\n",
        "  X = X[indices]\n",
        "  Y = Y[indices]\n",
        "\n",
        "  for i in range(0, train_images.shape[0], batch_size):\n",
        "    Xb = X[i:i + batch_size]\n",
        "    Yb = Y[i:i + batch_size]\n",
        "\n",
        "    logits = model(Xb)\n",
        "\n",
        "    loss = Loss(logits, Yb)\n",
        "\n",
        "    #Backward Pass\n",
        "    dout = Loss.grad(logits, Yb)\n",
        "    dout = model.backward(dout)\n",
        "\n",
        "\n",
        "    batch_num = i//batch_size\n",
        "    total_batch = train_images.shape[0]//batch_size\n",
        "\n",
        "\n",
        "\n",
        "    #Parameter Update\n",
        "    opt(model.parameters(), dout[1])\n",
        "\n",
        "    if batch_num%500 == 0: # print every once in a while uhh to be precise after 200 batch\n",
        "      print(f'Epoch({epoch+1}/{nepoch})\\t Batch({batch_num:2d}/{total_batch:2d}): \\tTrain Loss  {loss:.4f}')\n",
        "\n",
        "  opt.t += 1\n",
        "  logits = model(X)\n",
        "  train_accuracy = np.mean(np.argmax(logits, axis=1) == np.argmax(Y, axis=1))\n",
        "\n",
        "  # val_logits = model(val_images.reshape(val_images.shape[0], -1)/ 255.0)\n",
        "  # Yv = np.eye(10)[val_labels]\n",
        "  # val_accuracy = np.mean(np.argmax(val_logits, axis=1) == np.argmax(Yv, axis=1))\n",
        "  logits = model(X)\n",
        "  train_loss = Loss(logits, Y)\n",
        "  train_accuracy = np.mean(np.argmax(logits, axis=1) == np.argmax(Y, axis=1))\n",
        "\n",
        "  val_logits = model(val_images.reshape(val_images.shape[0], -1)/ 255.0)\n",
        "  Yv = np.eye(10)[val_labels]\n",
        "  val_loss = Loss(val_logits, Yv)\n",
        "  val_accuracy = np.mean(np.argmax(val_logits, axis=1) == np.argmax(Yv, axis=1))\n",
        "\n",
        "    # Log metrics\n",
        "  wandb.log({\n",
        "      \"Fashion_MNIST_Epoch\": epoch+1,\n",
        "      \"Fashion_MNIST_Train Loss\": train_loss,\n",
        "      \"Fashion_MNIST_Val Loss\": val_loss,\n",
        "      \"Fashion_MNIST_Train Accuracy\": train_accuracy,\n",
        "      \"Fashion_MNIST_Val Accuracy\": val_accuracy\n",
        "    })\n",
        "  print(f\"End of Epoch: {epoch+1} Train Accuracy: {train_accuracy:.4f} Val Loss: {val_loss:.4f} Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "x = test_images\n",
        "x = x.reshape(x.shape[0], -1)/255.0\n",
        "y = test_labels\n",
        "\n",
        "X_val = val_images.reshape(val_images.shape[0], -1)/255.0\n",
        "Y_val = val_labels\n",
        "\n",
        "    # Final evaluation\n",
        "train_pred = np.argmax(model(X), axis=1)\n",
        "val_pred = np.argmax(model(X_val), axis=1)\n",
        "test_pred = np.argmax(model(x), axis=1)\n",
        "\n",
        "\n",
        "x = test_images\n",
        "x = x.reshape(x.shape[0], -1)/255.0\n",
        "y = test_labels\n",
        "\n",
        "#Forward Pass\n",
        "logits = model(x)\n",
        "accuracy_formula = np.mean(np.argmax(logits, axis=1) == y)\n",
        "print(f\"Test Accuracy: {accuracy_formula}\")\n",
        "\n",
        "#MNIST\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Train-val split\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(\n",
        "    train_images, train_labels, test_size=0.1)\n",
        "\n",
        "# Preprocess data\n",
        "X = train_images.reshape(train_images.shape[0], -1)/ 255.0\n",
        "Y = train_labels\n",
        "Y = np.eye(10)[Y]\n",
        "\n",
        "for epoch in range(nepoch):\n",
        "  print(\"-------x-------\")\n",
        "  #Shuffling\n",
        "  indices = np.random.permutation(X.shape[0])\n",
        "  X = X[indices]\n",
        "  Y = Y[indices]\n",
        "\n",
        "  for i in range(0, train_images.shape[0], batch_size):\n",
        "    Xb = X[i:i + batch_size]\n",
        "    Yb = Y[i:i + batch_size]\n",
        "\n",
        "    logits = model(Xb)\n",
        "\n",
        "    loss = Loss(logits, Yb)\n",
        "\n",
        "    #Backward Pass\n",
        "    dout = Loss.grad(logits, Yb)\n",
        "    dout = model.backward(dout)\n",
        "\n",
        "\n",
        "    batch_num = i//batch_size\n",
        "    total_batch = train_images.shape[0]//batch_size\n",
        "\n",
        "\n",
        "\n",
        "    #Parameter Update\n",
        "    opt(model.parameters(), dout[1])\n",
        "\n",
        "    if batch_num%500 == 0: # print every once in a while uhh to be precise after 200 batch\n",
        "      print(f'Epoch({epoch+1}/{nepoch})\\t Batch({batch_num:2d}/{total_batch:2d}): \\tTrain Loss  {loss:.4f}')\n",
        "\n",
        "  opt.t += 1\n",
        "  logits = model(X)\n",
        "  train_accuracy = np.mean(np.argmax(logits, axis=1) == np.argmax(Y, axis=1))\n",
        "\n",
        "  # val_logits = model(val_images.reshape(val_images.shape[0], -1)/ 255.0)\n",
        "  # Yv = np.eye(10)[val_labels]\n",
        "  # val_accuracy = np.mean(np.argmax(val_logits, axis=1) == np.argmax(Yv, axis=1))\n",
        "  logits = model(X)\n",
        "  train_loss = Loss(logits, Y)\n",
        "  train_accuracy = np.mean(np.argmax(logits, axis=1) == np.argmax(Y, axis=1))\n",
        "\n",
        "  val_logits = model(val_images.reshape(val_images.shape[0], -1)/ 255.0)\n",
        "  Yv = np.eye(10)[val_labels]\n",
        "  val_loss = Loss(val_logits, Yv)\n",
        "  val_accuracy = np.mean(np.argmax(val_logits, axis=1) == np.argmax(Yv, axis=1))\n",
        "\n",
        "    # Log metrics\n",
        "  wandb.log({\n",
        "      \"Epoch\": epoch+1,\n",
        "      \"MNIST_Train Loss\": train_loss,\n",
        "      \"MNIST_Val Loss\": val_loss,\n",
        "      \"MNIST_Train Accuracy\": train_accuracy,\n",
        "      \"MNIST_Val Accuracy\": val_accuracy\n",
        "    })\n",
        "  print(f\"End of Epoch: {epoch+1} Train Accuracy: {train_accuracy:.4f} Val Loss: {val_loss:.4f} Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "x = test_images\n",
        "x = x.reshape(x.shape[0], -1)/255.0\n",
        "y = test_labels\n",
        "\n",
        "#Forward Pass\n",
        "logits = model(x)\n",
        "accuracy_formula = np.mean(np.argmax(logits, axis=1) == y)\n",
        "print(f\"MNIST_Test Accuracy: {accuracy_formula}\")\n",
        "\n",
        "\n",
        "wandb.finish()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UV7BRG02L1VL",
        "outputId": "b17d2867-2e16-4444-e87f-1a95a2d7d677"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250316_141521-hs3cank1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/hs3cank1' target=\"_blank\">revived-plasma-217</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/hs3cank1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/hs3cank1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  2.3022\n",
            "Epoch(1/10)\t Batch(500/843): \tTrain Loss  1.7390\n",
            "End of Epoch: 1 Train Accuracy: 0.8549 Val Loss: 1.6970 Val Accuracy: 0.8520\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  1.6707\n",
            "Epoch(2/10)\t Batch(500/843): \tTrain Loss  1.7074\n",
            "End of Epoch: 2 Train Accuracy: 0.8710 Val Loss: 1.6644 Val Accuracy: 0.8672\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  1.7236\n",
            "Epoch(3/10)\t Batch(500/843): \tTrain Loss  1.6344\n",
            "End of Epoch: 3 Train Accuracy: 0.8833 Val Loss: 1.6304 Val Accuracy: 0.8753\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  1.6437\n",
            "Epoch(4/10)\t Batch(500/843): \tTrain Loss  1.6351\n",
            "End of Epoch: 4 Train Accuracy: 0.8928 Val Loss: 1.6464 Val Accuracy: 0.8812\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  1.6565\n",
            "Epoch(5/10)\t Batch(500/843): \tTrain Loss  1.5669\n",
            "End of Epoch: 5 Train Accuracy: 0.8989 Val Loss: 1.6046 Val Accuracy: 0.8877\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  1.6323\n",
            "Epoch(6/10)\t Batch(500/843): \tTrain Loss  1.6079\n",
            "End of Epoch: 6 Train Accuracy: 0.9047 Val Loss: 1.6065 Val Accuracy: 0.8903\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  1.6570\n",
            "Epoch(7/10)\t Batch(500/843): \tTrain Loss  1.5716\n",
            "End of Epoch: 7 Train Accuracy: 0.9013 Val Loss: 1.6201 Val Accuracy: 0.8833\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  1.6445\n",
            "Epoch(8/10)\t Batch(500/843): \tTrain Loss  1.5374\n",
            "End of Epoch: 8 Train Accuracy: 0.9119 Val Loss: 1.6068 Val Accuracy: 0.8925\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  1.5898\n",
            "Epoch(9/10)\t Batch(500/843): \tTrain Loss  1.6473\n",
            "End of Epoch: 9 Train Accuracy: 0.9139 Val Loss: 1.6119 Val Accuracy: 0.8912\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  1.5692\n",
            "Epoch(10/10)\t Batch(500/843): \tTrain Loss  1.6792\n",
            "End of Epoch: 10 Train Accuracy: 0.9151 Val Loss: 1.5974 Val Accuracy: 0.8873\n",
            "Test Accuracy: 0.8814\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  2.2794\n",
            "Epoch(1/10)\t Batch(500/843): \tTrain Loss  1.6165\n",
            "End of Epoch: 1 Train Accuracy: 0.9576 Val Loss: 1.5351 Val Accuracy: 0.9463\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  1.5810\n",
            "Epoch(2/10)\t Batch(500/843): \tTrain Loss  1.5165\n",
            "End of Epoch: 2 Train Accuracy: 0.9736 Val Loss: 1.5282 Val Accuracy: 0.9585\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  1.5449\n",
            "Epoch(3/10)\t Batch(500/843): \tTrain Loss  1.5339\n",
            "End of Epoch: 3 Train Accuracy: 0.9798 Val Loss: 1.5153 Val Accuracy: 0.9672\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  1.4978\n",
            "Epoch(4/10)\t Batch(500/843): \tTrain Loss  1.5290\n",
            "End of Epoch: 4 Train Accuracy: 0.9848 Val Loss: 1.4939 Val Accuracy: 0.9732\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  1.4899\n",
            "Epoch(5/10)\t Batch(500/843): \tTrain Loss  1.4845\n",
            "End of Epoch: 5 Train Accuracy: 0.9885 Val Loss: 1.4977 Val Accuracy: 0.9762\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  1.4679\n",
            "Epoch(6/10)\t Batch(500/843): \tTrain Loss  1.5400\n",
            "End of Epoch: 6 Train Accuracy: 0.9904 Val Loss: 1.4982 Val Accuracy: 0.9755\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  1.4838\n",
            "Epoch(7/10)\t Batch(500/843): \tTrain Loss  1.4777\n",
            "End of Epoch: 7 Train Accuracy: 0.9922 Val Loss: 1.4921 Val Accuracy: 0.9775\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  1.4612\n",
            "Epoch(8/10)\t Batch(500/843): \tTrain Loss  1.4758\n",
            "End of Epoch: 8 Train Accuracy: 0.9937 Val Loss: 1.4927 Val Accuracy: 0.9763\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  1.4669\n",
            "Epoch(9/10)\t Batch(500/843): \tTrain Loss  1.4842\n",
            "End of Epoch: 9 Train Accuracy: 0.9943 Val Loss: 1.4932 Val Accuracy: 0.9772\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  1.4763\n",
            "Epoch(10/10)\t Batch(500/843): \tTrain Loss  1.4557\n",
            "End of Epoch: 10 Train Accuracy: 0.9959 Val Loss: 1.4910 Val Accuracy: 0.9772\n",
            "MNIST_Test Accuracy: 0.98\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>Fashion_MNIST_Epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>Fashion_MNIST_Train Accuracy</td><td>▁▃▄▅▆▇▆███</td></tr><tr><td>Fashion_MNIST_Train Loss</td><td>█▆▄▅▂▂▃▂▂▁</td></tr><tr><td>Fashion_MNIST_Val Accuracy</td><td>▁▄▅▆▇█▆██▇</td></tr><tr><td>Fashion_MNIST_Val Loss</td><td>█▆▃▄▂▂▃▂▂▁</td></tr><tr><td>MNIST_Train Accuracy</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>MNIST_Train Loss</td><td>█▇▅▂▂▂▁▁▁▁</td></tr><tr><td>MNIST_Val Accuracy</td><td>▁▄▆▇██████</td></tr><tr><td>MNIST_Val Loss</td><td>█▇▅▁▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Fashion_MNIST_Epoch</td><td>10</td></tr><tr><td>Fashion_MNIST_Train Accuracy</td><td>0.91506</td></tr><tr><td>Fashion_MNIST_Train Loss</td><td>1.57766</td></tr><tr><td>Fashion_MNIST_Val Accuracy</td><td>0.88733</td></tr><tr><td>Fashion_MNIST_Val Loss</td><td>1.59742</td></tr><tr><td>MNIST_Train Accuracy</td><td>0.99585</td></tr><tr><td>MNIST_Train Loss</td><td>1.47163</td></tr><tr><td>MNIST_Val Accuracy</td><td>0.97717</td></tr><tr><td>MNIST_Val Loss</td><td>1.49099</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">revived-plasma-217</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/hs3cank1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/hs3cank1</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250316_141521-hs3cank1/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "# wandb.login(key='e6b43dd118f9a14e83fe12c597ad8d06bdfed432')\n",
        "# Define class names for Fashion-MNIST\n",
        "\n",
        "\n",
        "run = wandb.init(project=\"da6401-asg1\")\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "decay = 0.0005\n",
        "hid_layers = 4\n",
        "hid_size = 128\n",
        "nepoch = 10\n",
        "init = \"Xavier\"\n",
        "activation=\"relu\"\n",
        "# Load dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Train-val split\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(\n",
        "    train_images, train_labels, test_size=0.1)\n",
        "\n",
        "# Preprocess data\n",
        "X = train_images.reshape(train_images.shape[0], -1)/ 255.0\n",
        "Y = train_labels\n",
        "Y = np.eye(10)[Y]     #one_hot encoding\n",
        "\n",
        "\n",
        "# X_train = train_images.reshape(train_images.shape[0], -1) / 255.0\n",
        "# X_val = val_images.reshape(val_images.shape[0], -1) / 255.0\n",
        "# X_test = test_images.reshape(test_images.shape[0], -1) / 255.0\n",
        "\n",
        "# # One-hot encode labels\n",
        "# Y_train = np.eye(10)[train_labels]\n",
        "# Y_val = np.eye(10)[val_labels]\n",
        "# Y_test = np.eye(10)[test_labels]\n",
        "\n",
        "# Model configuration\n",
        "# config = wandb.config\n",
        "layer_sizes = [hid_size] * (hid_layers + 1)\n",
        "\n",
        "# Model initialization\n",
        "model = initialize_model(activation, layer_sizes, init)\n",
        "opt = Optimizer(lr=1e-4, optimizer=\"rmsprop\", decay=0.0005, param=model.parameters())\n",
        "Loss = CrossEntropyLoss() #if config.loss == \"cross_entropy\" else MSE()\n",
        "\n",
        "# Training loop\n",
        "\n",
        "for epoch in range(nepoch):\n",
        "  print(\"-------x-------\")\n",
        "  #Shuffling\n",
        "  indices = np.random.permutation(X.shape[0])\n",
        "  X = X[indices]\n",
        "  Y = Y[indices]\n",
        "\n",
        "  for i in range(0, train_images.shape[0], batch_size):\n",
        "    Xb = X[i:i + batch_size]\n",
        "    Yb = Y[i:i + batch_size]\n",
        "\n",
        "    logits = model(Xb)\n",
        "\n",
        "    loss = Loss(logits, Yb)\n",
        "\n",
        "    #Backward Pass\n",
        "    dout = Loss.grad(logits, Yb)\n",
        "    dout = model.backward(dout)\n",
        "\n",
        "\n",
        "    batch_num = i//batch_size\n",
        "    total_batch = train_images.shape[0]//batch_size\n",
        "\n",
        "\n",
        "\n",
        "    #Parameter Update\n",
        "    opt(model.parameters(), dout[1])\n",
        "\n",
        "    if batch_num%500 == 0: # print every once in a while uhh to be precise after 200 batch\n",
        "      print(f'Epoch({epoch+1}/{nepoch})\\t Batch({batch_num:2d}/{total_batch:2d}): \\tTrain Loss  {loss:.4f}')\n",
        "\n",
        "  opt.t += 1\n",
        "  logits = model(X)\n",
        "  train_accuracy = np.mean(np.argmax(logits, axis=1) == np.argmax(Y, axis=1))\n",
        "\n",
        "  # val_logits = model(val_images.reshape(val_images.shape[0], -1)/ 255.0)\n",
        "  # Yv = np.eye(10)[val_labels]\n",
        "  # val_accuracy = np.mean(np.argmax(val_logits, axis=1) == np.argmax(Yv, axis=1))\n",
        "  logits = model(X)\n",
        "  train_loss = Loss(logits, Y)\n",
        "  train_accuracy = np.mean(np.argmax(logits, axis=1) == np.argmax(Y, axis=1))\n",
        "\n",
        "  val_logits = model(val_images.reshape(val_images.shape[0], -1)/ 255.0)\n",
        "  Yv = np.eye(10)[val_labels]\n",
        "  val_loss = Loss(val_logits, Yv)\n",
        "  val_accuracy = np.mean(np.argmax(val_logits, axis=1) == np.argmax(Yv, axis=1))\n",
        "\n",
        "    # Log metrics\n",
        "  wandb.log({\n",
        "      \"Epoch\": epoch+1,\n",
        "      \"Train Loss\": train_loss,\n",
        "      \"Val Loss\": val_loss,\n",
        "      \"Train Accuracy\": train_accuracy,\n",
        "      \"Val Accuracy\": val_accuracy\n",
        "    })\n",
        "  print(f\"End of Epoch: {epoch+1} Train Accuracy: {train_accuracy:.4f} Val Loss: {val_loss:.4f} Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "x = test_images\n",
        "x = x.reshape(x.shape[0], -1)/255.0\n",
        "y = test_labels\n",
        "\n",
        "X_val = val_images.reshape(val_images.shape[0], -1)/255.0\n",
        "Y_val = val_labels\n",
        "\n",
        "    # Final evaluation\n",
        "train_pred = np.argmax(model(X), axis=1)\n",
        "val_pred = np.argmax(model(X_val), axis=1)\n",
        "test_pred = np.argmax(model(x), axis=1)\n",
        "\n",
        "\n",
        "x = test_images\n",
        "x = x.reshape(x.shape[0], -1)/255.0\n",
        "y = test_labels\n",
        "\n",
        "#Forward Pass\n",
        "logits = model(x)\n",
        "accuracy_formula = np.mean(np.argmax(logits, axis=1) == y)\n",
        "print(f\"Test Accuracy: {accuracy_formula}\")\n",
        "\n",
        "# Log confusion matrices\n",
        "wandb.log({\n",
        "    'Train Confusion Matrix': wandb.plot.confusion_matrix(\n",
        "        y_true=train_labels,\n",
        "        preds=train_pred,\n",
        "        class_names=CLASS_NAMES,\n",
        "        title=\"Train Confusion Matrix\"),\n",
        "\n",
        "    'Validation Confusion Matrix': wandb.plot.confusion_matrix(\n",
        "        y_true=val_labels,\n",
        "        preds=val_pred,\n",
        "        class_names=CLASS_NAMES,\n",
        "        title=\"Validation Confusion Matrix\"),\n",
        "\n",
        "    'Test Confusion Matrix': wandb.plot.confusion_matrix(\n",
        "        y_true=test_labels,\n",
        "        preds=test_pred,\n",
        "        class_names=CLASS_NAMES,\n",
        "        title=\"Test Confusion Matrix\")\n",
        "})\n",
        "# Log sklearn-style matrix\n",
        "wandb.log({'confusion_matrix': wandb.sklearn.plot_confusion_matrix(\n",
        "    test_labels,\n",
        "    test_pred,\n",
        "    labels=CLASS_NAMES)\n",
        "})\n",
        "\n",
        "wandb.finish()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9O0R3IcRV2M7",
        "outputId": "7d447b8f-9d01-46ce-ba66-870500c104a2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  2.3041\n",
            "Epoch(1/10)\t Batch(500/843): \tTrain Loss  1.7701\n",
            "End of Epoch: 1 Train Accuracy: 0.8210 Val Loss: 1.7336 Val Accuracy: 0.8212\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  1.7084\n",
            "Epoch(2/10)\t Batch(500/843): \tTrain Loss  1.7309\n",
            "End of Epoch: 2 Train Accuracy: 0.8520 Val Loss: 1.7082 Val Accuracy: 0.8505\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  1.7272\n",
            "Epoch(3/10)\t Batch(500/843): \tTrain Loss  1.7166\n",
            "End of Epoch: 3 Train Accuracy: 0.8632 Val Loss: 1.6802 Val Accuracy: 0.8598\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  1.6299\n",
            "Epoch(4/10)\t Batch(500/843): \tTrain Loss  1.6233\n",
            "End of Epoch: 4 Train Accuracy: 0.8734 Val Loss: 1.6603 Val Accuracy: 0.8680\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  1.6953\n",
            "Epoch(5/10)\t Batch(500/843): \tTrain Loss  1.5754\n",
            "End of Epoch: 5 Train Accuracy: 0.8753 Val Loss: 1.6553 Val Accuracy: 0.8680\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  1.6130\n",
            "Epoch(6/10)\t Batch(500/843): \tTrain Loss  1.6347\n",
            "End of Epoch: 6 Train Accuracy: 0.8837 Val Loss: 1.6464 Val Accuracy: 0.8733\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  1.6606\n",
            "Epoch(7/10)\t Batch(500/843): \tTrain Loss  1.6255\n",
            "End of Epoch: 7 Train Accuracy: 0.8893 Val Loss: 1.6339 Val Accuracy: 0.8802\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  1.5587\n",
            "Epoch(8/10)\t Batch(500/843): \tTrain Loss  1.6219\n",
            "End of Epoch: 8 Train Accuracy: 0.8933 Val Loss: 1.6215 Val Accuracy: 0.8813\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  1.5688\n",
            "Epoch(9/10)\t Batch(500/843): \tTrain Loss  1.6567\n",
            "End of Epoch: 9 Train Accuracy: 0.8915 Val Loss: 1.6274 Val Accuracy: 0.8768\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  1.5873\n",
            "Epoch(10/10)\t Batch(500/843): \tTrain Loss  1.6125\n",
            "End of Epoch: 10 Train Accuracy: 0.8976 Val Loss: 1.6210 Val Accuracy: 0.8817\n",
            "Test Accuracy: 0.8744\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▃▃▄▅▆▆▇█▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>Train Accuracy</td><td>▂▄▅▅▆▇▇▇▇█▁▄▅▆▆▇▇▇▇█</td></tr><tr><td>Train Loss</td><td>█▅▄▃▂▃▂▂▂▂▇▆▄▃▃▃▂▁▁▁</td></tr><tr><td>Val Accuracy</td><td>▂▄▆▆▆▇▇▆▇█▁▄▅▆▆▆▇▇▇▇</td></tr><tr><td>Val Loss</td><td>█▅▄▃▂▂▂▁▂▂▇▆▄▃▃▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.89757</td></tr><tr><td>Train Loss</td><td>1.60895</td></tr><tr><td>Val Accuracy</td><td>0.88167</td></tr><tr><td>Val Loss</td><td>1.62097</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">electric-sun-218</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/0xde9duv' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/0xde9duv</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1</a><br>Synced 5 W&B file(s), 4 media file(s), 8 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250316_143019-0xde9duv/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "BQuXOWFAq1yP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0fa890b9-47a3-4578-bbc3-9680b05f8713"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: 7bcahxlc\n",
            "Sweep URL: https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: m0yfwaub with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_162002-m0yfwaub</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/m0yfwaub' target=\"_blank\">autumn-sweep-1</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/m0yfwaub' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/m0yfwaub</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Start of Training: 1 Train Accuracy: 0.0861 Val Loss: 0.1679 Val Accuracy: 0.0898\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/3375): \tTrain Loss  0.1863\n",
            "Epoch(1/10)\t Batch(200/3375): \tTrain Loss  0.1560\n",
            "Epoch(1/10)\t Batch(400/3375): \tTrain Loss  0.1839\n",
            "Epoch(1/10)\t Batch(600/3375): \tTrain Loss  0.1962\n",
            "Epoch(1/10)\t Batch(800/3375): \tTrain Loss  0.1682\n",
            "Epoch(1/10)\t Batch(1000/3375): \tTrain Loss  0.1498\n",
            "Epoch(1/10)\t Batch(1200/3375): \tTrain Loss  0.1710\n",
            "Epoch(1/10)\t Batch(1400/3375): \tTrain Loss  0.1736\n",
            "Epoch(1/10)\t Batch(1600/3375): \tTrain Loss  0.1785\n",
            "Epoch(1/10)\t Batch(1800/3375): \tTrain Loss  0.1732\n",
            "Epoch(1/10)\t Batch(2000/3375): \tTrain Loss  0.1585\n",
            "Epoch(1/10)\t Batch(2200/3375): \tTrain Loss  0.1812\n",
            "Epoch(1/10)\t Batch(2400/3375): \tTrain Loss  0.1562\n",
            "Epoch(1/10)\t Batch(2600/3375): \tTrain Loss  0.1795\n",
            "Epoch(1/10)\t Batch(2800/3375): \tTrain Loss  0.1602\n",
            "Epoch(1/10)\t Batch(3000/3375): \tTrain Loss  0.1700\n",
            "Epoch(1/10)\t Batch(3200/3375): \tTrain Loss  0.1872\n",
            "End of Epoch: 1 Train Accuracy: 0.0933 Val Loss: 0.1676 Val Accuracy: 0.0897\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/3375): \tTrain Loss  0.1352\n",
            "Epoch(2/10)\t Batch(200/3375): \tTrain Loss  0.1720\n",
            "Epoch(2/10)\t Batch(400/3375): \tTrain Loss  0.1732\n",
            "Epoch(2/10)\t Batch(600/3375): \tTrain Loss  0.1395\n",
            "Epoch(2/10)\t Batch(800/3375): \tTrain Loss  0.1800\n",
            "Epoch(2/10)\t Batch(1000/3375): \tTrain Loss  0.1623\n",
            "Epoch(2/10)\t Batch(1200/3375): \tTrain Loss  0.1829\n",
            "Epoch(2/10)\t Batch(1400/3375): \tTrain Loss  0.1658\n",
            "Epoch(2/10)\t Batch(1600/3375): \tTrain Loss  0.1763\n",
            "Epoch(2/10)\t Batch(1800/3375): \tTrain Loss  0.1802\n",
            "Epoch(2/10)\t Batch(2000/3375): \tTrain Loss  0.1519\n",
            "Epoch(2/10)\t Batch(2200/3375): \tTrain Loss  0.1617\n",
            "Epoch(2/10)\t Batch(2400/3375): \tTrain Loss  0.1691\n",
            "Epoch(2/10)\t Batch(2600/3375): \tTrain Loss  0.1484\n",
            "Epoch(2/10)\t Batch(2800/3375): \tTrain Loss  0.1754\n",
            "Epoch(2/10)\t Batch(3000/3375): \tTrain Loss  0.1820\n",
            "Epoch(2/10)\t Batch(3200/3375): \tTrain Loss  0.1546\n",
            "End of Epoch: 2 Train Accuracy: 0.0971 Val Loss: 0.1678 Val Accuracy: 0.0917\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/3375): \tTrain Loss  0.1333\n",
            "Epoch(3/10)\t Batch(200/3375): \tTrain Loss  0.1542\n",
            "Epoch(3/10)\t Batch(400/3375): \tTrain Loss  0.1758\n",
            "Epoch(3/10)\t Batch(600/3375): \tTrain Loss  0.1537\n",
            "Epoch(3/10)\t Batch(800/3375): \tTrain Loss  0.1876\n",
            "Epoch(3/10)\t Batch(1000/3375): \tTrain Loss  0.1768\n",
            "Epoch(3/10)\t Batch(1200/3375): \tTrain Loss  0.1763\n",
            "Epoch(3/10)\t Batch(1400/3375): \tTrain Loss  0.1676\n",
            "Epoch(3/10)\t Batch(1600/3375): \tTrain Loss  0.1546\n",
            "Epoch(3/10)\t Batch(1800/3375): \tTrain Loss  0.1480\n",
            "Epoch(3/10)\t Batch(2000/3375): \tTrain Loss  0.1622\n",
            "Epoch(3/10)\t Batch(2200/3375): \tTrain Loss  0.1573\n",
            "Epoch(3/10)\t Batch(2400/3375): \tTrain Loss  0.1857\n",
            "Epoch(3/10)\t Batch(2600/3375): \tTrain Loss  0.1614\n",
            "Epoch(3/10)\t Batch(2800/3375): \tTrain Loss  0.1857\n",
            "Epoch(3/10)\t Batch(3000/3375): \tTrain Loss  0.1870\n",
            "Epoch(3/10)\t Batch(3200/3375): \tTrain Loss  0.1878\n",
            "End of Epoch: 3 Train Accuracy: 0.1005 Val Loss: 0.1675 Val Accuracy: 0.0887\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/3375): \tTrain Loss  0.1505\n",
            "Epoch(4/10)\t Batch(200/3375): \tTrain Loss  0.1406\n",
            "Epoch(4/10)\t Batch(400/3375): \tTrain Loss  0.1648\n",
            "Epoch(4/10)\t Batch(600/3375): \tTrain Loss  0.1639\n",
            "Epoch(4/10)\t Batch(800/3375): \tTrain Loss  0.1669\n",
            "Epoch(4/10)\t Batch(1000/3375): \tTrain Loss  0.1302\n",
            "Epoch(4/10)\t Batch(1200/3375): \tTrain Loss  0.1457\n",
            "Epoch(4/10)\t Batch(1400/3375): \tTrain Loss  0.1592\n",
            "Epoch(4/10)\t Batch(1600/3375): \tTrain Loss  0.1626\n",
            "Epoch(4/10)\t Batch(1800/3375): \tTrain Loss  0.1786\n",
            "Epoch(4/10)\t Batch(2000/3375): \tTrain Loss  0.1524\n",
            "Epoch(4/10)\t Batch(2200/3375): \tTrain Loss  0.1900\n",
            "Epoch(4/10)\t Batch(2400/3375): \tTrain Loss  0.1615\n",
            "Epoch(4/10)\t Batch(2600/3375): \tTrain Loss  0.1608\n",
            "Epoch(4/10)\t Batch(2800/3375): \tTrain Loss  0.1663\n",
            "Epoch(4/10)\t Batch(3000/3375): \tTrain Loss  0.1488\n",
            "Epoch(4/10)\t Batch(3200/3375): \tTrain Loss  0.1758\n",
            "End of Epoch: 4 Train Accuracy: 0.1037 Val Loss: 0.1665 Val Accuracy: 0.0953\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/3375): \tTrain Loss  0.1620\n",
            "Epoch(5/10)\t Batch(200/3375): \tTrain Loss  0.1431\n",
            "Epoch(5/10)\t Batch(400/3375): \tTrain Loss  0.1567\n",
            "Epoch(5/10)\t Batch(600/3375): \tTrain Loss  0.1379\n",
            "Epoch(5/10)\t Batch(800/3375): \tTrain Loss  0.1529\n",
            "Epoch(5/10)\t Batch(1000/3375): \tTrain Loss  0.1401\n",
            "Epoch(5/10)\t Batch(1200/3375): \tTrain Loss  0.1811\n",
            "Epoch(5/10)\t Batch(1400/3375): \tTrain Loss  0.1794\n",
            "Epoch(5/10)\t Batch(1600/3375): \tTrain Loss  0.1775\n",
            "Epoch(5/10)\t Batch(1800/3375): \tTrain Loss  0.1535\n",
            "Epoch(5/10)\t Batch(2000/3375): \tTrain Loss  0.1734\n",
            "Epoch(5/10)\t Batch(2200/3375): \tTrain Loss  0.1728\n",
            "Epoch(5/10)\t Batch(2400/3375): \tTrain Loss  0.1677\n",
            "Epoch(5/10)\t Batch(2600/3375): \tTrain Loss  0.1748\n",
            "Epoch(5/10)\t Batch(2800/3375): \tTrain Loss  0.1607\n",
            "Epoch(5/10)\t Batch(3000/3375): \tTrain Loss  0.1398\n",
            "Epoch(5/10)\t Batch(3200/3375): \tTrain Loss  0.1573\n",
            "End of Epoch: 5 Train Accuracy: 0.1068 Val Loss: 0.1668 Val Accuracy: 0.0942\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/3375): \tTrain Loss  0.1701\n",
            "Epoch(6/10)\t Batch(200/3375): \tTrain Loss  0.1844\n",
            "Epoch(6/10)\t Batch(400/3375): \tTrain Loss  0.1625\n",
            "Epoch(6/10)\t Batch(600/3375): \tTrain Loss  0.1913\n",
            "Epoch(6/10)\t Batch(800/3375): \tTrain Loss  0.1658\n",
            "Epoch(6/10)\t Batch(1000/3375): \tTrain Loss  0.1790\n",
            "Epoch(6/10)\t Batch(1200/3375): \tTrain Loss  0.1421\n",
            "Epoch(6/10)\t Batch(1400/3375): \tTrain Loss  0.1544\n",
            "Epoch(6/10)\t Batch(1600/3375): \tTrain Loss  0.1545\n",
            "Epoch(6/10)\t Batch(1800/3375): \tTrain Loss  0.1652\n",
            "Epoch(6/10)\t Batch(2000/3375): \tTrain Loss  0.1387\n",
            "Epoch(6/10)\t Batch(2200/3375): \tTrain Loss  0.1649\n",
            "Epoch(6/10)\t Batch(2400/3375): \tTrain Loss  0.1616\n",
            "Epoch(6/10)\t Batch(2600/3375): \tTrain Loss  0.1790\n",
            "Epoch(6/10)\t Batch(2800/3375): \tTrain Loss  0.1366\n",
            "Epoch(6/10)\t Batch(3000/3375): \tTrain Loss  0.1506\n",
            "Epoch(6/10)\t Batch(3200/3375): \tTrain Loss  0.1532\n",
            "End of Epoch: 6 Train Accuracy: 0.1095 Val Loss: 0.1661 Val Accuracy: 0.0953\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/3375): \tTrain Loss  0.1664\n",
            "Epoch(7/10)\t Batch(200/3375): \tTrain Loss  0.1750\n",
            "Epoch(7/10)\t Batch(400/3375): \tTrain Loss  0.1622\n",
            "Epoch(7/10)\t Batch(600/3375): \tTrain Loss  0.1883\n",
            "Epoch(7/10)\t Batch(800/3375): \tTrain Loss  0.1530\n",
            "Epoch(7/10)\t Batch(1000/3375): \tTrain Loss  0.1502\n",
            "Epoch(7/10)\t Batch(1200/3375): \tTrain Loss  0.1728\n",
            "Epoch(7/10)\t Batch(1400/3375): \tTrain Loss  0.1491\n",
            "Epoch(7/10)\t Batch(1600/3375): \tTrain Loss  0.1685\n",
            "Epoch(7/10)\t Batch(1800/3375): \tTrain Loss  0.1727\n",
            "Epoch(7/10)\t Batch(2000/3375): \tTrain Loss  0.1537\n",
            "Epoch(7/10)\t Batch(2200/3375): \tTrain Loss  0.1592\n",
            "Epoch(7/10)\t Batch(2400/3375): \tTrain Loss  0.1811\n",
            "Epoch(7/10)\t Batch(2600/3375): \tTrain Loss  0.1677\n",
            "Epoch(7/10)\t Batch(2800/3375): \tTrain Loss  0.1622\n",
            "Epoch(7/10)\t Batch(3000/3375): \tTrain Loss  0.1604\n",
            "Epoch(7/10)\t Batch(3200/3375): \tTrain Loss  0.1703\n",
            "End of Epoch: 7 Train Accuracy: 0.1120 Val Loss: 0.1655 Val Accuracy: 0.0983\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/3375): \tTrain Loss  0.1454\n",
            "Epoch(8/10)\t Batch(200/3375): \tTrain Loss  0.1566\n",
            "Epoch(8/10)\t Batch(400/3375): \tTrain Loss  0.1582\n",
            "Epoch(8/10)\t Batch(600/3375): \tTrain Loss  0.1672\n",
            "Epoch(8/10)\t Batch(800/3375): \tTrain Loss  0.1564\n",
            "Epoch(8/10)\t Batch(1000/3375): \tTrain Loss  0.1508\n",
            "Epoch(8/10)\t Batch(1200/3375): \tTrain Loss  0.1551\n",
            "Epoch(8/10)\t Batch(1400/3375): \tTrain Loss  0.1668\n",
            "Epoch(8/10)\t Batch(1600/3375): \tTrain Loss  0.1748\n",
            "Epoch(8/10)\t Batch(1800/3375): \tTrain Loss  0.1456\n",
            "Epoch(8/10)\t Batch(2000/3375): \tTrain Loss  0.1250\n",
            "Epoch(8/10)\t Batch(2200/3375): \tTrain Loss  0.1376\n",
            "Epoch(8/10)\t Batch(2400/3375): \tTrain Loss  0.1728\n",
            "Epoch(8/10)\t Batch(2600/3375): \tTrain Loss  0.1734\n",
            "Epoch(8/10)\t Batch(2800/3375): \tTrain Loss  0.1721\n",
            "Epoch(8/10)\t Batch(3000/3375): \tTrain Loss  0.1392\n",
            "Epoch(8/10)\t Batch(3200/3375): \tTrain Loss  0.1623\n",
            "End of Epoch: 8 Train Accuracy: 0.1155 Val Loss: 0.1653 Val Accuracy: 0.0978\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/3375): \tTrain Loss  0.1732\n",
            "Epoch(9/10)\t Batch(200/3375): \tTrain Loss  0.1585\n",
            "Epoch(9/10)\t Batch(400/3375): \tTrain Loss  0.1690\n",
            "Epoch(9/10)\t Batch(600/3375): \tTrain Loss  0.1559\n",
            "Epoch(9/10)\t Batch(800/3375): \tTrain Loss  0.1525\n",
            "Epoch(9/10)\t Batch(1000/3375): \tTrain Loss  0.1735\n",
            "Epoch(9/10)\t Batch(1200/3375): \tTrain Loss  0.1251\n",
            "Epoch(9/10)\t Batch(1400/3375): \tTrain Loss  0.1484\n",
            "Epoch(9/10)\t Batch(1600/3375): \tTrain Loss  0.1805\n",
            "Epoch(9/10)\t Batch(1800/3375): \tTrain Loss  0.1924\n",
            "Epoch(9/10)\t Batch(2000/3375): \tTrain Loss  0.1472\n",
            "Epoch(9/10)\t Batch(2200/3375): \tTrain Loss  0.1683\n",
            "Epoch(9/10)\t Batch(2400/3375): \tTrain Loss  0.1699\n",
            "Epoch(9/10)\t Batch(2600/3375): \tTrain Loss  0.1668\n",
            "Epoch(9/10)\t Batch(2800/3375): \tTrain Loss  0.1786\n",
            "Epoch(9/10)\t Batch(3000/3375): \tTrain Loss  0.1536\n",
            "Epoch(9/10)\t Batch(3200/3375): \tTrain Loss  0.1672\n",
            "End of Epoch: 9 Train Accuracy: 0.1190 Val Loss: 0.1656 Val Accuracy: 0.0960\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/3375): \tTrain Loss  0.1799\n",
            "Epoch(10/10)\t Batch(200/3375): \tTrain Loss  0.1842\n",
            "Epoch(10/10)\t Batch(400/3375): \tTrain Loss  0.1515\n",
            "Epoch(10/10)\t Batch(600/3375): \tTrain Loss  0.1720\n",
            "Epoch(10/10)\t Batch(800/3375): \tTrain Loss  0.1793\n",
            "Epoch(10/10)\t Batch(1000/3375): \tTrain Loss  0.1729\n",
            "Epoch(10/10)\t Batch(1200/3375): \tTrain Loss  0.1521\n",
            "Epoch(10/10)\t Batch(1400/3375): \tTrain Loss  0.1539\n",
            "Epoch(10/10)\t Batch(1600/3375): \tTrain Loss  0.1573\n",
            "Epoch(10/10)\t Batch(1800/3375): \tTrain Loss  0.1801\n",
            "Epoch(10/10)\t Batch(2000/3375): \tTrain Loss  0.1632\n",
            "Epoch(10/10)\t Batch(2200/3375): \tTrain Loss  0.1828\n",
            "Epoch(10/10)\t Batch(2400/3375): \tTrain Loss  0.1690\n",
            "Epoch(10/10)\t Batch(2600/3375): \tTrain Loss  0.1173\n",
            "Epoch(10/10)\t Batch(2800/3375): \tTrain Loss  0.1667\n",
            "Epoch(10/10)\t Batch(3000/3375): \tTrain Loss  0.1431\n",
            "Epoch(10/10)\t Batch(3200/3375): \tTrain Loss  0.1559\n",
            "End of Epoch: 10 Train Accuracy: 0.1214 Val Loss: 0.1653 Val Accuracy: 0.1015\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▂▃▄▄▅▆▆▇██</td></tr><tr><td>Train Loss</td><td>▁█▇▂▄▄█▄▇▇</td></tr><tr><td>Val Accuracy</td><td>▂▂▃▁▅▄▅▆▆▅█</td></tr><tr><td>Val Loss</td><td>█▇█▇▄▅▃▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.12143</td></tr><tr><td>Train Loss</td><td>0.17862</td></tr><tr><td>Val Accuracy</td><td>0.1015</td></tr><tr><td>Val Loss</td><td>0.16527</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_sgd|loss_mse|lr=0.0001|batch_16|act_tanh|hid_4|neurons_128|nrns_10|init_Random600</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/m0yfwaub' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/m0yfwaub</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_162002-m0yfwaub/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2n56gphl with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: mse\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_162313-2n56gphl</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/2n56gphl' target=\"_blank\">radiant-sweep-2</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/2n56gphl' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/2n56gphl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.0998 Val Loss: 0.0927 Val Accuracy: 0.1018\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  0.0909\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  0.0901\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  0.0898\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  0.0892\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  0.0816\n",
            "End of Epoch: 1 Train Accuracy: 0.2521 Val Loss: 0.0809 Val Accuracy: 0.2587\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  0.0812\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  0.0757\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  0.0744\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  0.0705\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  0.0761\n",
            "End of Epoch: 2 Train Accuracy: 0.2949 Val Loss: 0.0719 Val Accuracy: 0.2892\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  0.0767\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  0.0695\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  0.0714\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  0.0690\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  0.0732\n",
            "End of Epoch: 3 Train Accuracy: 0.3851 Val Loss: 0.0676 Val Accuracy: 0.3773\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  0.0708\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  0.0630\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  0.0669\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  0.0676\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  0.0667\n",
            "End of Epoch: 4 Train Accuracy: 0.4399 Val Loss: 0.0639 Val Accuracy: 0.4507\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  0.0633\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  0.0608\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  0.0589\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  0.0608\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  0.0582\n",
            "End of Epoch: 5 Train Accuracy: 0.5070 Val Loss: 0.0618 Val Accuracy: 0.5027\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  0.0670\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  0.0591\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  0.0608\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  0.0578\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  0.0547\n",
            "End of Epoch: 6 Train Accuracy: 0.5601 Val Loss: 0.0568 Val Accuracy: 0.5620\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  0.0594\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  0.0612\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  0.0595\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  0.0586\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  0.0548\n",
            "End of Epoch: 7 Train Accuracy: 0.5858 Val Loss: 0.0546 Val Accuracy: 0.5763\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  0.0595\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  0.0536\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  0.0488\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  0.0608\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  0.0503\n",
            "End of Epoch: 8 Train Accuracy: 0.6036 Val Loss: 0.0541 Val Accuracy: 0.6025\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  0.0499\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  0.0539\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  0.0576\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  0.0520\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  0.0571\n",
            "End of Epoch: 9 Train Accuracy: 0.6057 Val Loss: 0.0542 Val Accuracy: 0.6090\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  0.0590\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  0.0493\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  0.0505\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  0.0553\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  0.0584\n",
            "End of Epoch: 10 Train Accuracy: 0.6322 Val Loss: 0.0541 Val Accuracy: 0.6275\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▃▄▅▅▆▇▇███</td></tr><tr><td>Train Loss</td><td>█▅▅▄▂▃▃▃▁▂</td></tr><tr><td>Val Accuracy</td><td>▁▃▃▅▆▆▇▇███</td></tr><tr><td>Val Loss</td><td>█▆▄▃▃▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.63222</td></tr><tr><td>Train Loss</td><td>0.05727</td></tr><tr><td>Val Accuracy</td><td>0.6275</td></tr><tr><td>Val Loss</td><td>0.05411</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_adam|loss_mse|lr=0.001|batch_64|act_sigmoid|hid_5|neurons_128|nrns_10|init_Xavier407</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/2n56gphl' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/2n56gphl</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_162313-2n56gphl/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bri2qq5a with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_162534-bri2qq5a</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/bri2qq5a' target=\"_blank\">fallen-sweep-3</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/bri2qq5a' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/bri2qq5a</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0919 Val Loss: 25.1284 Val Accuracy: 0.0883\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/3375): \tTrain Loss  24.1771\n",
            "Epoch(1/5)\t Batch(200/3375): \tTrain Loss  25.9041\n",
            "Epoch(1/5)\t Batch(400/3375): \tTrain Loss  25.9041\n",
            "Epoch(1/5)\t Batch(600/3375): \tTrain Loss  24.1771\n",
            "Epoch(1/5)\t Batch(800/3375): \tTrain Loss  24.1771\n",
            "Epoch(1/5)\t Batch(1000/3375): \tTrain Loss  24.7830\n",
            "Epoch(1/5)\t Batch(1200/3375): \tTrain Loss  25.9041\n",
            "Epoch(1/5)\t Batch(1400/3375): \tTrain Loss  22.4461\n",
            "Epoch(1/5)\t Batch(1600/3375): \tTrain Loss  24.0694\n",
            "Epoch(1/5)\t Batch(1800/3375): \tTrain Loss  23.1237\n",
            "Epoch(1/5)\t Batch(2000/3375): \tTrain Loss  22.0604\n",
            "Epoch(1/5)\t Batch(2200/3375): \tTrain Loss  25.0616\n",
            "Epoch(1/5)\t Batch(2400/3375): \tTrain Loss  21.0593\n",
            "Epoch(1/5)\t Batch(2600/3375): \tTrain Loss  22.3892\n",
            "Epoch(1/5)\t Batch(2800/3375): \tTrain Loss  21.9689\n",
            "Epoch(1/5)\t Batch(3000/3375): \tTrain Loss  15.8249\n",
            "Epoch(1/5)\t Batch(3200/3375): \tTrain Loss  11.4364\n",
            "End of Epoch: 1 Train Accuracy: 0.1317 Val Loss: 17.2088 Val Accuracy: 0.1295\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/3375): \tTrain Loss  22.4377\n",
            "Epoch(2/5)\t Batch(200/3375): \tTrain Loss  17.5526\n",
            "Epoch(2/5)\t Batch(400/3375): \tTrain Loss  16.7064\n",
            "Epoch(2/5)\t Batch(600/3375): \tTrain Loss  17.5499\n",
            "Epoch(2/5)\t Batch(800/3375): \tTrain Loss  13.1716\n",
            "Epoch(2/5)\t Batch(1000/3375): \tTrain Loss  10.0062\n",
            "Epoch(2/5)\t Batch(1200/3375): \tTrain Loss  11.7619\n",
            "Epoch(2/5)\t Batch(1400/3375): \tTrain Loss  12.9485\n",
            "Epoch(2/5)\t Batch(1600/3375): \tTrain Loss  8.4703\n",
            "Epoch(2/5)\t Batch(1800/3375): \tTrain Loss  11.8306\n",
            "Epoch(2/5)\t Batch(2000/3375): \tTrain Loss  5.8635\n",
            "Epoch(2/5)\t Batch(2200/3375): \tTrain Loss  5.5182\n",
            "Epoch(2/5)\t Batch(2400/3375): \tTrain Loss  5.0033\n",
            "Epoch(2/5)\t Batch(2600/3375): \tTrain Loss  4.9980\n",
            "Epoch(2/5)\t Batch(2800/3375): \tTrain Loss  3.5983\n",
            "Epoch(2/5)\t Batch(3000/3375): \tTrain Loss  3.8243\n",
            "Epoch(2/5)\t Batch(3200/3375): \tTrain Loss  3.9462\n",
            "End of Epoch: 2 Train Accuracy: 0.1070 Val Loss: 4.2837 Val Accuracy: 0.1038\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/3375): \tTrain Loss  6.0402\n",
            "Epoch(3/5)\t Batch(200/3375): \tTrain Loss  4.7263\n",
            "Epoch(3/5)\t Batch(400/3375): \tTrain Loss  2.5642\n",
            "Epoch(3/5)\t Batch(600/3375): \tTrain Loss  3.6964\n",
            "Epoch(3/5)\t Batch(800/3375): \tTrain Loss  2.5360\n",
            "Epoch(3/5)\t Batch(1000/3375): \tTrain Loss  3.1018\n",
            "Epoch(3/5)\t Batch(1200/3375): \tTrain Loss  2.2974\n",
            "Epoch(3/5)\t Batch(1400/3375): \tTrain Loss  3.3551\n",
            "Epoch(3/5)\t Batch(1600/3375): \tTrain Loss  2.4204\n",
            "Epoch(3/5)\t Batch(1800/3375): \tTrain Loss  2.4633\n",
            "Epoch(3/5)\t Batch(2000/3375): \tTrain Loss  2.3743\n",
            "Epoch(3/5)\t Batch(2200/3375): \tTrain Loss  2.4527\n",
            "Epoch(3/5)\t Batch(2400/3375): \tTrain Loss  5.7152\n",
            "Epoch(3/5)\t Batch(2600/3375): \tTrain Loss  3.2508\n",
            "Epoch(3/5)\t Batch(2800/3375): \tTrain Loss  2.2794\n",
            "Epoch(3/5)\t Batch(3000/3375): \tTrain Loss  3.8111\n",
            "Epoch(3/5)\t Batch(3200/3375): \tTrain Loss  2.3301\n",
            "End of Epoch: 3 Train Accuracy: 0.1038 Val Loss: 2.9076 Val Accuracy: 0.0995\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/3375): \tTrain Loss  2.2515\n",
            "Epoch(4/5)\t Batch(200/3375): \tTrain Loss  3.0264\n",
            "Epoch(4/5)\t Batch(400/3375): \tTrain Loss  2.1930\n",
            "Epoch(4/5)\t Batch(600/3375): \tTrain Loss  4.6885\n",
            "Epoch(4/5)\t Batch(800/3375): \tTrain Loss  3.2421\n",
            "Epoch(4/5)\t Batch(1000/3375): \tTrain Loss  3.8482\n",
            "Epoch(4/5)\t Batch(1200/3375): \tTrain Loss  4.6806\n",
            "Epoch(4/5)\t Batch(1400/3375): \tTrain Loss  3.7020\n",
            "Epoch(4/5)\t Batch(1600/3375): \tTrain Loss  2.2981\n",
            "Epoch(4/5)\t Batch(1800/3375): \tTrain Loss  2.5603\n",
            "Epoch(4/5)\t Batch(2000/3375): \tTrain Loss  4.2605\n",
            "Epoch(4/5)\t Batch(2200/3375): \tTrain Loss  3.9909\n",
            "Epoch(4/5)\t Batch(2400/3375): \tTrain Loss  3.8787\n",
            "Epoch(4/5)\t Batch(2600/3375): \tTrain Loss  2.3884\n",
            "Epoch(4/5)\t Batch(2800/3375): \tTrain Loss  2.4277\n",
            "Epoch(4/5)\t Batch(3000/3375): \tTrain Loss  3.2297\n",
            "Epoch(4/5)\t Batch(3200/3375): \tTrain Loss  2.2841\n",
            "End of Epoch: 4 Train Accuracy: 0.1037 Val Loss: 2.6127 Val Accuracy: 0.0993\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/3375): \tTrain Loss  2.2794\n",
            "Epoch(5/5)\t Batch(200/3375): \tTrain Loss  2.2942\n",
            "Epoch(5/5)\t Batch(400/3375): \tTrain Loss  2.2642\n",
            "Epoch(5/5)\t Batch(600/3375): \tTrain Loss  2.3036\n",
            "Epoch(5/5)\t Batch(800/3375): \tTrain Loss  2.3420\n",
            "Epoch(5/5)\t Batch(1000/3375): \tTrain Loss  2.3042\n",
            "Epoch(5/5)\t Batch(1200/3375): \tTrain Loss  2.2389\n",
            "Epoch(5/5)\t Batch(1400/3375): \tTrain Loss  2.2988\n",
            "Epoch(5/5)\t Batch(1600/3375): \tTrain Loss  2.3043\n",
            "Epoch(5/5)\t Batch(1800/3375): \tTrain Loss  2.2976\n",
            "Epoch(5/5)\t Batch(2000/3375): \tTrain Loss  2.3021\n",
            "Epoch(5/5)\t Batch(2200/3375): \tTrain Loss  3.4616\n",
            "Epoch(5/5)\t Batch(2400/3375): \tTrain Loss  2.3055\n",
            "Epoch(5/5)\t Batch(2600/3375): \tTrain Loss  2.3434\n",
            "Epoch(5/5)\t Batch(2800/3375): \tTrain Loss  2.2998\n",
            "Epoch(5/5)\t Batch(3000/3375): \tTrain Loss  2.3001\n",
            "Epoch(5/5)\t Batch(3200/3375): \tTrain Loss  2.7306\n",
            "End of Epoch: 5 Train Accuracy: 0.1031 Val Loss: 2.4942 Val Accuracy: 0.0980\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁█▄▃▃▃</td></tr><tr><td>Train Loss</td><td>█▂▁▁▁</td></tr><tr><td>Val Accuracy</td><td>▁█▄▃▃▃</td></tr><tr><td>Val Loss</td><td>█▆▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.10309</td></tr><tr><td>Train Loss</td><td>2.20494</td></tr><tr><td>Val Accuracy</td><td>0.098</td></tr><tr><td>Val Loss</td><td>2.49423</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_rmsprop|loss_cross_entropy|lr=0.0001|batch_16|act_relu|hid_3|neurons_32|nrns_5|init_Random637</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/bri2qq5a' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/bri2qq5a</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_162534-bri2qq5a/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: icswrw34 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_162614-icswrw34</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/icswrw34' target=\"_blank\">light-sweep-4</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/icswrw34' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/icswrw34</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.0996 Val Loss: 2.3168 Val Accuracy: 0.1037\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/843): \tTrain Loss  2.2466\n",
            "Epoch(1/10)\t Batch(200/843): \tTrain Loss  2.1313\n",
            "Epoch(1/10)\t Batch(400/843): \tTrain Loss  2.0936\n",
            "Epoch(1/10)\t Batch(600/843): \tTrain Loss  2.0179\n",
            "Epoch(1/10)\t Batch(800/843): \tTrain Loss  1.9852\n",
            "End of Epoch: 1 Train Accuracy: 0.6395 Val Loss: 1.9671 Val Accuracy: 0.6337\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/843): \tTrain Loss  1.9987\n",
            "Epoch(2/10)\t Batch(200/843): \tTrain Loss  2.0240\n",
            "Epoch(2/10)\t Batch(400/843): \tTrain Loss  1.9703\n",
            "Epoch(2/10)\t Batch(600/843): \tTrain Loss  1.9747\n",
            "Epoch(2/10)\t Batch(800/843): \tTrain Loss  1.9573\n",
            "End of Epoch: 2 Train Accuracy: 0.6509 Val Loss: 1.9296 Val Accuracy: 0.6482\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/843): \tTrain Loss  1.9517\n",
            "Epoch(3/10)\t Batch(200/843): \tTrain Loss  1.9478\n",
            "Epoch(3/10)\t Batch(400/843): \tTrain Loss  1.9850\n",
            "Epoch(3/10)\t Batch(600/843): \tTrain Loss  1.8620\n",
            "Epoch(3/10)\t Batch(800/843): \tTrain Loss  1.8911\n",
            "End of Epoch: 3 Train Accuracy: 0.6495 Val Loss: 1.8980 Val Accuracy: 0.6475\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/843): \tTrain Loss  1.8576\n",
            "Epoch(4/10)\t Batch(200/843): \tTrain Loss  1.8849\n",
            "Epoch(4/10)\t Batch(400/843): \tTrain Loss  1.8475\n",
            "Epoch(4/10)\t Batch(600/843): \tTrain Loss  1.8878\n",
            "Epoch(4/10)\t Batch(800/843): \tTrain Loss  1.8742\n",
            "End of Epoch: 4 Train Accuracy: 0.6710 Val Loss: 1.8840 Val Accuracy: 0.6730\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/843): \tTrain Loss  1.8763\n",
            "Epoch(5/10)\t Batch(200/843): \tTrain Loss  1.8608\n",
            "Epoch(5/10)\t Batch(400/843): \tTrain Loss  1.8808\n",
            "Epoch(5/10)\t Batch(600/843): \tTrain Loss  1.8770\n",
            "Epoch(5/10)\t Batch(800/843): \tTrain Loss  1.9016\n",
            "End of Epoch: 5 Train Accuracy: 0.6529 Val Loss: 1.8850 Val Accuracy: 0.6467\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/843): \tTrain Loss  1.9007\n",
            "Epoch(6/10)\t Batch(200/843): \tTrain Loss  1.8249\n",
            "Epoch(6/10)\t Batch(400/843): \tTrain Loss  1.9011\n",
            "Epoch(6/10)\t Batch(600/843): \tTrain Loss  1.8938\n",
            "Epoch(6/10)\t Batch(800/843): \tTrain Loss  1.8849\n",
            "End of Epoch: 6 Train Accuracy: 0.6590 Val Loss: 1.8827 Val Accuracy: 0.6553\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/843): \tTrain Loss  1.9294\n",
            "Epoch(7/10)\t Batch(200/843): \tTrain Loss  1.9090\n",
            "Epoch(7/10)\t Batch(400/843): \tTrain Loss  1.8427\n",
            "Epoch(7/10)\t Batch(600/843): \tTrain Loss  1.9258\n",
            "Epoch(7/10)\t Batch(800/843): \tTrain Loss  1.8713\n",
            "End of Epoch: 7 Train Accuracy: 0.6504 Val Loss: 1.8810 Val Accuracy: 0.6430\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/843): \tTrain Loss  1.9354\n",
            "Epoch(8/10)\t Batch(200/843): \tTrain Loss  1.8754\n",
            "Epoch(8/10)\t Batch(400/843): \tTrain Loss  1.9148\n",
            "Epoch(8/10)\t Batch(600/843): \tTrain Loss  1.8706\n",
            "Epoch(8/10)\t Batch(800/843): \tTrain Loss  1.8808\n",
            "End of Epoch: 8 Train Accuracy: 0.6718 Val Loss: 1.8823 Val Accuracy: 0.6683\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/843): \tTrain Loss  1.8536\n",
            "Epoch(9/10)\t Batch(200/843): \tTrain Loss  1.8753\n",
            "Epoch(9/10)\t Batch(400/843): \tTrain Loss  1.8098\n",
            "Epoch(9/10)\t Batch(600/843): \tTrain Loss  1.8538\n",
            "Epoch(9/10)\t Batch(800/843): \tTrain Loss  1.8485\n",
            "End of Epoch: 9 Train Accuracy: 0.6465 Val Loss: 1.8825 Val Accuracy: 0.6460\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/843): \tTrain Loss  1.8880\n",
            "Epoch(10/10)\t Batch(200/843): \tTrain Loss  1.8686\n",
            "Epoch(10/10)\t Batch(400/843): \tTrain Loss  1.8720\n",
            "Epoch(10/10)\t Batch(600/843): \tTrain Loss  1.9256\n",
            "Epoch(10/10)\t Batch(800/843): \tTrain Loss  1.8454\n",
            "End of Epoch: 10 Train Accuracy: 0.6766 Val Loss: 1.8832 Val Accuracy: 0.6750\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁██████████</td></tr><tr><td>Train Loss</td><td>█▆▅▁▄▄▁▅▃▃</td></tr><tr><td>Val Accuracy</td><td>▁▇█████████</td></tr><tr><td>Val Loss</td><td>█▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Train Accuracy</td><td>0.67665</td></tr><tr><td>Train Loss</td><td>1.84585</td></tr><tr><td>Val Accuracy</td><td>0.675</td></tr><tr><td>Val Loss</td><td>1.88319</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_adam|loss_cross_entropy|lr=0.001|batch_64|act_sigmoid|hid_4|neurons_128|nrns_10|init_Xavier81</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/icswrw34' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/icswrw34</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_162614-icswrw34/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tck4af09 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_162801-tck4af09</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/tck4af09' target=\"_blank\">golden-sweep-5</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/tck4af09' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/tck4af09</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Sigmoid\n",
            "Start of Training: 1 Train Accuracy: 0.0986 Val Loss: 2.3662 Val Accuracy: 0.1122\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/843): \tTrain Loss  2.3444\n",
            "Epoch(1/5)\t Batch(200/843): \tTrain Loss  2.1290\n",
            "Epoch(1/5)\t Batch(400/843): \tTrain Loss  1.9630\n",
            "Epoch(1/5)\t Batch(600/843): \tTrain Loss  1.8230\n",
            "Epoch(1/5)\t Batch(800/843): \tTrain Loss  1.7418\n",
            "End of Epoch: 1 Train Accuracy: 0.7634 Val Loss: 1.7583 Val Accuracy: 0.7570\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/843): \tTrain Loss  1.7187\n",
            "Epoch(2/5)\t Batch(200/843): \tTrain Loss  1.7168\n",
            "Epoch(2/5)\t Batch(400/843): \tTrain Loss  1.7261\n",
            "Epoch(2/5)\t Batch(600/843): \tTrain Loss  1.6263\n",
            "Epoch(2/5)\t Batch(800/843): \tTrain Loss  1.6391\n",
            "End of Epoch: 2 Train Accuracy: 0.8491 Val Loss: 1.6877 Val Accuracy: 0.8367\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/843): \tTrain Loss  1.6741\n",
            "Epoch(3/5)\t Batch(200/843): \tTrain Loss  1.6075\n",
            "Epoch(3/5)\t Batch(400/843): \tTrain Loss  1.6715\n",
            "Epoch(3/5)\t Batch(600/843): \tTrain Loss  1.6587\n",
            "Epoch(3/5)\t Batch(800/843): \tTrain Loss  1.6603\n",
            "End of Epoch: 3 Train Accuracy: 0.8664 Val Loss: 1.6635 Val Accuracy: 0.8487\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/843): \tTrain Loss  1.6770\n",
            "Epoch(4/5)\t Batch(200/843): \tTrain Loss  1.6109\n",
            "Epoch(4/5)\t Batch(400/843): \tTrain Loss  1.5949\n",
            "Epoch(4/5)\t Batch(600/843): \tTrain Loss  1.6336\n",
            "Epoch(4/5)\t Batch(800/843): \tTrain Loss  1.6546\n",
            "End of Epoch: 4 Train Accuracy: 0.8759 Val Loss: 1.6397 Val Accuracy: 0.8560\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/843): \tTrain Loss  1.5660\n",
            "Epoch(5/5)\t Batch(200/843): \tTrain Loss  1.6375\n",
            "Epoch(5/5)\t Batch(400/843): \tTrain Loss  1.6007\n",
            "Epoch(5/5)\t Batch(600/843): \tTrain Loss  1.6032\n",
            "Epoch(5/5)\t Batch(800/843): \tTrain Loss  1.6733\n",
            "End of Epoch: 5 Train Accuracy: 0.8899 Val Loss: 1.6249 Val Accuracy: 0.8690\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁▇████</td></tr><tr><td>Train Loss</td><td>█▄▄▃▁</td></tr><tr><td>Val Accuracy</td><td>▁▇████</td></tr><tr><td>Val Loss</td><td>█▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.88991</td></tr><tr><td>Train Loss</td><td>1.60051</td></tr><tr><td>Val Accuracy</td><td>0.869</td></tr><tr><td>Val Loss</td><td>1.6249</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nadam|loss_cross_entropy|lr=0.001|batch_64|act_sigmoid|hid_5|neurons_128|nrns_5|init_Xavier271</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/tck4af09' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/tck4af09</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_162801-tck4af09/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: csqysze6 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thid_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnepoch: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250315_162911-csqysze6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/csqysze6' target=\"_blank\">hopeful-sweep-6</a></strong> to <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/sweeps/7bcahxlc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/csqysze6' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/csqysze6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is ReLu\n",
            "Start of Training: 1 Train Accuracy: 0.0557 Val Loss: 2.3025 Val Accuracy: 0.0563\n",
            "-------x-------\n",
            "Epoch(1/5)\t Batch( 0/843): \tTrain Loss  2.3011\n",
            "Epoch(1/5)\t Batch(200/843): \tTrain Loss  1.7322\n",
            "Epoch(1/5)\t Batch(400/843): \tTrain Loss  1.7720\n",
            "Epoch(1/5)\t Batch(600/843): \tTrain Loss  1.6675\n",
            "Epoch(1/5)\t Batch(800/843): \tTrain Loss  1.6841\n",
            "End of Epoch: 1 Train Accuracy: 0.8597 Val Loss: 1.6568 Val Accuracy: 0.8587\n",
            "-------x-------\n",
            "Epoch(2/5)\t Batch( 0/843): \tTrain Loss  1.6391\n",
            "Epoch(2/5)\t Batch(200/843): \tTrain Loss  1.6010\n",
            "Epoch(2/5)\t Batch(400/843): \tTrain Loss  1.6849\n",
            "Epoch(2/5)\t Batch(600/843): \tTrain Loss  1.6407\n",
            "Epoch(2/5)\t Batch(800/843): \tTrain Loss  1.5958\n",
            "End of Epoch: 2 Train Accuracy: 0.8804 Val Loss: 1.6281 Val Accuracy: 0.8703\n",
            "-------x-------\n",
            "Epoch(3/5)\t Batch( 0/843): \tTrain Loss  1.5936\n",
            "Epoch(3/5)\t Batch(200/843): \tTrain Loss  1.6777\n",
            "Epoch(3/5)\t Batch(400/843): \tTrain Loss  1.6070\n",
            "Epoch(3/5)\t Batch(600/843): \tTrain Loss  1.6767\n",
            "Epoch(3/5)\t Batch(800/843): \tTrain Loss  1.6305\n",
            "End of Epoch: 3 Train Accuracy: 0.8833 Val Loss: 1.6216 Val Accuracy: 0.8812\n",
            "-------x-------\n",
            "Epoch(4/5)\t Batch( 0/843): \tTrain Loss  1.6195\n",
            "Epoch(4/5)\t Batch(200/843): \tTrain Loss  1.5906\n",
            "Epoch(4/5)\t Batch(400/843): \tTrain Loss  1.6315\n",
            "Epoch(4/5)\t Batch(600/843): \tTrain Loss  1.6107\n",
            "Epoch(4/5)\t Batch(800/843): \tTrain Loss  1.5510\n",
            "End of Epoch: 4 Train Accuracy: 0.8818 Val Loss: 1.6260 Val Accuracy: 0.8727\n",
            "-------x-------\n",
            "Epoch(5/5)\t Batch( 0/843): \tTrain Loss  1.6217\n",
            "Epoch(5/5)\t Batch(200/843): \tTrain Loss  1.6422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: could not find agent tt8dtxmg during agentHeartbeat (<Response [404]>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch(5/5)\t Batch(400/843): \tTrain Loss  1.6003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-596 (_heartbeat):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/internal/internal_api.py\", line 3144, in agent_heartbeat\n",
            "    response = self.gql(\n",
            "               ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/internal/internal_api.py\", line 370, in gql\n",
            "    ret = self._retry_gql(\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/retry.py\", line 134, in __call__\n",
            "    result = self._call_fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/internal/internal_api.py\", line 398, in execute\n",
            "    return self.client.execute(*args, **kwargs)  # type: ignore\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py\", line 52, in execute\n",
            "    result = self._get_result(document, *args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/vendor/gql-0.2.0/wandb_gql/client.py\", line 60, in _get_result\n",
            "    return self.transport.execute(document, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/gql_request.py\", line 59, in execute\n",
            "    request.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/models.py\", line 1024, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://api.wandb.ai/graphql\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 165, in _heartbeat\n",
            "    commands = self._api.agent_heartbeat(self._agent_id, {}, run_status)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/apis/internal.py\", line 161, in agent_heartbeat\n",
            "    return self.api.agent_heartbeat(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/internal/internal_api.py\", line 3155, in agent_heartbeat\n",
            "    message = ast.literal_eval(e.args[0])[\"message\"]\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ast.py\", line 64, in literal_eval\n",
            "    node_or_string = parse(node_or_string.lstrip(\" \\t\"), mode='eval')\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ast.py\", line 50, in parse\n",
            "    return compile(source, filename, mode, flags,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<unknown>\", line 1\n",
            "    404 Client Error: Not Found for url: https://api.wandb.ai/graphql\n",
            "        ^^^^^^\n",
            "SyntaxError: invalid syntax\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch(5/5)\t Batch(600/843): \tTrain Loss  1.6085\n",
            "Epoch(5/5)\t Batch(800/843): \tTrain Loss  1.6162\n",
            "End of Epoch: 5 Train Accuracy: 0.8950 Val Loss: 1.6068 Val Accuracy: 0.8797\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▄▄▅▅▇▇██</td></tr><tr><td>Train Accuracy</td><td>▁█████</td></tr><tr><td>Train Loss</td><td>█▄▁▃▃</td></tr><tr><td>Val Accuracy</td><td>▁█████</td></tr><tr><td>Val Loss</td><td>█▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.89496</td></tr><tr><td>Train Loss</td><td>1.5657</td></tr><tr><td>Val Accuracy</td><td>0.87967</td></tr><tr><td>Val Loss</td><td>1.60677</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">opt_nadam|loss_cross_entropy|lr=0.001|batch_64|act_relu|hid_5|neurons_128|nrns_5|init_Xavier135</strong> at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/csqysze6' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1/runs/csqysze6</a><br> View project at: <a href='https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1' target=\"_blank\">https://wandb.ai/da24s016-indian-institute-of-technology-madras/da6401-asg1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250315_162911-csqysze6/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ],
      "source": [
        "wandb.login(key='e6b43dd118f9a14e83fe12c597ad8d06bdfed432')\n",
        "\n",
        "\n",
        "# Sweep configuration\n",
        "sweep_config = {\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\"goal\": \"maximize\", \"name\": \"Val Accuracy\"},\n",
        "    \"parameters\": {\n",
        "        \"batch_size\": {\"values\": [16, 32, 64]},\n",
        "        \"decay\": {\"values\": [0, 0.5, 0.0005]},\n",
        "        \"hid_layers\": {\"values\": [3, 4, 5]},\n",
        "        \"hid_size\": {\"values\": [32, 64, 128]},\n",
        "        \"nepoch\": {\"values\": [5, 10]},\n",
        "        \"activation\": {\"values\": [\"relu\", \"tanh\", \"sigmoid\"]},\n",
        "        \"init\": {\"values\": [\"Xavier\", \"Random\"]},\n",
        "        \"optimizer\": {\"values\": [\"sgd\", \"momentum\", \"rmsprop\", \"nag\", \"adam\", \"nadam\"]},\n",
        "        \"loss\": {\"values\": [\"mse\", \"cross_entropy\"]},\n",
        "        \"lr\": {\"values\": [0.0001, 0.001]},\n",
        "    },\n",
        "}\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"da6401-asg1\")\n",
        "wandb.agent(sweep_id, function=train, count = 200)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YMRj5dpxGUL6",
        "outputId": "f2d1cf8a-901c-4274-81ad-dcb070f5609f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation used is Tanh\n",
            "Begining of Epoch: 1 Train Accuracy: 0.1426 Validation Accuracy: 0.1532\n",
            "-------x-------\n",
            "Epoch(1/10)\t Batch( 0/3375): \tTrain Loss  17.0848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-4f78d507ffd0>:36: RuntimeWarning: overflow encountered in matmul\n",
            "  dzdx = d_out@self.weight.T\n",
            "<ipython-input-2-4f78d507ffd0>:63: RuntimeWarning: invalid value encountered in multiply\n",
            "  return dout * (1 - self.out**2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch(1/10)\t Batch(200/3375): \tTrain Loss  nan\n",
            "Epoch(1/10)\t Batch(400/3375): \tTrain Loss  nan\n",
            "Epoch(1/10)\t Batch(600/3375): \tTrain Loss  nan\n",
            "Epoch(1/10)\t Batch(800/3375): \tTrain Loss  nan\n",
            "Epoch(1/10)\t Batch(1000/3375): \tTrain Loss  nan\n",
            "Epoch(1/10)\t Batch(1200/3375): \tTrain Loss  nan\n",
            "Epoch(1/10)\t Batch(1400/3375): \tTrain Loss  nan\n",
            "Epoch(1/10)\t Batch(1600/3375): \tTrain Loss  nan\n",
            "Epoch(1/10)\t Batch(1800/3375): \tTrain Loss  nan\n",
            "Epoch(1/10)\t Batch(2000/3375): \tTrain Loss  nan\n",
            "Epoch(1/10)\t Batch(2200/3375): \tTrain Loss  nan\n",
            "Epoch(1/10)\t Batch(2400/3375): \tTrain Loss  nan\n",
            "Epoch(1/10)\t Batch(2600/3375): \tTrain Loss  nan\n",
            "Epoch(1/10)\t Batch(2800/3375): \tTrain Loss  nan\n",
            "Epoch(1/10)\t Batch(3000/3375): \tTrain Loss  nan\n",
            "Epoch(1/10)\t Batch(3200/3375): \tTrain Loss  nan\n",
            "Begining of Epoch: 2 Train Accuracy: 0.0996 Validation Accuracy: 0.1037\n",
            "-------x-------\n",
            "Epoch(2/10)\t Batch( 0/3375): \tTrain Loss  nan\n",
            "Epoch(2/10)\t Batch(200/3375): \tTrain Loss  nan\n",
            "Epoch(2/10)\t Batch(400/3375): \tTrain Loss  nan\n",
            "Epoch(2/10)\t Batch(600/3375): \tTrain Loss  nan\n",
            "Epoch(2/10)\t Batch(800/3375): \tTrain Loss  nan\n",
            "Epoch(2/10)\t Batch(1000/3375): \tTrain Loss  nan\n",
            "Epoch(2/10)\t Batch(1200/3375): \tTrain Loss  nan\n",
            "Epoch(2/10)\t Batch(1400/3375): \tTrain Loss  nan\n",
            "Epoch(2/10)\t Batch(1600/3375): \tTrain Loss  nan\n",
            "Epoch(2/10)\t Batch(1800/3375): \tTrain Loss  nan\n",
            "Epoch(2/10)\t Batch(2000/3375): \tTrain Loss  nan\n",
            "Epoch(2/10)\t Batch(2200/3375): \tTrain Loss  nan\n",
            "Epoch(2/10)\t Batch(2400/3375): \tTrain Loss  nan\n",
            "Epoch(2/10)\t Batch(2600/3375): \tTrain Loss  nan\n",
            "Epoch(2/10)\t Batch(2800/3375): \tTrain Loss  nan\n",
            "Epoch(2/10)\t Batch(3000/3375): \tTrain Loss  nan\n",
            "Epoch(2/10)\t Batch(3200/3375): \tTrain Loss  nan\n",
            "Begining of Epoch: 3 Train Accuracy: 0.0996 Validation Accuracy: 0.1037\n",
            "-------x-------\n",
            "Epoch(3/10)\t Batch( 0/3375): \tTrain Loss  nan\n",
            "Epoch(3/10)\t Batch(200/3375): \tTrain Loss  nan\n",
            "Epoch(3/10)\t Batch(400/3375): \tTrain Loss  nan\n",
            "Epoch(3/10)\t Batch(600/3375): \tTrain Loss  nan\n",
            "Epoch(3/10)\t Batch(800/3375): \tTrain Loss  nan\n",
            "Epoch(3/10)\t Batch(1000/3375): \tTrain Loss  nan\n",
            "Epoch(3/10)\t Batch(1200/3375): \tTrain Loss  nan\n",
            "Epoch(3/10)\t Batch(1400/3375): \tTrain Loss  nan\n",
            "Epoch(3/10)\t Batch(1600/3375): \tTrain Loss  nan\n",
            "Epoch(3/10)\t Batch(1800/3375): \tTrain Loss  nan\n",
            "Epoch(3/10)\t Batch(2000/3375): \tTrain Loss  nan\n",
            "Epoch(3/10)\t Batch(2200/3375): \tTrain Loss  nan\n",
            "Epoch(3/10)\t Batch(2400/3375): \tTrain Loss  nan\n",
            "Epoch(3/10)\t Batch(2600/3375): \tTrain Loss  nan\n",
            "Epoch(3/10)\t Batch(2800/3375): \tTrain Loss  nan\n",
            "Epoch(3/10)\t Batch(3000/3375): \tTrain Loss  nan\n",
            "Epoch(3/10)\t Batch(3200/3375): \tTrain Loss  nan\n",
            "Begining of Epoch: 4 Train Accuracy: 0.0996 Validation Accuracy: 0.1037\n",
            "-------x-------\n",
            "Epoch(4/10)\t Batch( 0/3375): \tTrain Loss  nan\n",
            "Epoch(4/10)\t Batch(200/3375): \tTrain Loss  nan\n",
            "Epoch(4/10)\t Batch(400/3375): \tTrain Loss  nan\n",
            "Epoch(4/10)\t Batch(600/3375): \tTrain Loss  nan\n",
            "Epoch(4/10)\t Batch(800/3375): \tTrain Loss  nan\n",
            "Epoch(4/10)\t Batch(1000/3375): \tTrain Loss  nan\n",
            "Epoch(4/10)\t Batch(1200/3375): \tTrain Loss  nan\n",
            "Epoch(4/10)\t Batch(1400/3375): \tTrain Loss  nan\n",
            "Epoch(4/10)\t Batch(1600/3375): \tTrain Loss  nan\n",
            "Epoch(4/10)\t Batch(1800/3375): \tTrain Loss  nan\n",
            "Epoch(4/10)\t Batch(2000/3375): \tTrain Loss  nan\n",
            "Epoch(4/10)\t Batch(2200/3375): \tTrain Loss  nan\n",
            "Epoch(4/10)\t Batch(2400/3375): \tTrain Loss  nan\n",
            "Epoch(4/10)\t Batch(2600/3375): \tTrain Loss  nan\n",
            "Epoch(4/10)\t Batch(2800/3375): \tTrain Loss  nan\n",
            "Epoch(4/10)\t Batch(3000/3375): \tTrain Loss  nan\n",
            "Epoch(4/10)\t Batch(3200/3375): \tTrain Loss  nan\n",
            "Begining of Epoch: 5 Train Accuracy: 0.0996 Validation Accuracy: 0.1037\n",
            "-------x-------\n",
            "Epoch(5/10)\t Batch( 0/3375): \tTrain Loss  nan\n",
            "Epoch(5/10)\t Batch(200/3375): \tTrain Loss  nan\n",
            "Epoch(5/10)\t Batch(400/3375): \tTrain Loss  nan\n",
            "Epoch(5/10)\t Batch(600/3375): \tTrain Loss  nan\n",
            "Epoch(5/10)\t Batch(800/3375): \tTrain Loss  nan\n",
            "Epoch(5/10)\t Batch(1000/3375): \tTrain Loss  nan\n",
            "Epoch(5/10)\t Batch(1200/3375): \tTrain Loss  nan\n",
            "Epoch(5/10)\t Batch(1400/3375): \tTrain Loss  nan\n",
            "Epoch(5/10)\t Batch(1600/3375): \tTrain Loss  nan\n",
            "Epoch(5/10)\t Batch(1800/3375): \tTrain Loss  nan\n",
            "Epoch(5/10)\t Batch(2000/3375): \tTrain Loss  nan\n",
            "Epoch(5/10)\t Batch(2200/3375): \tTrain Loss  nan\n",
            "Epoch(5/10)\t Batch(2400/3375): \tTrain Loss  nan\n",
            "Epoch(5/10)\t Batch(2600/3375): \tTrain Loss  nan\n",
            "Epoch(5/10)\t Batch(2800/3375): \tTrain Loss  nan\n",
            "Epoch(5/10)\t Batch(3000/3375): \tTrain Loss  nan\n",
            "Epoch(5/10)\t Batch(3200/3375): \tTrain Loss  nan\n",
            "Begining of Epoch: 6 Train Accuracy: 0.0996 Validation Accuracy: 0.1037\n",
            "-------x-------\n",
            "Epoch(6/10)\t Batch( 0/3375): \tTrain Loss  nan\n",
            "Epoch(6/10)\t Batch(200/3375): \tTrain Loss  nan\n",
            "Epoch(6/10)\t Batch(400/3375): \tTrain Loss  nan\n",
            "Epoch(6/10)\t Batch(600/3375): \tTrain Loss  nan\n",
            "Epoch(6/10)\t Batch(800/3375): \tTrain Loss  nan\n",
            "Epoch(6/10)\t Batch(1000/3375): \tTrain Loss  nan\n",
            "Epoch(6/10)\t Batch(1200/3375): \tTrain Loss  nan\n",
            "Epoch(6/10)\t Batch(1400/3375): \tTrain Loss  nan\n",
            "Epoch(6/10)\t Batch(1600/3375): \tTrain Loss  nan\n",
            "Epoch(6/10)\t Batch(1800/3375): \tTrain Loss  nan\n",
            "Epoch(6/10)\t Batch(2000/3375): \tTrain Loss  nan\n",
            "Epoch(6/10)\t Batch(2200/3375): \tTrain Loss  nan\n",
            "Epoch(6/10)\t Batch(2400/3375): \tTrain Loss  nan\n",
            "Epoch(6/10)\t Batch(2600/3375): \tTrain Loss  nan\n",
            "Epoch(6/10)\t Batch(2800/3375): \tTrain Loss  nan\n",
            "Epoch(6/10)\t Batch(3000/3375): \tTrain Loss  nan\n",
            "Epoch(6/10)\t Batch(3200/3375): \tTrain Loss  nan\n",
            "Begining of Epoch: 7 Train Accuracy: 0.0996 Validation Accuracy: 0.1037\n",
            "-------x-------\n",
            "Epoch(7/10)\t Batch( 0/3375): \tTrain Loss  nan\n",
            "Epoch(7/10)\t Batch(200/3375): \tTrain Loss  nan\n",
            "Epoch(7/10)\t Batch(400/3375): \tTrain Loss  nan\n",
            "Epoch(7/10)\t Batch(600/3375): \tTrain Loss  nan\n",
            "Epoch(7/10)\t Batch(800/3375): \tTrain Loss  nan\n",
            "Epoch(7/10)\t Batch(1000/3375): \tTrain Loss  nan\n",
            "Epoch(7/10)\t Batch(1200/3375): \tTrain Loss  nan\n",
            "Epoch(7/10)\t Batch(1400/3375): \tTrain Loss  nan\n",
            "Epoch(7/10)\t Batch(1600/3375): \tTrain Loss  nan\n",
            "Epoch(7/10)\t Batch(1800/3375): \tTrain Loss  nan\n",
            "Epoch(7/10)\t Batch(2000/3375): \tTrain Loss  nan\n",
            "Epoch(7/10)\t Batch(2200/3375): \tTrain Loss  nan\n",
            "Epoch(7/10)\t Batch(2400/3375): \tTrain Loss  nan\n",
            "Epoch(7/10)\t Batch(2600/3375): \tTrain Loss  nan\n",
            "Epoch(7/10)\t Batch(2800/3375): \tTrain Loss  nan\n",
            "Epoch(7/10)\t Batch(3000/3375): \tTrain Loss  nan\n",
            "Epoch(7/10)\t Batch(3200/3375): \tTrain Loss  nan\n",
            "Begining of Epoch: 8 Train Accuracy: 0.0996 Validation Accuracy: 0.1037\n",
            "-------x-------\n",
            "Epoch(8/10)\t Batch( 0/3375): \tTrain Loss  nan\n",
            "Epoch(8/10)\t Batch(200/3375): \tTrain Loss  nan\n",
            "Epoch(8/10)\t Batch(400/3375): \tTrain Loss  nan\n",
            "Epoch(8/10)\t Batch(600/3375): \tTrain Loss  nan\n",
            "Epoch(8/10)\t Batch(800/3375): \tTrain Loss  nan\n",
            "Epoch(8/10)\t Batch(1000/3375): \tTrain Loss  nan\n",
            "Epoch(8/10)\t Batch(1200/3375): \tTrain Loss  nan\n",
            "Epoch(8/10)\t Batch(1400/3375): \tTrain Loss  nan\n",
            "Epoch(8/10)\t Batch(1600/3375): \tTrain Loss  nan\n",
            "Epoch(8/10)\t Batch(1800/3375): \tTrain Loss  nan\n",
            "Epoch(8/10)\t Batch(2000/3375): \tTrain Loss  nan\n",
            "Epoch(8/10)\t Batch(2200/3375): \tTrain Loss  nan\n",
            "Epoch(8/10)\t Batch(2400/3375): \tTrain Loss  nan\n",
            "Epoch(8/10)\t Batch(2600/3375): \tTrain Loss  nan\n",
            "Epoch(8/10)\t Batch(2800/3375): \tTrain Loss  nan\n",
            "Epoch(8/10)\t Batch(3000/3375): \tTrain Loss  nan\n",
            "Epoch(8/10)\t Batch(3200/3375): \tTrain Loss  nan\n",
            "Begining of Epoch: 9 Train Accuracy: 0.0996 Validation Accuracy: 0.1037\n",
            "-------x-------\n",
            "Epoch(9/10)\t Batch( 0/3375): \tTrain Loss  nan\n",
            "Epoch(9/10)\t Batch(200/3375): \tTrain Loss  nan\n",
            "Epoch(9/10)\t Batch(400/3375): \tTrain Loss  nan\n",
            "Epoch(9/10)\t Batch(600/3375): \tTrain Loss  nan\n",
            "Epoch(9/10)\t Batch(800/3375): \tTrain Loss  nan\n",
            "Epoch(9/10)\t Batch(1000/3375): \tTrain Loss  nan\n",
            "Epoch(9/10)\t Batch(1200/3375): \tTrain Loss  nan\n",
            "Epoch(9/10)\t Batch(1400/3375): \tTrain Loss  nan\n",
            "Epoch(9/10)\t Batch(1600/3375): \tTrain Loss  nan\n",
            "Epoch(9/10)\t Batch(1800/3375): \tTrain Loss  nan\n",
            "Epoch(9/10)\t Batch(2000/3375): \tTrain Loss  nan\n",
            "Epoch(9/10)\t Batch(2200/3375): \tTrain Loss  nan\n",
            "Epoch(9/10)\t Batch(2400/3375): \tTrain Loss  nan\n",
            "Epoch(9/10)\t Batch(2600/3375): \tTrain Loss  nan\n",
            "Epoch(9/10)\t Batch(2800/3375): \tTrain Loss  nan\n",
            "Epoch(9/10)\t Batch(3000/3375): \tTrain Loss  nan\n",
            "Epoch(9/10)\t Batch(3200/3375): \tTrain Loss  nan\n",
            "Begining of Epoch: 10 Train Accuracy: 0.0996 Validation Accuracy: 0.1037\n",
            "-------x-------\n",
            "Epoch(10/10)\t Batch( 0/3375): \tTrain Loss  nan\n",
            "Epoch(10/10)\t Batch(200/3375): \tTrain Loss  nan\n",
            "Epoch(10/10)\t Batch(400/3375): \tTrain Loss  nan\n",
            "Epoch(10/10)\t Batch(600/3375): \tTrain Loss  nan\n",
            "Epoch(10/10)\t Batch(800/3375): \tTrain Loss  nan\n",
            "Epoch(10/10)\t Batch(1000/3375): \tTrain Loss  nan\n",
            "Epoch(10/10)\t Batch(1200/3375): \tTrain Loss  nan\n",
            "Epoch(10/10)\t Batch(1400/3375): \tTrain Loss  nan\n",
            "Epoch(10/10)\t Batch(1600/3375): \tTrain Loss  nan\n",
            "Epoch(10/10)\t Batch(1800/3375): \tTrain Loss  nan\n",
            "Epoch(10/10)\t Batch(2000/3375): \tTrain Loss  nan\n",
            "Epoch(10/10)\t Batch(2200/3375): \tTrain Loss  nan\n",
            "Epoch(10/10)\t Batch(2400/3375): \tTrain Loss  nan\n",
            "Epoch(10/10)\t Batch(2600/3375): \tTrain Loss  nan\n",
            "Epoch(10/10)\t Batch(2800/3375): \tTrain Loss  nan\n",
            "Epoch(10/10)\t Batch(3000/3375): \tTrain Loss  nan\n",
            "Epoch(10/10)\t Batch(3200/3375): \tTrain Loss  nan\n",
            "End of Training: 9.1 Train Accuracy: 0.0996 Validation Accuracy: 0.1037\n",
            "Test Accuracy: 0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "#Parameters\n",
        "n_hidden = 16\n",
        "hid_layers = 3\n",
        "activation = \"tanh\"\n",
        "init = \"Random\"\n",
        "batch_size=16\n",
        "lossi = []\n",
        "nepoch = 10\n",
        "# optimizer = \"rmsprop\"\n",
        "# opt = Optimizer(lr=1e-3, optimizer=\"nag\")\n",
        "loss_fn = \"cross_entropy\"\n",
        "Loss = CrossEntropyLoss() if loss_fn==\"cross_entropy\" else MSE()\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "\n",
        "X = train_images.reshape(train_images.shape[0], -1)/ 255.0\n",
        "Y = train_labels\n",
        "Y = np.eye(10)[Y]     #one_hot encoding\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "#Model\n",
        "\n",
        "# Define layer sizes\n",
        "layer_sizes = [128]*hid_layers\n",
        "\n",
        "\n",
        "\n",
        "if activation == \"tanh\":\n",
        "  print(\"Activation used is Tanh\")\n",
        "\n",
        "  model = Sequential([Linear(784, layer_sizes[1], weight_init=init)])\n",
        "\n",
        "  # Hidden layers\n",
        "  for i in range(1, len(layer_sizes)-1):\n",
        "      model.append(Tanh())\n",
        "      model.append(Linear(layer_sizes[i], layer_sizes[i+1], weight_init=init))\n",
        "\n",
        "  # Final output layer\n",
        "  model.append(Tanh())\n",
        "  model.append(Linear(layer_sizes[-1], 10, weight_init=init))  # 128->10\n",
        "\n",
        "\n",
        "elif activation == \"relu\":\n",
        "  print(\"Activation used is ReLu\")\n",
        "  model = Sequential([Linear(784, layer_sizes[1], weight_init=init)])\n",
        "\n",
        "  # Hidden layers\n",
        "  for i in range(1, len(layer_sizes)-1):\n",
        "      model.append(Relu())\n",
        "      model.append(Linear(layer_sizes[i], layer_sizes[i+1], weight_init=init))\n",
        "\n",
        "  # Final output layer\n",
        "  model.append(Relu())\n",
        "  model.append(Linear(layer_sizes[-1], 10, weight_init=init))  # 128->10\n",
        "\n",
        "\n",
        "elif activation == \"sigmoid\":\n",
        "  print(\"Activation used is Sigmoid\")\n",
        "  model = Sequential([Linear(784, layer_sizes[1], weight_init=init)])\n",
        "\n",
        "  # Hidden layers\n",
        "  for i in range(1, len(layer_sizes)-1):\n",
        "      model.append(Sigmoid())\n",
        "      model.append(Linear(layer_sizes[i], layer_sizes[i+1], weight_init=init))\n",
        "\n",
        "  # Final output layer\n",
        "  model.append(Sigmoid())\n",
        "  model.append(Linear(layer_sizes[-1], 10, weight_init=init))  # 128->10\n",
        "\n",
        "\n",
        "else:\n",
        "  raise Exception(\"Invalid activation function\")\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "#Training Loop\n",
        "opt = Optimizer(lr=1e-1, optimizer=\"ndam\", param=model.parameters())\n",
        "\n",
        "for epoch in range(nepoch):\n",
        "  logits = model(X)\n",
        "  # train_loss = CrossEntropyLoss()(logits, Y)\n",
        "  train_accuracy = np.mean(np.argmax(logits, axis=1) == np.argmax(Y, axis=1))\n",
        "\n",
        "  val_logits = model(val_images.reshape(val_images.shape[0], -1)/ 255.0)\n",
        "  Yv = np.eye(10)[val_labels]\n",
        "  # val_loss = CrossEntropyLoss()(val_logits, Yv)\n",
        "  val_accuracy = np.mean(np.argmax(val_logits, axis=1) == np.argmax(Yv, axis=1))\n",
        "\n",
        "  print(f\"Begining of Epoch: {epoch+1} Train Accuracy: {train_accuracy:.4f} Validation Accuracy: {val_accuracy:.4f}\")\n",
        "  print(\"-------x-------\")\n",
        "  #Shuffling\n",
        "  indices = np.random.permutation(X.shape[0])\n",
        "  X = X[indices]\n",
        "  Y = Y[indices]\n",
        "  for i in range(0, train_images.shape[0], batch_size):\n",
        "    Xb = X[i:i + batch_size]\n",
        "    Yb = Y[i:i + batch_size]\n",
        "\n",
        "    # Forward Pass\n",
        "    logits = model(Xb)\n",
        "    loss = Loss(logits, Yb)\n",
        "\n",
        "    #Backward Pass\n",
        "    dout = Loss.grad(logits, Yb)\n",
        "    dout = model.backward(dout)\n",
        "\n",
        "    batch_num = i//batch_size\n",
        "    total_batch = train_images.shape[0]//batch_size\n",
        "\n",
        "    #parameter update\n",
        "    if batch_num%200 == 0: # print every once in a while\n",
        "      print(f'Epoch({epoch+1}/{nepoch})\\t Batch({batch_num:2d}/{total_batch:2d}): \\tTrain Loss  {loss:.4f}')\n",
        "\n",
        "    opt(model.parameters(), dout[1])\n",
        "    lossi.append(loss)\n",
        "\n",
        "  opt.t += 1\n",
        "\n",
        "logits = model(X)\n",
        "train_accuracy = np.mean(np.argmax(logits, axis=1) == np.argmax(Y, axis=1))\n",
        "\n",
        "val_logits = model(val_images.reshape(val_images.shape[0], -1)/ 255.0)\n",
        "Yv = np.eye(10)[val_labels]\n",
        "val_accuracy = np.mean(np.argmax(val_logits, axis=1) == np.argmax(Yv, axis=1))\n",
        "\n",
        "print(f\"End of Training: {epoch+1/nepoch} Train Accuracy: {train_accuracy:.4f} Validation Accuracy: {val_accuracy:.4f}\")\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "# Test Accuracy\n",
        "x = test_images\n",
        "x = x.reshape(x.shape[0], -1)\n",
        "y = test_labels\n",
        "\n",
        "#Forward Pass\n",
        "logits = model(x)\n",
        "accuracy_formula = np.mean(np.argmax(logits, axis=1) == y)\n",
        "print(f\"Test Accuracy: {accuracy_formula}\")\n",
        "\n",
        "plt.plot(lossi)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "xjvmJMZBLhOm",
        "outputId": "1b7cddc0-1cb6-439e-f9ed-a1ee6cfd1e66"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(np.array(lossi).reshape(-1, 25).mean(1))\n",
        "plt.plot()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "89dXNDTfBtp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBu8F1HNK5wF"
      },
      "outputs": [],
      "source": [
        "    # Learning Rate Scheduling\n",
        "    # lr = 1e-4\n",
        "    # if epoch < 3:\n",
        "    #   lr = 1e-1\n",
        "    # else:\n",
        "    #   lr = 1e-3 if (batch_num*1.25) < total_batch else 1e-5\n",
        "    # lr = max(lr * 0.5, 1e-5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfcU-lCiwyFb"
      },
      "outputs": [],
      "source": [
        "    # clip_value=1e-3\n",
        "    # velocity = [np.zeros_like(param) for param in model.parameters()]\n",
        "    # for i, (u, param, grad) in enumerate(zip(velocity, model.parameters(), dout[1])):\n",
        "    #     #momentum\n",
        "    #     clipped_dparam = np.clip(grad, -clip_value, clip_value)\n",
        "    #     u = 0.9 * u + 0.1 * clipped_dparam\n",
        "    #     param -= lr * u + clipped_dparam #grad\n",
        "\n",
        "    # RMS Prop\n",
        "    #     clipped_dparam = np.clip(grad, -clip_value, clip_value)\n",
        "    #     u = 0.9 * u + (1 - 0.9) * (clipped_dparam**2)\n",
        "    #     param -= ((lr * clipped_dparam)/ (np.sqrt(u + 1e-8)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV1Lz8zRQH6P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjzMd-R_S67F",
        "outputId": "367a5f54-1b0b-48c8-85c5-abd147f972cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixEYWI-1iaHn"
      },
      "outputs": [],
      "source": [
        "# class Dropout:\n",
        "#     \"\"\"Dropout layer which randomly sets activations to zero.\n",
        "\n",
        "#     Parameters\n",
        "#     ----------\n",
        "#     p : float\n",
        "#         The probability of zeroing any activation. Must be smaller than 1 and larger than 0\n",
        "#     \"\"\"\n",
        "#     def __init__(self, p=0.5):\n",
        "#         super(Dropout, self).__init__()\n",
        "#         self.p = p\n",
        "#         self.cache = dict(a=None)\n",
        "\n",
        "#     def __call__(self, x):\n",
        "#         return self.forward(x)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         if self.training:\n",
        "#             mask = np.random.random(x.shape) > self.p\n",
        "#             scale = 1.0 / (1-self.p)\n",
        "#             a = x * mask * scale\n",
        "#             self.cache = dict(a=a)\n",
        "#             return a\n",
        "#         return x\n",
        "\n",
        "#     def backward(self, delta_in):\n",
        "#         delta_out = delta_in * self.cache['a']\n",
        "#         return delta_out\n",
        "\n",
        "#     def parameters(self):\n",
        "#       return []\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2Go5SkqTIxW",
        "outputId": "b0eb938f-77ee-472f-936f-5150ec70007d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "test_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4yOWeB1KkQC",
        "outputId": "0a7168f5-7aa7-41b7-e96c-edad650c899d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.810546875"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "ix = np.random.randint(0, test_images.shape[0], (1024))\n",
        "x = test_images[ix]\n",
        "x = x.reshape(1024, -1)\n",
        "y = test_labels[ix]\n",
        "# y = np.eye(10)[y]\n",
        "\n",
        "#Forward Pass\n",
        "logits = model(x)\n",
        "exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "probs = exp_logits / np.sum(exp_logits + 1e-12, axis=1, keepdims=True)\n",
        "#accuracy\n",
        "accuracy_formula = np.mean(np.argmax(probs, axis=1) == test_labels[ix])\n",
        "accuracy_formula\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQq-h_6oSRIF",
        "outputId": "7ae1295a-3d9b-4fa4-a908-8a558a696617"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 1, 8, ..., 7, 1, 8], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0X4Mo6FLT8j"
      },
      "outputs": [],
      "source": [
        "probs;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEoT_j7AFzM9",
        "outputId": "d8a1701a-8d9b-402b-8b74-43d72b2a0c9a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(784, 1024), (1024,), (1024, 512), (512,), (512, 10), (10,)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "[i.shape for i in model.parameters()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFJvWZBeFUkZ",
        "outputId": "b21241c5-35d7-436d-ca29-50e6eac34034"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(784, 1024), (1024,), (1024, 512), (512,), (512, 10), (10,)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "[i.shape for i in dout[1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vsu1v68m9G_l",
        "outputId": "d31a2725-3296-42c6-c241-12f2a0143d98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(784, 1024), (1024,), (1024, 512), (512,), (512, 10), (10,)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "#Parameter Update\n",
        "[i.shape for i in model.parameters()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16yD0iG77YSe"
      },
      "outputs": [],
      "source": [
        "# Assuming you have a learning rate defined (e.g., lr = 0.001)\n",
        "lr = 0.01\n",
        "\n",
        "# Parameter Update\n",
        "for i, param in enumerate(model.parameters()):\n",
        "    param -= lr * model.parameters()[i]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzTFxrVB8Pzv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "66epypUu5h6M",
        "outputId": "a6fb3cb0-77b7-41e2-8eff-ae93de41de8d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (32,) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-322fcc561845>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3502\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3504\u001b[0;31m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0m\u001b[1;32m   3505\u001b[0m                           out=out, **kwargs)\n\u001b[1;32m   3506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (32,) + inhomogeneous part."
          ]
        }
      ],
      "source": [
        "probs[0], y[0], np.mean([-np.log(probs[i][y[i].astype(bool)]) for i in range(32)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHIrwhrdoaBK"
      },
      "outputs": [],
      "source": [
        "# class Softmax:\n",
        "#   def __call__(self, x):\n",
        "#       exp_x = np.exp(x - np.max(x))\n",
        "#       softmax_output = exp_x / np.sum(exp_x + 1e-12, axis=1, keepdims=True)\n",
        "#       self.cache = {\"x\": softmax_output}\n",
        "#       return softmax_output\n",
        "\n",
        "#   def grad(self, d_out):\n",
        "#       x = self.cache[\"x\"]\n",
        "#       # Softmax gradient using Jacobian matrix simplification\n",
        "#       return x * (d_out - np.sum((d_out * x), axis=1, keepdim=True))\n",
        "\n",
        "#   def parameters(self):\n",
        "#       return []\n",
        "\n",
        "class Softmax:\n",
        "    def forward(self, x):\n",
        "        self.cache['x'] = x\n",
        "        exp_x = np.exp(x - np.max(x))\n",
        "        softmax_output = exp_x / np.sum(exp_x + 1e-12, axis=1, keepdims=True)\n",
        "        self.cache['y'] = softmax_output\n",
        "        return softmax_output\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    # def grad(self, grad):\n",
        "    #    pass\n",
        "\n",
        "    def backward(self, grad):\n",
        "        softmax_output = self.cache['y']\n",
        "        jacobian_matrix = np.zeros((softmax_output.shape[0], softmax_output.shape[1], softmax_output.shape[1]))\n",
        "        for i in range(softmax_output.shape[0]):\n",
        "            for j in range(softmax_output.shape[1]):\n",
        "                for k in range(softmax_output.shape[1]):\n",
        "                    if j == k:\n",
        "                        jacobian_matrix[i, j, k] = softmax_output[i, j] * (1 - softmax_output[i, k])\n",
        "                    else:\n",
        "                        jacobian_matrix[i, j, k] = -softmax_output[i, j] * softmax_output[i, k]\n",
        "        grad_input = np.matmul(grad.reshape(grad.shape[0], 1, grad.shape[1]), jacobian_matrix).squeeze()\n",
        "        return grad_input\n",
        "\n",
        "    def parameters(self):\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agil7zrNzsYO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xra64cX5wx0B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDqMRLcMxLIY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR5D2cgzudT3",
        "outputId": "590369ac-6800-4a64-96ee-cfbbc4839d1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1.21812830e+08, 5.00494765e+00, 2.88747793e+16, 8.19706125e+00,\n",
              "       2.18774776e+51, 5.76565793e-04, 7.93381879e-15, 5.07658567e+17,\n",
              "       7.54881469e-04, 1.05752289e-15])"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "exp_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZaEbq38q1N4",
        "outputId": "5a1101b8-3121-421b-94df-47e49520d98e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([5.56795589e-44, 2.28771696e-51, 1.31984042e-35, 3.74680363e-51,\n",
              "       1.00000000e+00, 2.63543084e-55, 3.62647784e-66, 2.32046205e-34,\n",
              "       3.45049590e-55, 4.83384285e-67])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "exp_x = np.exp(model(x)[0])\n",
        "exp_x / np.sum(exp_x + 1e-12,keepdims=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CC-THy5ArGpP",
        "outputId": "4cf12bc2-63a7-4c86-b784-fcb015dd6a7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([134.17359438, 235.51615363, 225.55801352, 189.03856156,\n",
              "       227.79684358, 416.92455184, 512.01980769, 321.44690602,\n",
              "       258.26047933, 423.34805362])"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "-np.log(softmax()(model(x))[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9UVfpcCrhwx",
        "outputId": "0c0c566b-1533-43a5-de8b-7c461bbdff54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((32, 10), (32, 1))"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logits.shape, np.max(logits, axis=1, keepdims=True).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0Lhqe6QrxVC",
        "outputId": "3847af48-03f4-4412-949e-bb8e24cd3809"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[425.50188237519126,\n",
              " 185.9094704847457,\n",
              " 290.1206641997822,\n",
              " 265.02186817180507,\n",
              " 340.9891968659556,\n",
              " 440.00742015485577,\n",
              " 272.2057741111338,\n",
              " 296.72707107667895,\n",
              " 344.4222659414061,\n",
              " 357.0928708290836,\n",
              " 212.5657694618118,\n",
              " 232.1679842541356,\n",
              " 224.62922170386668,\n",
              " 307.3223774692562,\n",
              " 486.0651473556204,\n",
              " 499.24873340385466,\n",
              " 317.013440900536,\n",
              " 315.2569022635988,\n",
              " 229.15654673802587,\n",
              " 309.7206506396219,\n",
              " 633.4780787632286,\n",
              " 414.4626196034568,\n",
              " 472.40680273895975,\n",
              " 395.5300074701161,\n",
              " 277.6063515598283,\n",
              " 172.51061125268632,\n",
              " 155.2245678627663,\n",
              " 406.95842297515213,\n",
              " 363.629894835442,\n",
              " 302.3364751552298,\n",
              " 342.0113187046049,\n",
              " 401.26389149417207]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[np.max(logits[i]) for i in range(len(logits))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c3u3uIOrvyF",
        "outputId": "7b3a9c01-05bb-4577-cdd6-8b1b812800f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[2.62207366e-141, 9.71711789e-220, 7.59852842e-021,\n",
              "        1.73433708e-147, 3.36217896e-103, 1.00000000e+000,\n",
              "        9.85886044e-100, 2.78856416e-167, 3.46430731e-161,\n",
              "        2.28072253e-169],\n",
              "       [7.54880441e-120, 1.00000000e+000, 1.29041877e-083,\n",
              "        8.28147332e-161, 3.95743976e-151, 1.00761721e-006,\n",
              "        1.17152787e-048, 5.61877276e-116, 1.00083379e-215,\n",
              "        1.25502041e-161],\n",
              "       [8.17594826e-119, 4.07263077e-173, 4.24410091e-092,\n",
              "        1.49471482e-032, 4.22706790e-042, 1.28874399e-041,\n",
              "        1.00000000e+000, 5.26623207e-195, 2.22357302e-087,\n",
              "        5.61196663e-153],\n",
              "       [9.18517675e-041, 0.00000000e+000, 4.23747693e-180,\n",
              "        9.19149290e-162, 1.01909733e-258, 1.00000000e+000,\n",
              "        1.86940819e-183, 9.25278142e-210, 1.93209702e-277,\n",
              "        3.75415743e-117],\n",
              "       [1.50586616e-206, 9.72607346e-114, 7.66796067e-168,\n",
              "        6.76548592e-177, 1.31326308e-172, 1.00000000e+000,\n",
              "        3.27365893e-207, 6.14033865e-219, 0.00000000e+000,\n",
              "        5.79746396e-273],\n",
              "       [1.00000000e+000, 1.89266685e-172, 3.81795307e-198,\n",
              "        6.68565435e-030, 2.37497359e-176, 3.84708225e-029,\n",
              "        7.75142306e-200, 2.52266511e-166, 1.05092410e-206,\n",
              "        4.59941284e-170],\n",
              "       [1.00000000e+000, 5.58865056e-183, 1.22205799e-086,\n",
              "        4.56103595e-095, 4.28307344e-202, 2.17447197e-115,\n",
              "        3.50845772e-169, 1.70528919e-220, 1.91936666e-246,\n",
              "        1.53103995e-142],\n",
              "       [1.00000000e+000, 3.78748360e-176, 7.01510739e-044,\n",
              "        2.12067272e-120, 4.42338365e-161, 1.40133094e-093,\n",
              "        9.27814703e-077, 6.57939496e-182, 4.46595811e-271,\n",
              "        2.20655732e-076],\n",
              "       [1.05060756e-030, 1.65710194e-122, 2.58330396e-064,\n",
              "        5.23289736e-101, 2.26751905e-100, 8.86752223e-131,\n",
              "        1.00000000e+000, 3.81658884e-177, 4.43416200e-263,\n",
              "        3.64351789e-144],\n",
              "       [1.00000000e+000, 1.18637290e-240, 8.54778413e-113,\n",
              "        6.33471905e-143, 3.42252597e-127, 2.11363922e-003,\n",
              "        1.13589825e-064, 2.76153498e-080, 3.00038495e-213,\n",
              "        2.23443565e-141],\n",
              "       [1.00000000e+000, 5.84529114e-276, 5.78418196e-236,\n",
              "        3.93552640e-158, 4.27816218e-271, 9.71974625e-234,\n",
              "        6.38058862e-245, 1.02450186e-192, 0.00000000e+000,\n",
              "        2.46343234e-311],\n",
              "       [2.60018598e-051, 1.63313764e-298, 1.32496901e-158,\n",
              "        2.77615583e-211, 5.38660847e-257, 1.00000000e+000,\n",
              "        2.59080519e-235, 9.73882690e-138, 1.91983912e-238,\n",
              "        6.30306589e-180],\n",
              "       [1.00000000e+000, 3.06725479e-190, 8.07387982e-062,\n",
              "        1.64448358e-134, 2.98279273e-288, 5.31808658e-072,\n",
              "        6.99094050e-014, 3.04686863e-236, 0.00000000e+000,\n",
              "        1.59799413e-033],\n",
              "       [9.67210749e-063, 5.99833676e-024, 2.30144024e-039,\n",
              "        9.17137720e-012, 1.00000000e+000, 3.76324766e-091,\n",
              "        4.33111086e-087, 4.51355523e-101, 8.91638870e-086,\n",
              "        4.13055341e-067],\n",
              "       [5.13227654e-158, 1.94661907e-172, 9.22692322e-059,\n",
              "        4.19671456e-081, 3.66255633e-096, 5.84671257e-099,\n",
              "        7.05335922e-081, 1.00000000e+000, 1.51931030e-179,\n",
              "        1.87665298e-215],\n",
              "       [1.00000000e+000, 1.55046139e-207, 8.40589626e-128,\n",
              "        5.11425434e-141, 3.82493804e-243, 3.41221418e-120,\n",
              "        2.04810951e-186, 7.04650462e-315, 0.00000000e+000,\n",
              "        0.00000000e+000],\n",
              "       [1.00000000e+000, 0.00000000e+000, 2.84663170e-010,\n",
              "        1.85527043e-205, 2.50397366e-052, 1.19367425e-183,\n",
              "        5.25588830e-173, 3.19361008e-158, 0.00000000e+000,\n",
              "        1.93450481e-229],\n",
              "       [5.57084227e-173, 1.91927240e-249, 2.32019451e-117,\n",
              "        8.96241923e-147, 6.13671942e-088, 8.95948775e-158,\n",
              "        1.00000000e+000, 5.58942731e-122, 1.64925660e-186,\n",
              "        4.65656439e-313],\n",
              "       [1.48832274e-008, 8.56809600e-250, 1.00000000e+000,\n",
              "        1.36503758e-072, 1.01293453e-142, 4.86030926e-010,\n",
              "        1.70171538e-129, 5.59240959e-157, 3.99551942e-164,\n",
              "        6.29325109e-154],\n",
              "       [1.00000000e+000, 2.85305085e-185, 5.53922422e-016,\n",
              "        5.40597235e-126, 9.53025668e-115, 2.73097375e-018,\n",
              "        1.05709790e-187, 2.03531641e-143, 5.24673749e-251,\n",
              "        1.61525185e-216],\n",
              "       [4.17866690e-240, 5.06347327e-241, 9.48816519e-108,\n",
              "        5.49037684e-187, 6.04812075e-095, 1.31174340e-090,\n",
              "        1.88181240e-157, 1.00000000e+000, 0.00000000e+000,\n",
              "        8.69241352e-155],\n",
              "       [4.34504635e-056, 1.93465276e-046, 5.72717991e-081,\n",
              "        1.21932167e-175, 1.54680663e-237, 1.70621478e-036,\n",
              "        1.96327575e-188, 0.00000000e+000, 3.98686198e-258,\n",
              "        1.00000000e+000],\n",
              "       [4.71421306e-049, 9.77997433e-226, 8.39790865e-249,\n",
              "        4.13916153e-100, 5.78325776e-116, 5.23349768e-020,\n",
              "        1.00000000e+000, 1.14513554e-126, 0.00000000e+000,\n",
              "        1.28422766e-131],\n",
              "       [1.00000000e+000, 6.50926103e-161, 9.63390915e-161,\n",
              "        4.29167935e-151, 2.29492879e-187, 4.18179522e-118,\n",
              "        1.17927673e-265, 2.31560297e-186, 6.04463047e-153,\n",
              "        2.96105738e-267],\n",
              "       [3.97274149e-026, 0.00000000e+000, 2.70718422e-033,\n",
              "        7.12032891e-029, 1.00000000e+000, 3.06367826e-163,\n",
              "        1.60431508e-160, 2.38514803e-138, 2.32899695e-057,\n",
              "        6.49227330e-257],\n",
              "       [9.92255966e-021, 3.48247592e-158, 6.39661779e-113,\n",
              "        1.00000000e+000, 7.06145146e-031, 1.31681935e-139,\n",
              "        9.90600935e-112, 2.12014919e-123, 1.51824743e-100,\n",
              "        2.98532598e-078],\n",
              "       [1.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
              "        2.10553652e-162, 3.26083326e-322, 5.00086916e-081,\n",
              "        1.20988626e-305, 8.08634571e-314, 0.00000000e+000,\n",
              "        5.35700091e-216],\n",
              "       [1.00000000e+000, 1.10402190e-316, 4.13220672e-145,\n",
              "        1.30429416e-174, 2.70165876e-244, 1.34331214e-112,\n",
              "        1.34455715e-167, 5.26639838e-286, 0.00000000e+000,\n",
              "        0.00000000e+000],\n",
              "       [1.00000000e+000, 1.27126303e-116, 1.35210246e-164,\n",
              "        7.87140375e-079, 2.25320557e-118, 2.27959710e-145,\n",
              "        3.44031393e-247, 1.94726095e-223, 1.42648151e-285,\n",
              "        4.16345773e-171],\n",
              "       [6.46085931e-047, 4.12346976e-175, 1.00000000e+000,\n",
              "        1.28241401e-050, 1.03388329e-106, 2.65409993e-079,\n",
              "        3.44188931e-025, 3.72653570e-164, 4.34897598e-255,\n",
              "        8.27910463e-152],\n",
              "       [5.73892065e-166, 1.21460351e-284, 8.84356847e-057,\n",
              "        5.95480129e-161, 7.15612868e-184, 1.37621649e-008,\n",
              "        1.00000000e+000, 1.27019742e-288, 0.00000000e+000,\n",
              "        3.34786478e-230],\n",
              "       [2.75723461e-155, 0.00000000e+000, 3.71378714e-181,\n",
              "        6.66074976e-200, 2.23479333e-084, 2.26640172e-120,\n",
              "        1.00000000e+000, 3.10912834e-258, 3.37459074e-282,\n",
              "        2.52895854e-301]])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n = logits - np.max(logits, axis=1, keepdims=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MacCkB16ZgaV",
        "outputId": "a7437f50-26ce-424b-f25c-44f7f8a055f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-19d7e426f8ab>:24: RuntimeWarning: invalid value encountered in log\n",
            "  loss = -np.log(y[t.astype(bool)] + self.eps)\n"
          ]
        }
      ],
      "source": [
        "#Forward Pass\n",
        "softmax_obj = softmax()\n",
        "logits = model(x)\n",
        "probs = softmax_obj(logits)\n",
        "loss = CrossEntropyLoss()(logits, y)\n",
        "\n",
        "#Backward Pass\n",
        "\n",
        "dout = CrossEntropyLoss().grad(logits, y)\n",
        "dout = softmax_obj.grad(dout)\n",
        "dout = model.backward(dout)\n",
        "\n",
        "# z = model.backward(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXG7X1aWbn_2"
      },
      "outputs": [],
      "source": [
        "[i.shape for i in z]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMp8MZm_a_0q"
      },
      "outputs": [],
      "source": [
        "[i.shape for i in model.parameters()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVaRdVYeimhw"
      },
      "outputs": [],
      "source": [
        "# prompt: define targets using torch.randn\n",
        "\n",
        "targets = np.random.randn(32, 10) # Example: 32 samples, 10 classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlUCiRRBvYxs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUB38urpvD0X"
      },
      "outputs": [],
      "source": [
        "np.max(model(x), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n17lVxJ9WpWR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGQQ-UF0ijjM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ErpnHKxkXd9"
      },
      "outputs": [],
      "source": [
        "# Training Setup\n",
        "\n",
        "model = Sequential([Linear(784, n_hidden, weight_init=\"Xavier\")])\n",
        "model.append(Sigmoid())\n",
        "for _ in range(hid_layers-1):\n",
        "    model.append(Linear(n_hidden, n_hidden))\n",
        "    model.append(Sigmoid())\n",
        "model.append(Linear(n_hidden, 10))\n",
        "\n",
        "# Initialize parameters\n",
        "with torch.no_grad():\n",
        "    model.layers[-1].weight *= 0.1\n",
        "\n",
        "parameters = model.parameters()\n",
        "learning_rate = 0.1\n",
        "batch_size = 32\n",
        "\n",
        "# Example training loop\n",
        "def train_step(x_batch, y_batch):\n",
        "\n",
        "    # Forward pass\n",
        "    logits = model(x_batch)\n",
        "\n",
        "    # Compute loss (cross-entropy)\n",
        "    # probs = softmax(logits)\n",
        "    # loss = -np.log(probs[range(batch_size), y_batch]).mean()\n",
        "\n",
        "    # cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
        "    logit_maxes = np.max(logits, axis = 1, keepdims=True).values\n",
        "    norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "    counts = np.exp(norm_logits)\n",
        "    counts_sum = np.sum(counts, axis = 1, keepdims=True)\n",
        "    counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "    probs = counts * counts_sum_inv\n",
        "    logprobs = np.log(probs)\n",
        "    loss = -logprobs[range(batch_size), y_batch].mean()\n",
        "\n",
        "\n",
        "    dlogprobs = torch.zeros_like(logprobs)\n",
        "    dlogprobs[range(batch_size), y_batch] = -1.0/len(y_batch)\n",
        "    dprobs = (1.0 / probs) * dlogprobs\n",
        "    dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
        "    dcounts = counts_sum_inv * dprobs\n",
        "    dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
        "    dcounts += torch.ones_like(counts) * dcounts_sum\n",
        "    dnorm_logits = counts * dcounts\n",
        "    dlogits = dnorm_logits.clone()\n",
        "\n",
        "    # Backward pass preparation\n",
        "    d_out = np.clone(probs)\n",
        "    d_out[range(batch_size), y_batch] -= 1\n",
        "    d_out /= batch_size\n",
        "\n",
        "    # Backward pass\n",
        "    model.grad(d_out)\n",
        "\n",
        "    # # Parameter update\n",
        "    # with torch.no_grad():\n",
        "    #     for p in parameters:\n",
        "    #         p -= learning_rate * p.grad\n",
        "\n",
        "    # return loss.item()\n",
        "\n",
        "# Usage example with dummy data\n",
        "x_dummy = torch.randn(batch_size, 784)\n",
        "y_dummy = torch.randint(0, 10, (batch_size,))\n",
        "\n",
        "for epoch in range(10):\n",
        "    loss = train_step(x_dummy, y_dummy)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfoZZ4wQl-RM",
        "outputId": "7e1b6ec4-8a34-4d23-c2af-2b9cc8ecaf00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'6.0.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import yaml\n",
        "yaml.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wvx2RFhnqFY0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMr66RLVHIEFnCHIbVGOo/u",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}